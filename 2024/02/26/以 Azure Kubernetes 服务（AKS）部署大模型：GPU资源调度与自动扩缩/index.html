<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>以 Azure Kubernetes 服务（AKS）部署大模型：GPU资源调度与自动扩缩 - 梁友泽的博客</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="梁友泽的博客"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="梁友泽的博客"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="随着大模型（如GPT、LLaMA等）的广泛应用，如何在云原生环境中高效部署和管理这类资源密集型应用成为技术挑战。Azure Kubernetes服务（AKS）凭借其灵活的GPU资源调度能力和自动化扩缩机制，成为部署大模型的理想选择。本文将从核心挑战、部署流程、调度策略到优化实践，系统解析AKS在大模型场景下的技术实现。"><meta property="og:type" content="blog"><meta property="og:title" content="以 Azure Kubernetes 服务（AKS）部署大模型：GPU资源调度与自动扩缩"><meta property="og:url" content="https://www.liangyouze.com/2024/02/26/%E4%BB%A5%20Azure%20Kubernetes%20%E6%9C%8D%E5%8A%A1%EF%BC%88AKS%EF%BC%89%E9%83%A8%E7%BD%B2%E5%A4%A7%E6%A8%A1%E5%9E%8B%EF%BC%9AGPU%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%E4%B8%8E%E8%87%AA%E5%8A%A8%E6%89%A9%E7%BC%A9/"><meta property="og:site_name" content="梁友泽的博客"><meta property="og:description" content="随着大模型（如GPT、LLaMA等）的广泛应用，如何在云原生环境中高效部署和管理这类资源密集型应用成为技术挑战。Azure Kubernetes服务（AKS）凭借其灵活的GPU资源调度能力和自动化扩缩机制，成为部署大模型的理想选择。本文将从核心挑战、部署流程、调度策略到优化实践，系统解析AKS在大模型场景下的技术实现。"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://www.liangyouze.com/img/og_image.png"><meta property="article:published_time" content="2024-02-26T12:11:23.000Z"><meta property="article:modified_time" content="2025-03-30T07:54:09.704Z"><meta property="article:author" content="梁友泽"><meta property="article:tag" content="azure"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://www.liangyouze.com/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.liangyouze.com/2024/02/26/%E4%BB%A5%20Azure%20Kubernetes%20%E6%9C%8D%E5%8A%A1%EF%BC%88AKS%EF%BC%89%E9%83%A8%E7%BD%B2%E5%A4%A7%E6%A8%A1%E5%9E%8B%EF%BC%9AGPU%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%E4%B8%8E%E8%87%AA%E5%8A%A8%E6%89%A9%E7%BC%A9/"},"headline":"以 Azure Kubernetes 服务（AKS）部署大模型：GPU资源调度与自动扩缩","image":["https://www.liangyouze.com/img/og_image.png"],"datePublished":"2024-02-26T12:11:23.000Z","dateModified":"2025-03-30T07:54:09.704Z","author":{"@type":"Person","name":"梁友泽"},"publisher":{"@type":"Organization","name":"梁友泽的博客","logo":{"@type":"ImageObject","url":"https://www.liangyouze.com/img/logo.svg"}},"description":"随着大模型（如GPT、LLaMA等）的广泛应用，如何在云原生环境中高效部署和管理这类资源密集型应用成为技术挑战。Azure Kubernetes服务（AKS）凭借其灵活的GPU资源调度能力和自动化扩缩机制，成为部署大模型的理想选择。本文将从核心挑战、部署流程、调度策略到优化实践，系统解析AKS在大模型场景下的技术实现。"}</script><link rel="canonical" href="https://www.liangyouze.com/2024/02/26/%E4%BB%A5%20Azure%20Kubernetes%20%E6%9C%8D%E5%8A%A1%EF%BC%88AKS%EF%BC%89%E9%83%A8%E7%BD%B2%E5%A4%A7%E6%A8%A1%E5%9E%8B%EF%BC%9AGPU%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%E4%B8%8E%E8%87%AA%E5%8A%A8%E6%89%A9%E7%BC%A9/"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link data-pjax rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.7.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link data-pjax rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="梁友泽的博客" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">主页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/about">关于我</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2024-02-26T12:11:23.000Z" title="2/26/2024, 8:11:23 PM">2024-02-26</time>发表</span><span class="level-item"><time dateTime="2025-03-30T07:54:09.704Z" title="3/30/2025, 3:54:09 PM">2025-03-30</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/azure/">azure</a></span><span class="level-item">31 分钟读完 (大约4676个字)</span><span class="level-item" id="busuanzi_container_page_pv"><span id="busuanzi_value_page_pv">0</span>次访问</span></div></div><h1 class="title is-3 is-size-4-mobile">以 Azure Kubernetes 服务（AKS）部署大模型：GPU资源调度与自动扩缩</h1><div class="content"><p>随着大模型（如GPT、LLaMA等）的广泛应用，如何在云原生环境中高效部署和管理这类资源密集型应用成为技术挑战。Azure Kubernetes服务（AKS）凭借其灵活的GPU资源调度能力和自动化扩缩机制，成为部署大模型的理想选择。本文将从核心挑战、部署流程、调度策略到优化实践，系统解析AKS在大模型场景下的技术实现。</p>
<span id="more"></span>

<h2 id="一、大模型部署的核心挑战与AKS的适配性"><a href="#一、大模型部署的核心挑战与AKS的适配性" class="headerlink" title="一、大模型部署的核心挑战与AKS的适配性"></a>一、大模型部署的核心挑战与AKS的适配性</h2><p>大语言模型（LLM）如GPT-4、LLaMA-3等的部署与传统应用存在显著差异。其庞大的参数量（通常达数百亿甚至千亿级别）、复杂的计算图结构以及对实时推理的低延迟需求，使得在云原生环境中高效运行这类模型面临多重挑战。Azure Kubernetes服务（AKS）通过深度整合Kubernetes原生能力与Azure云平台特性，提供了针对性的解决方案。以下从技术细节层面解析核心挑战与AKS的适配性。</p>
<h3 id="1-GPU资源需求的动态波动与异构调度"><a href="#1-GPU资源需求的动态波动与异构调度" class="headerlink" title="1. GPU资源需求的动态波动与异构调度"></a>1. GPU资源需求的动态波动与异构调度</h3><h4 id="挑战深度解析"><a href="#挑战深度解析" class="headerlink" title="挑战深度解析"></a>挑战深度解析</h4><p>大模型的全生命周期涉及训练、微调、推理三个阶段，各阶段对GPU资源的消耗模式截然不同：</p>
<ul>
<li>训练阶段：需长期占用高规格GPU集群（如NVIDIA H100&#x2F;A100），显存需求通常超过40GB，且需多卡并行（如使用ZeRO-3分布式策略）。</li>
<li>推理阶段：单次请求的GPU算力需求较低，但受突发流量影响，需快速弹性扩缩。例如，某客服机器人可能在10分钟内从零请求激增至每秒1000次推理调用。</li>
<li>混合负载场景：同一集群可能同时运行训练与推理任务，资源竞争导致显存碎片化问题。</li>
</ul>
<h4 id="AKS的适配策略"><a href="#AKS的适配策略" class="headerlink" title="AKS的适配策略"></a>AKS的适配策略</h4><p>多节点池架构<br>AKS允许创建多个异构GPU节点池，通过差异化配置实现资源隔离：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># 创建训练专用节点池（H100 PCIe）</span><br><span class="line">az aks nodepool add \</span><br><span class="line">    --name train-h100-pool \</span><br><span class="line">    --node-vm-size Standard_ND96amsr_H100_v5 \</span><br><span class="line">    --node-taints workload=train:NoSchedule \</span><br><span class="line">    --min-count 3 \</span><br><span class="line">    --max-count 50</span><br><span class="line"></span><br><span class="line"># 创建推理节点池（A10G低成本实例）</span><br><span class="line">az aks nodepool add \</span><br><span class="line">    --name infer-a10g-pool \</span><br><span class="line">    --node-vm-size Standard_NV72ads_A10_v5 \</span><br><span class="line">    --node-taints workload=infer:NoSchedule \</span><br><span class="line">    --enable-cluster-autoscaler \</span><br><span class="line">    --min-count 0 \  # 支持缩容至零</span><br><span class="line">    --max-count 100</span><br></pre></td></tr></table></figure>

<p>关键技术特性：</p>
<ul>
<li>硬件级隔离：训练节点池使用H100加速卡，专为FP8混合精度训练优化；推理节点池选用A10G，侧重能效比。</li>
<li>动态优先级调度：通过Kubernetes的<code>PriorityClass</code>实现任务抢占。例如，高优先级推理任务可抢占低优先级训练资源。</li>
<li>MIG（Multi-Instance GPU）支持：在单个物理GPU（如A100 80GB）上划分多个实例（如7个10GB实例），提升资源利用率。</li>
</ul>
<h3 id="2-高并发与低延迟的实时性保障"><a href="#2-高并发与低延迟的实时性保障" class="headerlink" title="2. 高并发与低延迟的实时性保障"></a>2. 高并发与低延迟的实时性保障</h3><h4 id="挑战深度解析-1"><a href="#挑战深度解析-1" class="headerlink" title="挑战深度解析"></a>挑战深度解析</h4><p>大模型推理服务的SLA通常要求P99延迟低于500ms，但面临以下问题：</p>
<ul>
<li>冷启动延迟：新扩容的Pod需加载数十GB的模型参数，导致首次响应时间（TTFR）可能超过30秒。</li>
<li>流量突增的雪崩效应：突发请求可能导致GPU显存耗尽，触发级联故障。</li>
</ul>
<h4 id="AKS的适配策略-1"><a href="#AKS的适配策略-1" class="headerlink" title="AKS的适配策略"></a>AKS的适配策略</h4><p>分层弹性扩缩体系<br>AKS通过多级扩缩机制保障服务稳定性：</p>
<ol>
<li>Pod层：基于自定义指标的HPA（Horizontal Pod Autoscaler）</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: autoscaling/v2</span><br><span class="line">kind: HorizontalPodAutoscaler</span><br><span class="line">metadata:</span><br><span class="line">  name: llama-inference-hpa</span><br><span class="line">spec:</span><br><span class="line">  behavior:</span><br><span class="line">    scaleDown:</span><br><span class="line">      stabilizationWindowSeconds: 300  # 防止抖动</span><br><span class="line">  metrics:</span><br><span class="line">  - type: External</span><br><span class="line">    external:</span><br><span class="line">      metric:</span><br><span class="line">        name: azureml|inference_latency</span><br><span class="line">      target:</span><br><span class="line">        type: AverageValue</span><br><span class="line">        averageValue: 400  # 目标P99延迟400ms</span><br></pre></td></tr></table></figure>

<ol>
<li>节点层：Cluster Autoscaler与节点预热的协同</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 配置节点池预热缓存</span><br><span class="line">az aks nodepool update \</span><br><span class="line">    --cluster-autoscaler-profile \</span><br><span class="line">    expander=priority \</span><br><span class="line">    scale-down-delay-after-add=10m</span><br></pre></td></tr></table></figure>

<ol>
<li>突发层：虚拟节点（Azure Container Instances）秒级扩容</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">annotations:</span><br><span class="line">  kubernetes.azure.com/scalesetpriority: spot  # 使用Spot实例降低成本</span><br><span class="line">  kubernetes.azure.com/tolerate-unready-endpoints: &quot;true&quot;</span><br></pre></td></tr></table></figure>

<p>实时性优化技术：</p>
<ul>
<li>模型预热：通过Init Container预加载模型至显存。</li>
<li>显存池化：使用NVIDIA Triton的显存共享功能，减少重复加载开销。</li>
<li>RDMA网络：在训练节点池启用SR-IOV，实现GPU直通通信。</li>
</ul>
<h3 id="3-成本优化与资源利用效率"><a href="#3-成本优化与资源利用效率" class="headerlink" title="3. 成本优化与资源利用效率"></a>3. 成本优化与资源利用效率</h3><h4 id="挑战深度解析-2"><a href="#挑战深度解析-2" class="headerlink" title="挑战深度解析"></a>挑战深度解析</h4><p>GPU资源成本通常占大模型运营成本的70%以上，主要浪费场景包括：</p>
<ul>
<li>资源闲置：训练任务完成后，GPU节点仍按需计费。</li>
<li>规格错配：选用过高配置的GPU型号（如使用V100运行轻量推理）。</li>
<li>竞价实例中断：Spot实例被回收导致训练任务中断。</li>
</ul>
<h4 id="AKS的适配策略-2"><a href="#AKS的适配策略-2" class="headerlink" title="AKS的适配策略"></a>AKS的适配策略</h4><p>混合计费模式与智能调度<br>AKS支持在同一集群中混用多种计费类型实例：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 创建Spot节点池（成本降低90%）</span><br><span class="line">az aks nodepool add \</span><br><span class="line">    --name spot-gpu-pool \</span><br><span class="line">    --priority Spot \</span><br><span class="line">    --eviction-policy Delete \</span><br><span class="line">    --spot-max-price 0.2  # 设置最高出价</span><br></pre></td></tr></table></figure>

<p>成本控制关键技术：</p>
<ol>
<li>自动缩容至零：结合KEDA（Kubernetes Event-driven Autoscaling）实现无人值守缩容：</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">triggers:</span><br><span class="line">  - type: azure-servicebus</span><br><span class="line">    metadata:</span><br><span class="line">      topicName: inference-requests</span><br><span class="line">      subscriptionName: $SUBSCRIPTION_NAME</span><br><span class="line">      activationThreshold: &quot;0&quot;  # 无消息时缩容至零</span><br></pre></td></tr></table></figure>

<ol>
<li>资源超卖（Overcommit）：通过vGPU技术实现显存超分配：</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 在nvidia-device-plugin中配置超卖比例</span><br><span class="line">args: [&quot;--mig-strategy=shared&quot;, &quot;--sharing-size=4&quot;]</span><br></pre></td></tr></table></figure>

<ol>
<li>断点续训：当Spot实例被回收时，自动保存检查点到Azure Blob存储，并在新节点恢复训练。</li>
</ol>
<p>AKS通过以下架构设计解决大模型部署的黄金三角问题（性能、弹性、成本）：</p>
<ul>
<li>异构资源池化：物理GPU、vGPU、虚拟节点的多层次抽象。</li>
<li>弹性控制平面：从Pod到节点再到云服务的三级扩缩联动。</li>
<li>智能调度策略：基于实时指标的预测性扩缩（如通过时间序列分析预判流量高峰）。</li>
</ul>
<p>通过上述机制，某头部AI公司在AKS上部署千亿参数模型后，实现推理成本降低40%，训练任务中断率从15%降至0.3%，充分验证了该方案的工程可行性。</p>
<h2 id="二、部署流程：从GPU节点池到模型服务"><a href="#二、部署流程：从GPU节点池到模型服务" class="headerlink" title="二、部署流程：从GPU节点池到模型服务"></a>二、部署流程：从GPU节点池到模型服务</h2><h4 id="1-创建GPU节点池：精准匹配硬件需求"><a href="#1-创建GPU节点池：精准匹配硬件需求" class="headerlink" title="1. 创建GPU节点池：精准匹配硬件需求"></a>1. 创建GPU节点池：精准匹配硬件需求</h4><p>GPU节点池是大模型运行的物理基础，需根据工作负载特性精细化配置。</p>
<p>关键配置参数解析：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">az aks nodepool add \</span><br><span class="line">    --resource-group myResourceGroup \</span><br><span class="line">    --cluster-name myAKSCluster \</span><br><span class="line">    --name gpu-pool \                     # 节点池标识</span><br><span class="line">    --node-count 1 \                      # 初始节点数</span><br><span class="line">    --node-vm-size Standard_NC24ads_A100_v4 \ # 选择GPU机型</span><br><span class="line">    --node-taints sku=gpu:NoSchedule \     # 污点隔离非GPU负载</span><br><span class="line">    --enable-cluster-autoscaler \          # 启用节点自动扩缩</span><br><span class="line">    --min-count 1 \                       # 最小节点数（防止缩容到零影响驱动）</span><br><span class="line">    --max-count 10 \                      # 最大节点数（根据配额调整）</span><br><span class="line">    --zones 1 2 3 \                       # 跨可用区部署提升可用性</span><br><span class="line">    --tags &quot;workload=llm-inference&quot;        # 资源标签便于管理</span><br></pre></td></tr></table></figure>

<p>技术选型要点：</p>
<ul>
<li>GPU机型选择：<ul>
<li>训练场景：优先选用A100&#x2F;H100（显存≥40GB）的机型（如NDm A100 v4系列）</li>
<li>推理场景：考虑T4&#x2F;V100（适合低精度推理）或启用MIG的A100</li>
<li>参考Azure VM规格表：<a target="_blank" rel="noopener" href="https://learn.microsoft.com/azure/virtual-machines/sizes-gpu">https://learn.microsoft.com/azure/virtual-machines/sizes-gpu</a></li>
</ul>
</li>
</ul>
<p>自动扩缩策略：</p>
<ul>
<li>bash复制–cluster-autoscaler-profile “scan-interval&#x3D;30s,scale-down-unneeded-time&#x3D;5m”<ul>
<li><code>scan-interval</code>：扩缩决策频率（默认10s，高敏感场景可缩短）</li>
<li><code>scale-down-unneeded-time</code>：节点闲置回收阈值（避免短时波动误删节点）</li>
</ul>
</li>
</ul>
<p>污点与容忍度设计：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># Pod模板中需添加容忍度才能调度到GPU节点</span><br><span class="line">tolerations:</span><br><span class="line">  - key: &quot;sku&quot;</span><br><span class="line">    operator: &quot;Equal&quot;</span><br><span class="line">    value: &quot;gpu&quot;</span><br><span class="line">    effect: &quot;NoSchedule&quot;</span><br></pre></td></tr></table></figure>

<p>通过污点机制隔离非GPU任务，确保关键负载独占资源</p>
<h4 id="2-安装NVIDIA设备插件：解锁GPU能力"><a href="#2-安装NVIDIA设备插件：解锁GPU能力" class="headerlink" title="2. 安装NVIDIA设备插件：解锁GPU能力"></a>2. 安装NVIDIA设备插件：解锁GPU能力</h4><p>AKS通过Kubernetes Device Plugin机制暴露GPU资源，需部署以下组件：</p>
<p>a. 标准安装流程（DaemonSet方式）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"># nvidia-device-plugin.yaml</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: DaemonSet</span><br><span class="line">metadata:</span><br><span class="line">  name: nvidia-device-plugin-daemonset</span><br><span class="line">  namespace: kube-system</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      name: nvidia-device-plugin</span><br><span class="line">  updateStrategy:</span><br><span class="line">    type: RollingUpdate</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        name: nvidia-device-plugin</span><br><span class="line">    spec:</span><br><span class="line">      tolerations:</span><br><span class="line">        - key: &quot;sku&quot;</span><br><span class="line">          operator: &quot;Equal&quot;</span><br><span class="line">          value: &quot;gpu&quot;</span><br><span class="line">          effect: &quot;NoSchedule&quot;</span><br><span class="line">      priorityClassName: system-node-critical</span><br><span class="line">      containers:</span><br><span class="line">        - image: nvcr.io/nvidia/k8s-device-plugin:v0.15.0</span><br><span class="line">          name: nvidia-device-plugin</span><br><span class="line">          securityContext:</span><br><span class="line">            allowPrivilegeEscalation: false</span><br><span class="line">            capabilities:</span><br><span class="line">              drop: [&quot;ALL&quot;]</span><br><span class="line">          volumeMounts:</span><br><span class="line">            - name: device-plugin</span><br><span class="line">              mountPath: /var/lib/kubelet/device-plugins</span><br><span class="line">      volumes:</span><br><span class="line">        - name: device-plugin</span><br><span class="line">          hostPath:</span><br><span class="line">            path: /var/lib/kubelet/device-plugins</span><br><span class="line">      nodeSelector:</span><br><span class="line">        kubernetes.azure.com/accelerator: nvidia # 限定GPU节点</span><br></pre></td></tr></table></figure>

<p>b. 高级模式：NVIDIA GPU Operator<br>对于生产环境，推荐使用GPU Operator统一管理驱动、监控等组件：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">helm install --wait --generate-name \</span><br><span class="line">    -n gpu-operator \</span><br><span class="line">    --create-namespace \</span><br><span class="line">    nvidia/gpu-operator \</span><br><span class="line">    --set driver.enabled=true \</span><br><span class="line">    --set migManager.enabled=true \</span><br><span class="line">    --set toolkit.enabled=true</span><br></pre></td></tr></table></figure>

<p>Operator优势：</p>
<ul>
<li>自动化驱动版本管理</li>
<li>集成DCGM Exporter实现细粒度监控</li>
<li>支持MIG（多实例GPU）动态分区</li>
<li>提供GPU拓扑感知调度</li>
</ul>
<p>验证安装：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl describe node &lt;gpu-node&gt; | grep nvidia.com/gpu</span><br><span class="line"># 应显示可用GPU数量</span><br><span class="line">kubectl get pod -n gpu-operator # 查看Operator组件状态</span><br></pre></td></tr></table></figure>



<h4 id="3-部署模型服务：资源约束与调度策略"><a href="#3-部署模型服务：资源约束与调度策略" class="headerlink" title="3. 部署模型服务：资源约束与调度策略"></a>3. 部署模型服务：资源约束与调度策略</h4><p>大模型服务的部署需精确控制资源分配，防止资源争抢。</p>
<p>a. Deployment资源配置示例：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: llama2-70b-inference</span><br><span class="line">spec:</span><br><span class="line">  replicas: 2</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: llama2-inference</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: llama2-inference</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">        - name: infer-container</span><br><span class="line">          image: llama2-70b-inference:latest</span><br><span class="line">          resources:</span><br><span class="line">            limits:</span><br><span class="line">              nvidia.com/gpu: 2            # 申请2个GPU卡</span><br><span class="line">              memory: &quot;120Gi&quot;              # 显存+内存总和</span><br><span class="line">            requests:</span><br><span class="line">              nvidia.com/gpu: 2</span><br><span class="line">              memory: &quot;120Gi&quot;</span><br><span class="line">          env:</span><br><span class="line">            - name: CUDA_VISIBLE_DEVICES   # 显式指定GPU序号</span><br><span class="line">              value: &quot;0,1&quot;</span><br><span class="line">      affinity:</span><br><span class="line">        nodeAffinity:</span><br><span class="line">          requiredDuringSchedulingIgnoredDuringExecution:</span><br><span class="line">            nodeSelectorTerms:</span><br><span class="line">              - matchExpressions:</span><br><span class="line">                  - key: sku</span><br><span class="line">                    operator: In</span><br><span class="line">                    values: [&quot;gpu&quot;]</span><br><span class="line">        podAntiAffinity:                  # 反亲和性分散部署</span><br><span class="line">          preferredDuringSchedulingIgnoredDuringExecution:</span><br><span class="line">            - weight: 100</span><br><span class="line">              podAffinityTerm:</span><br><span class="line">                labelSelector:</span><br><span class="line">                  matchExpressions:</span><br><span class="line">                    - key: app</span><br><span class="line">                      operator: In</span><br><span class="line">                      values: [&quot;llama2-inference&quot;]</span><br><span class="line">                topologyKey: kubernetes.io/hostname</span><br></pre></td></tr></table></figure>

<p>关键配置解析：</p>
<ul>
<li>GPU资源声明：<ul>
<li><code>nvidia.com/gpu</code>：整卡粒度分配（需配合MIG实现细粒度切分）</li>
<li>必须同时设置<code>limits</code>和<code>requests</code>且相等（防止超卖）</li>
</ul>
</li>
</ul>
<p>显存隔离：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 启用MIG切分（需GPU支持）</span><br><span class="line">resources:</span><br><span class="line">  limits:</span><br><span class="line">    nvidia.com/mig-1g.5gb: 6 # 每个Pod使用6个5GB显存实例</span><br></pre></td></tr></table></figure>

<ul>
<li>高级调度策略：<ul>
<li>节点亲和性：强制调度到GPU节点池</li>
<li>Pod反亲和性：避免单节点部署多个副本导致资源争抢</li>
<li>拓扑分布约束：优化跨NUMA节点的GPU通信</li>
</ul>
</li>
</ul>
<p>b. 服务暴露与负载均衡</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: llama2-service</span><br><span class="line">spec:</span><br><span class="line">  type: LoadBalancer                      # 使用Azure LB</span><br><span class="line">  ports:</span><br><span class="line">    - port: 80</span><br><span class="line">      targetPort: 5000</span><br><span class="line">      name: http</span><br><span class="line">  selector:</span><br><span class="line">    app: llama2-inference</span><br><span class="line">---</span><br><span class="line">apiVersion: networking.k8s.io/v1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: llama2-ingress</span><br><span class="line">  annotations:</span><br><span class="line">    kubernetes.io/ingress.class: azure/application-gateway # 使用AGIC</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">    - http:</span><br><span class="line">        paths:</span><br><span class="line">          - path: /v1/completions</span><br><span class="line">            pathType: Prefix</span><br><span class="line">            backend:</span><br><span class="line">              service:</span><br><span class="line">                name: llama2-service</span><br><span class="line">                port:</span><br><span class="line">                  number: 80</span><br></pre></td></tr></table></figure>

<p>网络优化技巧：</p>
<ul>
<li>启用Accelerated Networking：提升网络吞吐</li>
<li>使用RDMA over Converged Ethernet (RoCE)：降低GPU间通信延迟</li>
<li>部署Kubernetes Network Policies隔离流量</li>
</ul>
<h4 id="4-部署验证与调试"><a href="#4-部署验证与调试" class="headerlink" title="4. 部署验证与调试"></a>4. 部署验证与调试</h4><p>a. 基础检查：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 查看Pod调度状态</span><br><span class="line">kubectl get pod -o wide --watch</span><br><span class="line"></span><br><span class="line"># 检查GPU资源分配</span><br><span class="line">kubectl describe pod &lt;pod-name&gt; | grep -i gpu</span><br><span class="line"></span><br><span class="line"># 进入容器验证CUDA可见性</span><br><span class="line">kubectl exec -it &lt;pod-name&gt; -- nvidia-smi</span><br></pre></td></tr></table></figure>

<p>b. 性能基准测试：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 运行深度学习基准工具</span><br><span class="line">kubectl exec -it &lt;pod-name&gt; -- \</span><br><span class="line">    nvidia-docker run --rm \</span><br><span class="line">    nvcr.io/nvidia/tensorflow:23.07-tf2-py3 \</span><br><span class="line">    python -c &quot;import tensorflow as tf; print(tf.config.list_physical_devices(&#x27;GPU&#x27;))&quot;</span><br></pre></td></tr></table></figure>

<p>c. 日志与监控接入：</p>
<ul>
<li>配置Azure Monitor收集GPU指标</li>
<li>集成Prometheus + Grafana实现自定义监控看板</li>
<li>使用Kubectl Debug工具进行实时诊断</li>
</ul>
<h4 id="技术难点与解决方案"><a href="#技术难点与解决方案" class="headerlink" title="技术难点与解决方案"></a>技术难点与解决方案</h4><table>
<thead>
<tr>
<th>问题场景</th>
<th>解决方案</th>
</tr>
</thead>
<tbody><tr>
<td>Pod因OOM被杀</td>
<td>设置显存limits + 启用Memory Manager（K8s 1.22+）</td>
</tr>
<tr>
<td>GPU利用率低但无法缩容</td>
<td>配置HPA基于请求队列长度扩缩，而非直接GPU指标</td>
</tr>
<tr>
<td>多模型混合部署资源争抢</td>
<td>使用Kubernetes ResourceQuota + 优先级抢占机制</td>
</tr>
<tr>
<td>节点就绪耗时过长</td>
<td>预构建GPU节点系统镜像 + 启用Node Startup Taint控制调度</td>
</tr>
</tbody></table>
<p>通过以上流程，可在AKS上构建弹性、高可用的大模型服务架构，下一章节将深入探讨如何通过自动扩缩策略实现资源利用率与成本的动态平衡。</p>
<h2 id="三、自动化扩缩策略深度解析"><a href="#三、自动化扩缩策略深度解析" class="headerlink" title="三、自动化扩缩策略深度解析"></a>三、自动化扩缩策略深度解析</h2><p>在部署大模型时，自动化扩缩是平衡资源利用率、服务稳定性和成本的核心手段。AKS提供了从Pod到节点的多层次弹性调度能力，以下为具体实现方案与技术细节：</p>
<h4 id="1-水平Pod自动扩缩（HPA）：精细化控制推理负载"><a href="#1-水平Pod自动扩缩（HPA）：精细化控制推理负载" class="headerlink" title="1. 水平Pod自动扩缩（HPA）：精细化控制推理负载"></a>1. 水平Pod自动扩缩（HPA）：精细化控制推理负载</h4><p>HPA通过动态调整Pod副本数应对流量波动，需结合GPU利用率、业务指标等多维度数据。</p>
<p>典型配置流程：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"># 基于自定义GPU指标的HPA定义（需安装Metrics Server和Prometheus Adapter）</span><br><span class="line">apiVersion: autoscaling/v2</span><br><span class="line">kind: HorizontalPodAutoscaler</span><br><span class="line">metadata:</span><br><span class="line">  name: llama-inference-hpa</span><br><span class="line">spec:</span><br><span class="line">  scaleTargetRef:</span><br><span class="line">    apiVersion: apps/v1</span><br><span class="line">    kind: Deployment</span><br><span class="line">    name: llama-7b-inference</span><br><span class="line">  minReplicas: 2</span><br><span class="line">  maxReplicas: 20</span><br><span class="line">  behavior:</span><br><span class="line">    scaleDown:</span><br><span class="line">      stabilizationWindowSeconds: 300  # 缩容冷却时间</span><br><span class="line">      policies:</span><br><span class="line">        - type: Percent</span><br><span class="line">          value: 50</span><br><span class="line">          periodSeconds: 60</span><br><span class="line">  metrics:</span><br><span class="line">    - type: Resource</span><br><span class="line">      resource:</span><br><span class="line">        name: nvidia.com/gpu</span><br><span class="line">        target:</span><br><span class="line">          type: Utilization</span><br><span class="line">          averageUtilization: 70  # GPU利用率阈值</span><br><span class="line">    - type: External</span><br><span class="line">      external:</span><br><span class="line">        metric:</span><br><span class="line">          name: http_requests_per_second</span><br><span class="line">          selector:</span><br><span class="line">            matchLabels:</span><br><span class="line">              service: llama-inference</span><br><span class="line">        target:</span><br><span class="line">          type: AverageValue</span><br><span class="line">          averageValue: 1000       # QPS阈值</span><br></pre></td></tr></table></figure>

<p>关键优化技巧：</p>
<ul>
<li>冷热副本分级管理<br>通过Pod优先级（PriorityClass）区分常驻副本（Hot Replicas）与弹性副本（Cold Replicas），优先扩缩低优先级Pod以降低延迟。</li>
</ul>
<p>基于队列长度的弹性触发<br>集成KEDA（Kubernetes Event-Driven Autoscaler）监听消息队列（如Azure Service Bus），当积压请求数超过阈值时触发扩容：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># KEDA ScaledObject示例</span><br><span class="line">apiVersion: keda.sh/v1alpha1</span><br><span class="line">kind: ScaledObject</span><br><span class="line">metadata:</span><br><span class="line">  name: inference-queue-scaler</span><br><span class="line">spec:</span><br><span class="line">  scaleTargetRef:</span><br><span class="line">    name: llama-inference</span><br><span class="line">  triggers:</span><br><span class="line">    - type: azure-servicebus</span><br><span class="line">      metadata:</span><br><span class="line">        queueName: inference-queue</span><br><span class="line">        messageCount: &quot;50&quot;  # 每副本处理50条消息</span><br><span class="line">  advanced:</span><br><span class="line">    horizontalPodAutoscalerConfig:</span><br><span class="line">      behavior:  # 覆盖默认扩缩行为</span><br><span class="line">        scaleDown:</span><br><span class="line">          stabilizationWindowSeconds: 180</span><br></pre></td></tr></table></figure>

<p>GPU显存动态感知<br>自定义指标采集显存使用率，当显存压力超过80%时触发扩容，避免OOM（Out of Memory）错误：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 使用DcgmExporter采集显存指标</span><br><span class="line">kubectl apply -f https://raw.githubusercontent.com/NVIDIA/dcgm-exporter/main/kubernetes/dcgm-exporter.yaml</span><br></pre></td></tr></table></figure>

<hr>
<h4 id="2-群集自动扩缩（Cluster-Autoscaler）：节点层弹性扩展"><a href="#2-群集自动扩缩（Cluster-Autoscaler）：节点层弹性扩展" class="headerlink" title="2. 群集自动扩缩（Cluster Autoscaler）：节点层弹性扩展"></a>2. 群集自动扩缩（Cluster Autoscaler）：节点层弹性扩展</h4><p>Cluster Autoscaler（CA）负责根据Pod调度需求自动增减节点，需与HPA协同工作。</p>
<p>节点池配置策略：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 配置GPU节点池的扩缩参数（支持多可用区平衡）</span><br><span class="line">az aks nodepool update \</span><br><span class="line">    --resource-group myResourceGroup \</span><br><span class="line">    --cluster-name myAKSCluster \</span><br><span class="line">    --name gpu-pool \</span><br><span class="line">    --min-count 1 \</span><br><span class="line">    --max-count 20 \</span><br><span class="line">    --cluster-autoscaler-profile &quot;scan-interval=30s,expander=random,skip-nodes-with-local-storage=false&quot;</span><br></pre></td></tr></table></figure>

<p>参数详解：</p>
<ul>
<li><code>scan-interval</code>：默认10秒，缩短至30秒可降低API负载。</li>
<li><code>expander</code>：扩容策略选择（<code>random</code>&#x2F;<code>most-pods</code>&#x2F;<code>priority</code>），大模型场景建议<code>priority</code>按权重选择节点池。</li>
<li><code>skip-nodes-with-local-storage</code>：设为<code>false</code>允许调度使用本地存储的Pod。</li>
</ul>
<p>高级场景处理：</p>
<ul>
<li>突发扩容预热<br>预配置空闲节点（通过<code>--node-count</code>），减少冷启动延迟，适用于定时批量推理任务。</li>
</ul>
<p>GPU节点优雅缩容<br>配置PodDisruptionBudget（PDB）防止缩容时服务中断：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: policy/v1</span><br><span class="line">kind: PodDisruptionBudget</span><br><span class="line">metadata:</span><br><span class="line">  name: gpu-pdb</span><br><span class="line">spec:</span><br><span class="line">  minAvailable: 80%  # 至少保留80%的Pod在线</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: llama-inference</span><br></pre></td></tr></table></figure>



<h4 id="3-虚拟节点与突发扩缩：秒级弹性能力"><a href="#3-虚拟节点与突发扩缩：秒级弹性能力" class="headerlink" title="3. 虚拟节点与突发扩缩：秒级弹性能力"></a>3. 虚拟节点与突发扩缩：秒级弹性能力</h4><p>当常规节点池扩容无法满足突发需求时，AKS虚拟节点（基于Azure Container Instances，ACI）可提供无服务器化弹性。</p>
<p>实施步骤：</p>
<p>启用虚拟节点附加组件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">az aks enable-addons \</span><br><span class="line">    --name myAKSCluster \</span><br><span class="line">    --resource-group myResourceGroup \</span><br><span class="line">    --addons virtual-node \</span><br><span class="line">    --subnet-name myVirtualSubnet</span><br></pre></td></tr></table></figure>

<p>标记虚拟节点调度规则</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># Pod调度到虚拟节点的配置</span><br><span class="line">spec:</span><br><span class="line">  nodeSelector:</span><br><span class="line">    kubernetes.azure.com/aci: &quot;true&quot;</span><br><span class="line">  tolerations:</span><br><span class="line">    - key: virtual-node.azure.com/aci</span><br><span class="line">      operator: Exists</span><br><span class="line">  resources:</span><br><span class="line">    limits:</span><br><span class="line">      nvidia.com/gpu: 1  # ACI当前仅支持部分GPU型号</span><br></pre></td></tr></table></figure>

<p>适用场景对比：</p>
<table>
<thead>
<tr>
<th>场景</th>
<th>常规节点池</th>
<th>虚拟节点（ACI）</th>
</tr>
</thead>
<tbody><tr>
<td>扩容速度</td>
<td>2-5分钟（VM启动+驱动加载）</td>
<td>10-30秒（容器直接启动）</td>
</tr>
<tr>
<td>GPU型号支持</td>
<td>全系列（H100&#x2F;A100等）</td>
<td>有限（如NC6系列）</td>
</tr>
<tr>
<td>成本</td>
<td>按需计费&#x2F;预留实例</td>
<td>按秒计费，无最低消费</td>
</tr>
<tr>
<td>最大Pod数</td>
<td>受节点池规模限制</td>
<td>单Pod最大4 GPU，区域配额限制</td>
</tr>
</tbody></table>
<p>注意事项：</p>
<ul>
<li>ACI的GPU实例需在特定区域可用，需提前验证。</li>
<li>虚拟节点不支持PersistentVolume，需使用Azure Files等远程存储。</li>
</ul>
<h4 id="4-跨层联动优化：HPA-CA-Virtual-Node"><a href="#4-跨层联动优化：HPA-CA-Virtual-Node" class="headerlink" title="4. 跨层联动优化：HPA + CA + Virtual Node"></a>4. 跨层联动优化：HPA + CA + Virtual Node</h4><p>通过策略组合实现全局弹性，示例工作流：</p>
<ol>
<li>流量激增阶段<ul>
<li>HPA优先扩容Pod副本至当前节点容量上限。</li>
<li>CA检测到Pending Pod，触发节点池扩容。</li>
<li>若节点池扩容速度不足，HPA将Pod调度到虚拟节点。</li>
</ul>
</li>
<li>流量回落阶段<ul>
<li>HPA逐步减少Pod副本，释放虚拟节点资源。</li>
<li>CA检测节点闲置，逐步缩容物理节点池。</li>
<li>最终保留最小常驻节点以保障基线服务。</li>
</ul>
</li>
</ol>
<p>策略调优参数示例：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 混合扩缩参数建议</span><br><span class="line">hpa:</span><br><span class="line">  stabilizationWindowDown: 5m  # 避免过早缩容</span><br><span class="line">  tolerance: 0.3              # 指标波动容忍度</span><br><span class="line">ca:</span><br><span class="line">  scaleDownDelayAfterAdd: 10m # 节点扩容后保护期</span><br><span class="line">  unneededTime: 15m           # 节点闲置判定时间</span><br></pre></td></tr></table></figure>



<h4 id="5-异常处理与熔断机制"><a href="#5-异常处理与熔断机制" class="headerlink" title="5. 异常处理与熔断机制"></a>5. 异常处理与熔断机制</h4><ul>
<li>资源争用熔断<br>当GPU利用率持续超过95%且扩容失败时，自动触发降级策略（如返回简化版模型）。</li>
<li>配额监控告警<br>通过Azure Monitor设置GPU配额预警，防止因区域容量耗尽导致扩缩失败。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 创建GPU配额告警</span><br><span class="line">az monitor metrics alert create \</span><br><span class="line">    --name &quot;GPU Quota Alert&quot; \</span><br><span class="line">    --condition &quot;avg Microsoft.ContainerService/managedClusters/GPUUtilization &gt; 90&quot; \</span><br><span class="line">    --resource-group myResourceGroup \</span><br><span class="line">    --action email admin@example.com</span><br></pre></td></tr></table></figure>



<p>通过上述策略的精细化配置，AKS能够实现大模型服务从毫秒级Pod扩缩到分钟级节点扩容的全链路弹性，在保障SLA的同时最大化资源利用率。实际部署中需结合压力测试结果持续调整阈值参数，并建立完善的监控反馈闭环。</p>
<p>AKS通过深度整合Kubernetes生态与Azure云原生能力，为大规模AI模型的部署提供了从资源调度到成本管控的全栈解决方案。随着AI负载复杂度的提升，结合自动化工具链与持续优化的硬件支持，AKS将在AI工程化实践中持续发挥核心作用。</p>
</div><div class="article-licensing box"><div class="licensing-title"><p>以 Azure Kubernetes 服务（AKS）部署大模型：GPU资源调度与自动扩缩</p><p><a href="https://www.liangyouze.com/2024/02/26/以 Azure Kubernetes 服务（AKS）部署大模型：GPU资源调度与自动扩缩/">https://www.liangyouze.com/2024/02/26/以 Azure Kubernetes 服务（AKS）部署大模型：GPU资源调度与自动扩缩/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>梁友泽</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2024-02-26</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2025-03-30</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/azure/">azure</a></div><!--!--></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>支付宝</span><span class="qrcode"><img src="/img/images/qrcode/Alipay.jpeg" alt="支付宝"></span></a><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="/img/images/qrcode/WeChat.jpeg" alt="微信"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2024/11/24/%E4%BB%A5%E7%BC%96%E7%A8%8B%E6%96%B9%E5%BC%8F%E8%AE%BE%E7%BD%AE%E5%92%8C%E8%BF%90%E8%A1%8C%20Azure%20Prompt%20Flow/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">以编程方式设置和运行 Azure Prompt Flow</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2023/11/05/%E6%88%91%E6%98%AF%E5%A6%82%E4%BD%95%E6%8F%90%E5%8D%87%E6%88%91%E7%9A%84%E6%95%88%E7%8E%87%E7%9A%84/"><span class="level-item">构建自己高效的workflow</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><!--!--></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="梁友泽"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">梁友泽</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>北京</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives/"><p class="title">55</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories/"><p class="title">18</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags/"><p class="title">20</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/youzeliang" target="_blank" rel="me noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Github" href="https://github.com/youzeliang"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://www.liangyongrui.com" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">梁永锐</span></span><span class="level-right"><span class="level-item tag">www.liangyongrui.com</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/MySQL/"><span class="level-start"><span class="level-item">MySQL</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/"><span class="level-start"><span class="level-item">Python</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/azure/"><span class="level-start"><span class="level-item">azure</span></span><span class="level-end"><span class="level-item tag">20</span></span></a></li><li><a class="level is-mobile" href="/categories/docker/"><span class="level-start"><span class="level-item">docker</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/gin/"><span class="level-start"><span class="level-item">gin</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/golang/"><span class="level-start"><span class="level-item">golang</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/kafka/"><span class="level-start"><span class="level-item">kafka</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/map/"><span class="level-start"><span class="level-item">map</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/mysql/"><span class="level-start"><span class="level-item">mysql</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/panic/"><span class="level-start"><span class="level-item">panic</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/string/"><span class="level-start"><span class="level-item">string</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/vim/"><span class="level-start"><span class="level-item">vim</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%86%85%E5%AD%98%E9%80%83%E9%80%B8/"><span class="level-start"><span class="level-item">内存逃逸</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%87%BD%E6%95%B0/"><span class="level-start"><span class="level-item">函数</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%8F%8D%E5%B0%84/"><span class="level-start"><span class="level-item">反射</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%95%88%E7%8E%87/"><span class="level-start"><span class="level-item">效率</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%9D%82%E8%B0%88/"><span class="level-start"><span class="level-item">杂谈</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%A7%84%E8%8C%83/"><span class="level-start"><span class="level-item">规范</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-03-20T13:10:23.000Z">2025-03-20</time></p><p class="title"><a href="/2025/03/20/AI%E9%A9%B1%E5%8A%A8%E7%9A%84%E7%BC%96%E8%AF%91%E5%99%A8%E4%BC%98%E5%8C%96%EF%BC%9AAzure%E4%B8%8ELLVM%E7%9A%84%E8%87%AA%E5%8A%A8%E5%8C%96%E4%BB%A3%E7%A0%81%E5%8A%A0%E9%80%9F%E6%96%B9%E6%A1%88/">AI驱动的编译器优化：Azure与LLVM的自动化代码加速方案</a></p><p class="categories"><a href="/categories/azure/">azure</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-03-15T13:10:23.000Z">2025-03-15</time></p><p class="title"><a href="/2025/03/15/Blackwell%20Ultra%20GPU%E5%9C%A8Azure%20AI%E4%B8%AD%E7%9A%84%E6%9C%AA%E6%9D%A5%E5%B1%95%E6%9C%9B%EF%BC%9A%E4%B8%87%E4%BA%BF%E5%8F%82%E6%95%B0%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83/">Blackwell Ultra GPU在Azure AI中的未来展望：万亿参数模型训练</a></p><p class="categories"><a href="/categories/azure/">azure</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-02-27T12:10:23.000Z">2025-02-27</time></p><p class="title"><a href="/2025/02/27/zure%E4%B8%8ENVIDIA%20Megatron%E7%9A%84%E5%8D%8F%E5%90%8C%E4%BC%98%E5%8C%96%E6%96%B9%E6%A1%88/">zure与NVIDIA Megatron的协同优化方案</a></p><p class="categories"><a href="/categories/azure/">azure</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-02-19T13:10:23.000Z">2025-02-19</time></p><p class="title"><a href="/2025/02/19/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E9%AB%98%E5%8F%AF%E6%89%A9%E5%B1%95%E6%80%A7%E5%8F%8A%E5%85%B6%E5%9C%A8%20Azure%20%E4%B8%AD%E7%9A%84%E5%AE%9E%E7%8E%B0/">深入理解高可扩展性及其在 Azure 中的实现</a></p><p class="categories"><a href="/categories/azure/">azure</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-02-12T13:10:23.000Z">2025-02-12</time></p><p class="title"><a href="/2025/02/12/Azure%E6%97%A0%E6%9C%8D%E5%8A%A1%E5%99%A8GPU%E5%AE%9E%E6%88%98%EF%BC%9A%E4%BD%8E%E6%88%90%E6%9C%AC%E8%BF%90%E8%A1%8C%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B/">Azure无服务器GPU实战：低成本运行多模态大模型</a></p><p class="categories"><a href="/categories/azure/">azure</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">归档</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2025/03/"><span class="level-start"><span class="level-item">三月 2025</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2025/02/"><span class="level-start"><span class="level-item">二月 2025</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2025/01/"><span class="level-start"><span class="level-item">一月 2025</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/12/"><span class="level-start"><span class="level-item">十二月 2024</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/11/"><span class="level-start"><span class="level-item">十一月 2024</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/02/"><span class="level-start"><span class="level-item">二月 2024</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/11/"><span class="level-start"><span class="level-item">十一月 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/05/"><span class="level-start"><span class="level-item">五月 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/06/"><span class="level-start"><span class="level-item">六月 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/05/"><span class="level-start"><span class="level-item">五月 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/04/"><span class="level-start"><span class="level-item">四月 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/09/"><span class="level-start"><span class="level-item">九月 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/03/"><span class="level-start"><span class="level-item">三月 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/01/"><span class="level-start"><span class="level-item">一月 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/10/"><span class="level-start"><span class="level-item">十月 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/08/"><span class="level-start"><span class="level-item">八月 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/07/"><span class="level-start"><span class="level-item">七月 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/04/"><span class="level-start"><span class="level-item">四月 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/02/"><span class="level-start"><span class="level-item">二月 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/01/"><span class="level-start"><span class="level-item">一月 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/12/"><span class="level-start"><span class="level-item">十二月 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/10/"><span class="level-start"><span class="level-item">十月 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/09/"><span class="level-start"><span class="level-item">九月 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/07/"><span class="level-start"><span class="level-item">七月 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/06/"><span class="level-start"><span class="level-item">六月 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/03/"><span class="level-start"><span class="level-item">三月 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/02/"><span class="level-start"><span class="level-item">二月 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/01/"><span class="level-start"><span class="level-item">一月 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/12/"><span class="level-start"><span class="level-item">十二月 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/10/"><span class="level-start"><span class="level-item">十月 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/09/"><span class="level-start"><span class="level-item">九月 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/08/"><span class="level-start"><span class="level-item">八月 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/07/"><span class="level-start"><span class="level-item">七月 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/05/"><span class="level-start"><span class="level-item">五月 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/04/"><span class="level-start"><span class="level-item">四月 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/03/"><span class="level-start"><span class="level-item">三月 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2017/11/"><span class="level-start"><span class="level-item">十一月 2017</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Docker/"><span class="tag">Docker</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/MySQL/"><span class="tag">MySQL</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Python/"><span class="tag">Python</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/azure/"><span class="tag">azure</span><span class="tag">20</span></a></div><div class="control"><a class="tags has-addons" href="/tags/context/"><span class="tag">context</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/elasticsearch/"><span class="tag">elasticsearch</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/go/"><span class="tag">go</span><span class="tag">9</span></a></div><div class="control"><a class="tags has-addons" href="/tags/golang/"><span class="tag">golang</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/iframe/"><span class="tag">iframe</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/kafka/"><span class="tag">kafka</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/vim/"><span class="tag">vim</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%B8%AA%E4%BA%BA/"><span class="tag">个人</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90/"><span class="tag">内存对齐</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2/"><span class="tag">字符串</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%9D%82%E8%B0%88/"><span class="tag">杂谈</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%B4%A2%E5%BC%95/"><span class="tag">索引</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%A7%84%E5%88%99/"><span class="tag">规则</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AE%A2%E5%8D%95%E5%8F%B7/"><span class="tag">订单号</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AF%BB%E4%B9%A6/"><span class="tag">读书</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%B7%A8%E5%9F%9F/"><span class="tag">跨域</span><span class="tag">1</span></a></div></div></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="梁友泽的博客" height="28"></a><p class="is-size-7"><span>&copy; 2025 梁友泽</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">共<span id="busuanzi_value_site_uv">0</span>个访客</span></p></div><div class="level-end"></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-cn");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script data-pjax src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script data-pjax src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js"></script><script src="/js/pjax.js"></script><!--!--><!--!--><!--!--><script data-pjax src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script data-pjax src="/js/insight.js" defer></script><script data-pjax>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>