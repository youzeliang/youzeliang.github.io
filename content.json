{"posts":[{"title":"go panic探索","text":"panic 发生之后，如果 Go 不做任何特殊处理，默认行为是打印堆栈，退出程序。 panic 到底是什么？ panic( ) 函数内部会产生一个关键的数据结构体 _panic ，并且挂接到 goroutine 之上； panic( ) 函数内部会执行 _defer 函数链条，并针对 _panic 的状态进行对应的处理； 什么叫做 panic( ) 的对应的处理？ 循环执行 goroutine 上面的 _defer 函数链，如果执行完了都还没有恢复 _panic 的状态，那就没得办法了，退出进程，打印堆栈。如果在 goroutine 的 _defer 链上，有个朋友 recover 了一下，把这个 _panic 标记成恢复，那事情就到此为止，就从这个 _defer 函数执行后续正常代码即可，走 deferreturn 的逻辑。 recover 函数 recover 对应了 runtime/panic.go 中的 gorecover 函数实现。 12345678910func gorecover(argp uintptr) interface{} { // 只处理 gp._panic 链表最新的这个 _panic； gp := getg() p := gp._panic if p != nil &amp;&amp; !p.recovered &amp;&amp; argp == uintptr(p.argp) { p.recovered = true return p.arg } return nil} 这个函数可太简单了： 取出当前 goroutine 结构体； 取出当前 goroutine 的 _panic 链表最新的一个 _panic，如果是非 nil 值，则进行处理； 该 _panic 结构体的 recovered 赋值 true，程序返回； 这就是 recover 函数的全部内容，只给 _panic.recovered 赋值而已，不涉及代码的神奇跳转。而 _panic.recovered 的赋值是在 panic 函数逻辑中发挥作用。 panic函数 panic 的实现在一个叫做 gopanic 的函数，位于 runtime/panic.go 文件。panic 机制最重要最重要的就是 gopanic 函数了，所有的 panic 细节尽在此。为什么 panic 会显得晦涩，主要有两个点： 嵌套 panic 的时候，gopanic 会有递归执行的场景； 程序指令跳转并不是常规的函数压栈，弹栈，在 recovery 的时候，是直接修改指令寄存器的结构体，从而直接越过了 gopanic 后面的逻辑，甚至是多层 gopanic 递归的逻辑； 一切秘密都在下面这个函数： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263// runtime/panic.gofunc gopanic(e interface{}) { // 在栈上分配一个 _panic 结构体 var p _panic // 把当前最新的 _panic 挂到链表最前面 p.link = gp._panic gp._panic = (*_panic)(noescape(unsafe.Pointer(&amp;p))) for { // 取出当前最近的 defer 函数； d := gp._defer if d == nil { // 如果没有 defer ，那就没有 recover 的时机，只能跳到循环外，退出进程了； break } // 进到这个逻辑，那说明了之前是有 panic 了，现在又有 panic 发生，这里一定处于递归之中； if d.started { if d._panic != nil { d._panic.aborted = true } // 把这个 defer 从链表中摘掉； gp._defer = d.link freedefer(d) continue } // 标记 _defer 为 started = true （panic 递归的时候有用） d.started = true // 记录当前 _defer 对应的 panic d._panic = (*_panic)(noescape(unsafe.Pointer(&amp;p))) // 执行 defer 函数 reflectcall(nil, unsafe.Pointer(d.fn), deferArgs(d), uint32(d.siz), uint32(d.siz)) // defer 执行完成，把这个 defer 从链表里摘掉； gp._defer = d.link // 取出 pc，sp 寄存器的值； pc := d.pc sp := unsafe.Pointer(d.sp) // 如果 _panic 被设置成恢复，那么到此为止； if p.recovered { // 摘掉当前的 _panic gp._panic = p.link // 如果前面还有 panic，并且是标记了 aborted 的，那么也摘掉； for gp._panic != nil &amp;&amp; gp._panic.aborted { gp._panic = gp._panic.link } // panic 的流程到此为止，恢复到业务函数堆栈上执行代码； gp.sigcode0 = uintptr(sp) gp.sigcode1 = pc // 注意：恢复的时候 panic 函数将从此处跳出，本 gopanic 调用结束，后面的代码永远都不会执行。 mcall(recovery) throw(&quot;recovery failed&quot;) // mcall should not return } } // 打印错误信息和堆栈，并且退出进程； preprintpanics(gp._panic) fatalpanic(gp._panic) // should not return *(*int)(nil) = 0 // not reached} 上面逻辑可以拆分为循环内和循环外两部分去理解： 循环内：程序执行 defer，是否恢复正常的指令执行，一切都在循环内决定； 循环外：一旦走到循环外，说明 _panic 没人处理，程序即将退出； for 循环内 循环内的事情拆解成： 遍历 goroutine 的 defer 链表，获取到一个 _defer 延迟函数； 获取到 _defer 延迟函数，设置标识 d.started，绑定当前 d._panic（用以在递归的时候判断）； 执行 _defer 延迟函数； 摘掉执行完的 _defer 函数； 判断 _panic.recovered 是否设置为 true，进行相应操作； 如果是 true 那么重置 pc，sp 寄存器（一般从 deferreturn 指令前开始执行），goroutine 投递到调度队列，等待执行； 重复以上步骤； 问题一：为什么 recover 一定要放在 defer 里面才生效？ 因为，这是唯一的修改 _panic.recovered 字段的时机 ！ 为什么 recover 已经放在 defer 里面，但是进程还是没有恢复？ 划重点：在 gopanic 里，只遍历执行当前 goroutine 上的 _defer 函数链条。所以，如果挂在其他 goroutine 的 defer 函数做了 recover ，那么没有丝毫用途。 例 12345678func main() { // g1 go func() { // g2 defer func() { recover() }() }() panic(&quot;test&quot;)} 因为，panic 和 recover 在两个不同的 goroutine，_panic 是挂在 g1 上的，recover 是在 g2 的 _defer 链条里。gopanic 遍历的是 g1 的 _defer 函数链表，跟 g2 八杆子打不着，g2 的 recover 自然拿不到 g1 的 _panic 结构，自然也不能设置 recovered 为 true ，所以程序还是崩了。 recover 函数在 gopanic 函数中，在循环执行 defer 函数的时候，如果发现 _panic.recovered 字段被设置成 true 的时候，调用 mcall(recovery) 来执行所谓的恢复。 看一眼 recovery 函数的实现，这个函数极其简单，就是恢复 pc，sp 寄存器，重新把 Goroutine 投递到调度队列中。 1234567891011// runtime/panic.gofunc recovery(gp *g) { // 取出栈寄存器和程序计数器的值 sp := gp.sigcode0 pc := gp.sigcode1 // 重置 goroutine 的 pc，sp 寄存器； gp.sched.sp = sp gp.sched.pc = pc // 重新投入调度队列 gogo(&amp;gp.sched)} 总结 panic() 会退出进程，是因为调用了 exit 的系统调用； recover() 所在的 defer 函数必须和 panic 都是挂在同一个goroutine 上，不能跨协程，因为 gopanic 只会执行当前 goroutine 的延迟函数； 参考深度细节 | Go 的 panic 的秘密都在这源码剖析panic与recover","link":"/2021/09/20/%20go%20panic%E6%8E%A2%E7%B4%A2/"},{"title":"2019年总结","text":"今年的最后一段时间是在罗振宇的跨年演讲中度过的，当然不是在现场。 2019是一个有意思的一年，如在北京待的越久，似乎就越不愿意离开了。在这里认识了更多厉害的人，就会看到自己的渺小。自己的认知也在逐渐走向远方，看到更远的视野。在这一年里，美国不断打压中国，我所在的公司也在美国的”名单”里，以及看到各大公司裁员，这似乎就有一种焦虑，被时代驱逐前行的我们，是容不得“舒适”的环境。但你的性格，眼界格局，认知等或许就已经决定了在这个格局变化里，下一个地方会何去何从。有庆幸，自己身处在这行业里，不说那些伟大的事情(普通人完全是扯淡)，但能凭借自己的能力，也能自立自足。但又有些不甘，不甘的是野心和努力不成正比吧。或许还是对自己不够狠。不甘中又有一丝惧怕，似乎看到了所谓的35岁后的自己。或许努力生活的人，都自在执着属性吧。 社会是多元化的，一朵朵海浪中荡漾着不同光芒的思想 感谢 从2017年末yqj就开始鼓励我，今年也是不例外。总是交流了一些超越我现在年龄阶段认知的一些思想，她自己也工作了4-5年后去港大上学了。优秀的人是真的没有停止下来。是最最感激的人之一了 然后就是qmj这个本该3月就该见一面认识的，话说都认识了好久了，一个很乐观，阳光的一个人，交流起来也挺有意思的。以积极向上，玩耍的心态工作(或许没有)生活，同样很感谢 zs这个每天都是打了鸡血的人，太拼了同时也太强了。说实话也影响了我很多，他的成长还是看在眼里的，就是有时候做事有点怪 还有其他一些人 然后回顾下回想一下2019的flag 早睡早起 上半年大概还能做到，然后到下半年的时候就逐渐又回到了之前的状态了，主要是真的早起动力不足哇。 阅读20本+ 书籍 回顾了下，2019年看完了34本书，其中数学(主要是科普类的了，因为再看一些类似于刷题之类的没啥意义了)书籍占了近40%吧，有历史(对，开始看自己以前不喜欢的历史了)，经济，当然还有一些技术类的书籍。总之来看，普遍是一些实用类的，反正是不会看文学类的书籍 参加不少于5场有价值的线下技术会议 大概4场吧，虽然有自己想深入去了解的方向，但是自制力是真的不够，时间眨眼就过去了。至少也算是开拓了眼界，认识到了一些厉害的人。这大概也是鞭挞前进的动力吧 看电影少于20部 好像不知不觉看了60部,这过分了 输出20篇有质量的博客 不到一半吧，还是需要继续沉淀 脱单 这真的是太难了,所以这是要继续作为2020年的目标么 投入一定时间在学习英语上 至少看一些技术文档优先于官网的，以及中途尝试翻译了一些英语技术文章，还有一篇得到了国外的一个技术人员的认可。大部分的时间通过英语文章来了解国际新闻，这当然国内的微博热搜也是一种途径 至少一次一个人的旅行 真没想到这也没完成，那就2020年换公司了来吧 一场音乐会 倒不是有这细胞，只是作为程序员感觉要多接触其他不同方面的东西，这让我想起了最近看的一个知乎问题，作为程序员你失去了什么，专注在自己喜欢的领域里是值得肯定的，但是圈子外面的世界也会是很精彩的 开始了解经济,股票 今年的收益率还是行了吧，投入的钱不多 做饭，少点外卖 这，上半年还在坚持做，但是到了10月份，就几乎没有了。一是隔壁租的其他人用厨房的频率高了，又不想你等我，我等你的这种 跑完60个5km，5个10km 大概在9月份吧，就完成了。本来以为今年有机会去参加半马的(不过这不在flag之内，看明年锻炼的情况)，但是看着锻炼的最长距离感觉还不行。而且主要目的又不是非要去参加半马。锻炼意志力了就行，身体锻炼是其次 2019过得并没有像自己想象中的样子，尽量在2020年重新拾起一些东西吧，勇敢大步向前走，去自己想去的公司。2020年的flag就不先公开了，希望在技术上像贝聿铭先生说的那样，”我一直沉浸在如何解决我自己的问题之中”,你成长的速度必须足够快，才能抓住一些你要抓住的东西以及未来想抓住的东西 未来可期","link":"/2020/01/01/2019%E5%B9%B4%E6%80%BB%E7%BB%93/"},{"title":"2024年总结","text":"距离上一次写年终总结已经好几年了，记得刚毕业那会儿还幻想过，自己在30附近的时候会是什么样子。职场上在带团队，爱情事业双丰收，组建自己的小家庭。 事实证明，自己push这些进展还远远不够。以至于感慨时间过了，好像很多东西都还没有，很多事情也还没有做。 单位时间内的行动力是非常重要的，机会为什么要留给有准备的人，因为机会有时间窗口期。 还是分几个小点来回顾2024和展望一下2025年。 目录读书健康爱情家人技术个人的一些想法 读书看了一下今年豆瓣mark的书籍，9本，正在读的还有2本。 很惭愧,今年读书量太少了。一直以来的观点是学习是终生的事情,读书也是。社会太浮躁，读书也是一种静下心来的一种方式。以及，自己的人生观不应该是刷刷抖音获取，应该多去和厉害的人交流，多去读书。 以上几本书，大部分是有收获的。跟人沟通的时候如何提问？如何让别人更愿意回答你？ 营养学是一门深厚的学问，了解一天正常的饮食结构，身体所需的营养元素，结合睡眠，运动，心理等方面，才能更好的保持健康。 自信或假装自信，对于争取生活中的机会是必要的。 交流的最佳效果来自谈吐得体且态度真诚，其关键点在于你不需要直愣愣地冒出大实话，而是适当修饰后的诚实表达。在不伤及别人的前提下又能实话实说，这对一些人来说轻松自如，而对另一些人来说则是一门需要学习的技巧。 毕业这几年来也陆陆续续读了100多本书了，明年要加大量了。 健康今年跑量不多，keep上的数据显示是123km。这几年虽然跑的不算多，但是配速却是一路提上来了。不过遗憾的是报了半马的名，均未中签。在今年让我有这么一个感悟。稳心率可能更健康。随着年龄的增加，或者说作为一个成熟的跑步者，一味的追求配速显得没有那么重要了。而且以一个长线主义的角度去看跑步这个问题，今年看了相关论文，略伤膝盖，虽然我这一点跑量不足以说明问题。但是我还是利用公司的爬楼梯机来代替了大部分时候本应该的跑步。然后就是骑行了306km，这一部分数据是因为在年中的某一时间段，我上班9km这距离，骑共享单车耗时20+min。所以只要不下雨，基本上都是骑共享单车回家了。不论是从经济角度还是时间角度，都是一个不错的选择。 本身力量是一个薄弱项，今年也有刻意在做一些力量训练。 健康的万恶之首是睡眠，在今年有很好的改善了。对比之前的几年，还是有不少的比例在12点之前就睡觉了。这一点有信心明年继续进一步改善。 明年继续改善睡眠就好了。 爱情很幸运、遇到了她。我从来都是一个很理性的人，在这之前我给自己条条框框。原来遇到了后，会发现之前以为的条条框框都是自己给自己设定的。有过甜甜的经历，也有过小争执。未来也还有其他的，始终本着要解决问题的角度去对待吧。携手并进，感谢cw。 看这24年的航线图，也是为爱努力奔赴了。 未来有很多不确定性，一起面对和解决。 家人聚少离多，父母一年一年逐渐老去。对待家人，更多的是期望他们少劳累一点，健健康康的生活。也是又一年没有跟父母有过争执的一年，而且是已经是连续好多年了。 技术发现人，是真的随着年龄的增长在技术上投入的时间确实少了。一方面我理解为技术热情下降了一些，还有时间精力自然会被一些其他给分散了去，当然还有比如工作占据的等等。 在去年有在写一些开源的代码，今年陆续也完善了一点。未完善的地方明年一定完成了，在这里确实愧对yqj的期望。 今年是大模型的百花齐放的一年，具有捣鼓精神的我也是尝试了各种花式搭建。企业是敏感的，从前几年关注的阿里天池的竞赛。从工程，算法方面。逐步新增了关于大模型的竞赛了。去捣鼓了一下，竞赛这个东西吧，真是需要花点时间去研究的，再也回不到大学打竞赛的那些时候了。 很感谢yqj在今年给予我的帮助，带我见世面，确实见到过不少厉害的人，不过很抱歉，微软MVP今年没有去申请，这里的很大一个原因是自己的时间没有太规划好，也包括在下半年也有一些其他的事情要忙。 也很感谢华为云社区的邀请 之前在GitHub上活跃了一下，今年还有一笔意外之财。 一些明年的展望 要多读一点书。以终为始，把拆分到每周的任务中来。 补回来一些2024年应该做的事情 提升架构上的能力 积累一些自己的工具，开箱即用 希望提升自身的 reputation（把之前的留下了） 输出一些有质量的文章 个人的一些想法关于时间的感知随着年龄的增长，感觉时间过得是越来越快 为此，还专门去查了一些资料，想知道这样的「感觉」是否有一些较为科学的解释，还真有，关于时间的感知在心理学、神经科学和认知科学上都有一些相关的研究。 情绪对时间知觉的影响及其神经生理学机制 情绪能影响个体时间感知，如恐惧和愤怒等负性情绪可延长时间知觉，这可能是机体适应方式。情绪对时间知觉的调节与注意和觉醒有关，也涉及神经网络参与及多巴胺能神经投射等机制。 新奇感与记忆密度 人的大脑对新奇事件更加敏感，新的体验会在记中留下更多的痕迹。年轻时，经历的新事物较多，记忆丰富，感觉时间「充实而缓慢」，随着年龄的增长，生活逐渐趋于单调和习惯化，记忆中新信息减少，人类的大脑会对熟悉的情景进行快速编码，减少对细节的关注，重复与忙碌让人无瑕感知时间，回想时自然觉得时间过得很快。 关于专注力的感知专注力是一种能力，是一种能够让我们在面对复杂的问题时，能够集中精力，不受外界干扰的能力。大部分人应该有是很少有专注的时间，特别是如果有一堆的事情。所以碎片化时间就显得格外重要。利用好了，能做不少事情。 这需要更好的专注力的提升。 尝试营造一个安静、整洁、有序的环境，减少外界干扰因素。手机调至静音，关闭不必要的电脑弹窗和社交媒体。合理安排时间和任务，保持充足的睡眠、合理的饮食和适当的运动 这一块儿2025年会刻意去练习 最后放一张第一次和cw约会时拍到的一张图","link":"/2024/12/31/2024%E5%B9%B4%E6%80%BB%E7%BB%93/"},{"title":"AI驱动的编译器优化：Azure与LLVM的自动化代码加速方案","text":"编译器优化领域的应用逐渐成为突破传统性能瓶颈的关键。编译器作为连接高级语言与底层硬件的桥梁，其优化能力直接影响计算任务的执行效率。然而，传统编译器（如LLVM）依赖人工经验设计优化规则，难以应对硬件多样性、算子复杂性及动态场景的挑战。本文将探讨如何通过AI技术重构编译器优化流程，并结合微软Azure云平台的算力资源与LLVM框架，构建端到端的自动化代码加速方案。 传统编译器优化的技术瓶颈与演进需求1.1 静态优化范式的深层制约传统编译器优化体系建立于静态分析框架之上，其核心机制依赖于预定义规则集合与启发式算法。这些技术在特定领域展现出有效性，但也面临日益显著的技术天花板： （1）典型优化技术的固有限制 循环优化方面：以循环展开(loop unrolling)为例，编译器需要精确估算循环迭代次数以确定展开因子(unroll factor)，但面对动态循环边界（如数据依赖型循环）时，常因静态预测偏差导致过度展开或展开不足。向量化优化(loop vectorization)则需匹配目标硬件的SIMD寄存器宽度，而不同GPU架构（如NVIDIA Ampere与AMD CDNA2）的向量处理单元存在架构级差异，通用参数难以适配[6]。 函数内联策略：虽然内联优化通过消除调用栈操作可提升5-15%的指令级并行度，但机械式应用会导致关键路径延长。在移动SoC场景下，过度内联引发的指令缓存(ICache)污染可能造成高达30%的性能回退[5]。当前主流编译器采用阈值模型控制内联深度，却无法动态感知运行时上下文。 窥孔优化局限：该技术通过滑动窗口进行指令级模式匹配（如将”add eax,0”替换为nop），但其局部视野难以捕捉跨基本块的数据流特征。现代超标量处理器中的指令级并行机会（如乱序执行窗口优化）约40%存在于跨区域指令调度中，传统窥孔优化无法有效覆盖[3]。 （2）系统性优化瓶颈架构多样性挑战：新兴计算范式（如存算一体架构）打破了冯·诺依曼体系的内存墙假设，传统优化器缺乏对3D堆叠内存带宽特征的建模能力。以TPUv4的脉动阵列为例，其数据搬运模式需要编译器显式管理多维数据流，传统循环分块(tiling)策略无法自动推导最优数据复用模式[1]。 算子组合复杂性：深度学习计算图中常见的算子融合场景（如Conv-BN-ReLU融合）涉及多个优化维度（内存访问、并行粒度、指令流水），人工规则库难以穷举所有合法融合模式。TensorFlow XLA的实践表明，仅卷积相关算子就存在200+种可能的融合组合，传统方法需指数级规则扩展[1]。 1.2 LLVM框架的技术突破与待解难题LLVM作为第三代编译器的代表，通过革命性的中间表示设计和模块化架构解决了传统编译器的诸多痛点： （1）架构创新解析多层次IR系统：其分层中间表示体系包含： 前端IR（如Clang AST）：保留源码级语义信息 中级IR（SSA form）：通过φ函数实现跨过程数据流分析 后端IR（Machine IR）：集成目标指令集特征这种设计允许在LLVM IR层实施与硬件无关的优化，例如全局值编号(GVN)算法可消除跨函数冗余计算，相比GCC的RTL优化提升约22%的公共子表达式消除率[4]。 可扩展Pass管道：LLVM 15.0提供128个优化Pass，形成多阶段处理链。以-O3优化级别为例，其Pass序列包含： 过程间常量传播(IPCP) 聚合体标量替换(SROA) 循环向量化(LV) 超级块优化(SLPVectorizer)每个Pass可配置细粒度参数，如循环向量化器的最大展开因子(max-unroll)和向量寄存器利用率阈值(vector-width)[6]。 （2）现实应用中的优化缺口动态优化盲区：LLVM的静态编译模型难以捕获运行时信息。以GPU上的矩阵转置算子为例，其最佳访存模式取决于运行时张量形状： 当dim_size &gt; 1024时，应使用共享内存转置策略 小尺寸张量适合直接寄存器转置但现有LLVM NVPTX后端无法在编译时获取这些动态参数，导致生成单一保守策略，实测性能损失达38%[9]。 智能调度缺失：AI编译器（如TVM）的AutoTVM模块显示，LLVM内置的指令调度器(llvm-mca)对新型计算指令（如DP4A）的延迟特性建模不准。在INT8卷积核生成任务中，手工调整指令顺序可获得比LLVM默认调度高17%的吞吐量，暴露出现有启发式规则的局限性[9]。 （3）生态演进方向业界正在LLVM基础上构建自适应优化框架，典型方案包括： 多目标优化：Facebook的BOLT工具在post-link阶段实施基于执行剖面的优化，使HHVM字节码的ICache缺失率降低19% ML引导优化：Google的MLGO项目使用强化学习训练内联决策模型，在Chromium项目中获得3.2%的运行时加速 异构编译：AMD ROCm堆栈扩展LLVM支持CDNA架构的矩阵核心，通过新增Wavefront级优化Pass提升MI250X的FP16计算效率达41% AI赋能的智能编译器优化技术体系2.1 基于搜索的自动调优系统 技术原理：构建参数空间探索引擎，采用混合搜索策略（遗传算法+贝叶斯优化）对编译器优化参数进行组合优化。以TVM Ansor框架为例，其采用分阶段搜索机制：首阶段通过随机采样建立参数空间拓扑，次阶段运用蒙特卡洛树搜索对循环分块因子（Tile Size）、循环展开深度（Unroll Depth）、线程绑定策略（Thread Binding）等关键参数进行组合寻优，最终生成适配目标硬件的优化配置方案。 硬件适配特性：针对Azure ND H100实例的NVIDIA H100 GPU架构特征，系统可自动感知硬件参数（包含108个流多处理器、18432个CUDA核心、3TB/s显存带宽），通过动态调整线程块维度（BlockDim）、共享内存分配策略（Shared Memory Allocation）以及NVLink 4.0互连拓扑感知的任务划分，使计算任务与硬件资源形成最佳映射关系。实验数据显示，在矩阵乘优化场景中，自动调优可使H100的Tensor Core利用率提升至92%，较人工优化提升37个百分点。 工程挑战与解决方案：当优化参数维度超过50维时，传统串行搜索的时间复杂度将呈现指数级增长（O(n^d)）。采用基于Ray框架的分布式异步进化算法，通过在Azure Kubernetes集群部署参数搜索节点，实现多代种群并行评估。测试表明，在8节点H100集群环境下，参数搜索效率可提升12.8倍，单次完整搜索周期从72小时压缩至5.6小时。 2.2 数据驱动的机器学习优化框架 智能决策系统架构： 策略选择模块：构建基于XGBoost的多任务分类模型，输入特征包含IR指令模式（Opcode Distribution）、控制流图复杂度（CFG Cyclomatic Complexity）、数据依赖关系（Data Dependency Distance）等236维特征向量。通过分析历史编译日志（含1.2亿条优化决策记录），模型可预测函数内联（Function Inlining）的收益阈值，在LLVM编译器中实现92.3%的决策准确率。 性能预测模块：开发层次化回归网络，结合图神经网络（GNN）处理LLVM-IR的图结构特征，辅以时序卷积网络（TCN）捕捉优化序列的时态相关性。该模型可预估不同优化策略的潜在加速比（Speedup Factor），在SPEC CPU2017基准测试中，预测误差率控制在±5%以内，减少实际编译测试次数达83%。 行业实践案例：Meta研发的LLM Compiler采用双阶段训练范式，首先在5460亿LLVM-IR令牌的语料库上进行自监督预训练，学习程序语义模式；随后在特定硬件优化数据集（含AMD EPYC/Xilinx FPGA/NVIDIA GPU多平台数据）进行指令微调。该系统可自动完成循环融合（Loop Fusion）、存储层次优化（Memory Hierarchy Optimization）等17类代码重构，在PyTorch模型编译场景中，自动优化方案覆盖77%的人工优化收益，开发效率提升9倍。 2.3 生成式AI代码合成技术 代码生成技术演进： 基础架构：基于Code Llama-34B的改进模型，采用混合精度训练（FP16/FP8）与FlashAttention-2优化。模型架构扩展支持跨语言语义理解，可处理Python/Julia算子描述到LLVM-IR/SYCL等多中间表示的转换。 指令优化能力：模型内建硬件指令知识库（涵盖SIMD向量化指令集、Tensor Core WMMA指令、AMD CDNA矩阵核心等），在代码生成阶段自动插入优化原语。例如，针对AVX-512指令集自动生成向量化循环体，在Intel Xeon Platinum 8480+处理器上实现4.2倍标量代码加速。 工程实现方案：在Azure ML平台构建端到端训练管道，利用ND H100实例的Transformer Engine进行混合精度训练。单卡H100通过第三代NVLink实现900GB/s互联带宽，支撑128K上下文窗口的LLVM-IR长序列训练。部署阶段采用Triton推理服务器，将AI生成的优化代码通过LLVM JIT编译器动态嵌入编译流程，实现μs级延迟的实时代码替换。 硬件协同优化：特别针对H100的Hopper架构特性，生成代码可激活以下硬件加速机制： 利用TMA（Tensor Memory Accelerator）实现全局内存到共享内存的零拷贝传输 通过异步拷贝引擎（Async Copy Engine）隐藏内存访问延迟 采用DPX指令加速动态规划类算法 激活H100特有的FP8格式计算单元实测显示，在Transformer推理场景中，AI生成内核相较CUDA手写代码可获得11%的性能提升，同时减少开发周期从3周至8小时。 Azure云平台的基础设施支持3.1 高性能硬件加速体系 Azure ND H100 v5实例基于第四代NVIDIA Hopper架构构建，专为AI训练与高性能计算场景提供革命性算力支撑。其核心架构创新体现在三个维度： GPU计算单元层面，每节点配备8块NVIDIA H100 Tensor Core GPU，每GPU集成18432个CUDA核心和576个第四代张量核心。特别值得关注的是其Transformer Engine技术，该硬件模块通过动态混合精度计算（FP8/FP16）和智能算子融合，可将大规模语言模型的训练速度提升至前代产品的6倍。针对编译过程中的张量计算优化，H100的TMA（Tensor Memory Accelerator）引擎可实现跨多GPU的零拷贝张量传输，显著加速中间表示（IR）的并行处理。 高速互联架构方面，该实例采用NVIDIA NVLink 4.0与NVSwitch组合方案。每个GPU通过18条NVLink通道实现全互连拓扑，形成总带宽达3.6Tb/s的GPU直连网络。这种突破性的互联性能使得大规模分布式调优任务能够实现近乎线性的扩展效率，例如在执行超参数搜索时，可同时维持超过5万个优化配置的并行评估，且各计算节点间的梯度同步延迟降低至微秒级。配合第三代PCIe Gen5接口，CPU-GPU间的数据搬运带宽相比前代提升2倍，有效消除传统编译优化中的数据传输瓶颈。 3.2 智能云原生编译流水线 Azure与LLVM工具链的深度协同优化构建了智能化编译即服务平台，其技术实现包含两大核心组件： 动态编译资源编排系统采用基于DAG（有向无环图）的任务调度算法，通过实时监控IR转换过程中的计算特征（如控制流复杂度、内存访问模式等），自动选择最优硬件执行单元。在语法分析等串行阶段分配vCPU集群处理，当检测到向量化优化机会时，立即调度配备HBM3显存的GPU进行加速。实际测试显示，在Polyhedral模型优化阶段，GPU加速可使循环嵌套优化速度提升17倍。系统内置智能预测模型，可根据历史编译数据预加载所需依赖库，将编译准备时延缩短至毫秒级。 端到端编译优化服务通过三层抽象实现SaaS化交付： 服务接入层提供RESTful API和VS Code插件，支持上传包含编译指令的manifest.yaml配置文件； 优化引擎层部署AutoTuning-as-a-Service微服务集群，采用强化学习算法在参数空间（包括循环分块策略、内存对齐方案等）进行多维搜索； 交付层生成包含优化元数据的可执行文件包，除二进制文件外，还提供包含优化决策树的PDF报告和性能热图分析。典型应用场景中，开发者上传C++代码后，系统在23分钟内完成287种优化策略的评估，返回经AVX-512指令集优化且适配目标硬件的可执行文件。 LLVM与AI协同的实践案例深度解析4.1 AI增强的Pass管理器4.1.1 智能化架构设计在传统LLVM编译框架基础上构建三层AI协同架构： 基础层：保留原有Pass执行引擎，维持对历史编译流程的兼容性 决策层：引入深度强化学习模型(DRL)作为智能调度核心，包含： 状态感知模块：实时采集函数特征（基本块数量、循环嵌套深度、内存访问模式等） 收益预测模型：基于图神经网络(GNN)构建Pass效果预测器，量化评估每个优化Pass在当前上下文的潜在收益 策略网络：采用PPO算法动态生成Pass调度序列，支持运行时动态插入/删除Pass 反馈层：建立编译效果追踪系统，通过代码插桩收集优化后程序的运行时特征，形成闭环训练机制 典型应用场景： 强化学习驱动的Pass调度针对大规模数值计算函数，模型可自主决策跳过冗余的LoopUnrollPass（循环展开优化），转而激活基于机器学习的AutoVectorizationPass（自动向量化）。在阿里云AnalyticDB PostgreSQL的OLAP工作负载中，该机制使TPC-H Q6查询的指令缓存未命中率降低67%。 多维度优化决策当检测到包含多层嵌套循环的计算密集型函数时： 启动Polyhedral模型进行深度分析，构建迭代空间的多面体表示 运用约束求解器自动生成最优循环变换方案（包括循环分块、融合、倾斜等） 结合硬件特性（如CPU缓存行大小、SIMD寄存器位宽）生成数据局部性优化方案某HPC场景测试显示，该技术使矩阵乘法的L1缓存利用率从58%提升至92%，执行耗时减少41%。 4.1.2 性能优化成效在阿里云AnalyticDB PostgreSQL中的落地实践表明： 查询编译时间平均缩短22%，消除传统固定Pass序列的试探性优化开销 生成代码的IPC（每时钟周期指令数）提升3.8倍 复杂分析型查询的端到端执行性能提升3-5倍，其中TPC-DS Q72查询响应时间从8.7秒降至2.1秒 4.2 自适应代码生成系统 4.2.1 动态编译体系构建数据驱动的JIT编译框架，包含三大核心组件： 运行时特征监控器 张量维度追踪：记录输入张量的秩、维度值、内存布局等信息 数据流分析：构建动态数据依赖图，识别热点计算路径 硬件状态感知：实时采集FPGA/DSP资源利用率、内存带宽等指标 智能代码生成器 分块策略选择器：基于张量形状自动选择最优分块方案 当检测到MxKxN三维张量时，采用Strassen分块算法提升矩阵乘效率 针对不规则形状（如[1023,511]），启用动态尾循环处理 指令集优化模块： 为Intel VNNI指令集自动生成8位整型点积指令序列 在检测到AMD CDNA架构时，启用wave32执行模式 自适应重编译机制 设立多级优化阈值（执行次数、热点程度等） 采用增量式编译技术，对已优化代码进行版本管理 实现编译策略的在线迁移（如从保守的-O1快速切换至激进的-O3） 4.2.2 硬件专用化实践在Azure FPGA加速场景中的技术实现： 流式编译流水线 前端：将LLVM IR转换为OpenCL内核描述 中间层：执行硬件感知优化 流水线深度自动调节（基于时序分析报告） BRAM资源分配策略优化（采用混合整数规划模型） 后端：生成比特流时插入动态占位符，支持运行时参数注入 实时指令生成案例当处理卷积神经网络时： 根据输入特征图尺寸动态选择脉动阵列配置 为3x3深度可分离卷积生成定制化数据流 实现计算与IO的精确重叠，使ResNet-50的层间流水线气泡减少83% 典型性能表现： 在BERT-Large模型推理中，端到端延迟从71ms降至22ms FPGA资源利用率峰值达91%，较静态编译方案提升35% 支持在200μs内完成新算子的即时编译部署 该技术体系已在Azure Machine Learning平台实现规模化应用，支持超过20类加速卡的动态代码适配，平均性能提升达4.2倍。 关于未来的一些挑战5.1 技术融合趋势的深化发展 （1）多模态协同优化体系构建随着异构计算架构的普及，深度学习的计算图优化与底层代码生成呈现深度融合趋势。当前研究热点聚焦于建立端到端的编译优化框架，例如通过构建PyTorch动态计算图到LLVM中间表示（IR）的自动化映射系统，实现算法模型与硬件指令集的无缝衔接。在此过程中，AI驱动的优化Pass（如基于图神经网络的算子融合策略、基于强化学习的指令调度算法）可深度介入编译流程，形成”算法-编译-硬件”协同优化闭环。这种多模态优化模式不仅能保留高层框架的编程灵活性，还能在寄存器分配、内存对齐等底层细节实现超越传统编译器的优化效果。 （2）量子-经典协同编译新范式量子计算的快速发展对编译技术提出革命性需求。针对量子比特错误率高的核心挑战，AI技术正在重塑量子指令调度体系：通过构建量子门操作时空约束的图模型，结合变分量子-经典混合算法，可动态优化量子线路的拓扑映射策略；基于深度强化学习的纠错码分配系统，能够实时评估退相干效应，在逻辑门合成阶段主动注入纠错操作。这些创新使得量子编译器可自适应处理门级并行度、量子比特拓扑连接等复杂约束，为容错量子计算奠定基础。 5.2 关键挑战与突破方向 （1）跨架构泛化能力提升现有AI编译模型普遍面临数据依赖困境：在特定硬件架构（如GPU张量核心）或专用算子（如深度卷积）上训练的模型，迁移到新兴计算单元（如Cerebras Wafer-Scale引擎）时性能显著下降。解决方案需构建多维度特征工程框架：①建立跨ISA指令集（x86/ARM/RISC-V）的统一中间表示，抽象硬件特征参数；②开发多精度混合数据集，涵盖从嵌入式DSP到云端TPU的典型计算模式；③设计元学习训练机制，使模型能快速适配新型硬件微架构。同时需建立动态基准测试平台，量化评估模型在稀疏计算、存内计算等新兴场景的泛化能力。 （2）可验证编译安全保障体系AI驱动的代码生成引发新的可信计算挑战：神经网络黑箱特性可能导致优化后的代码存在隐蔽的语义偏差。需构建多层次验证框架：①形式化验证层：通过抽象解释（Abstract Interpretation）建立程序不变式，利用SMT求解器证明优化前后代码的输入输出等价性；②动态监控层：在JIT编译阶段植入运行时断言，实时检测寄存器溢出、内存越界等异常行为；③对抗测试层：使用符号执行技术生成边界条件测试用例，验证极端场景下的计算正确性。同时需要发展可微分形式化方法，将验证约束反向传播至AI编译器训练过程，实现安全性与性能的联合优化。 AI驱动的编译器优化正从学术研究走向工业落地。通过结合LLVM的灵活性与Azure的高性能算力，开发者可构建自适应、跨平台的代码加速方案。未来，随着生成式AI与量子计算的发展，编译器将逐步进化为“自主优化系统”，彻底释放硬件潜力。","link":"/2025/03/20/AI%E9%A9%B1%E5%8A%A8%E7%9A%84%E7%BC%96%E8%AF%91%E5%99%A8%E4%BC%98%E5%8C%96%EF%BC%9AAzure%E4%B8%8ELLVM%E7%9A%84%E8%87%AA%E5%8A%A8%E5%8C%96%E4%BB%A3%E7%A0%81%E5%8A%A0%E9%80%9F%E6%96%B9%E6%A1%88/"},{"title":"Azure OpenAI服务全解析：从GPT-4到DALL-E的模型生态","text":"一、Azure OpenAI服务全景概览作为微软人工智能战略的核心载体，Azure OpenAI服务构建起覆盖自然语言处理、计算机视觉、语音交互的全栈式AI能力矩阵。该平台集成了OpenAI最前沿的技术成果，通过企业级云服务架构为开发者提供安全可控的AI能力调用环境。其模型体系呈现三大特征： 多模态融合：支持文本、图像、语音跨模态交互 行业垂直化：针对科研计算、程序开发等场景深度优化 服务分层化：提供从基础推理到实时交互的梯度能力 二、核心模型体系技术解析（一）GPT系列演进图谱1. GPT-4o系列创新突破 模型版本 核心特性 技术指标 gpt-4o (2024-11) 多模态统一架构/结构化输出/跨语言增强 128k输入/16k输出 gpt-4o-mini 轻量级推理引擎/快速响应 128k输入/16k输出 GPT-4 Turbo 视觉增强型推理/复杂问题解决 128k输入/4k输出 技术演进对比： 推理深度：o系列较Turbo提升3倍运算链长度 多语言支持：非英语任务准确率提升27% 图像理解：视觉特征提取效率提高40% 2. GPT-3.5技术定位 Turbo版本：聊天场景优化，支持16k上下文 指令版本：传统补全任务专用，推理成本降低35% （二）专业推理模型体系o系列专业模型矩阵 场景适配指南： 科研计算：推荐o1旗舰版（200k上下文窗口） 实时编程：选择o1-mini（毫秒级响应） 数据分析：采用o3-mini（结构化输出支持） （三）多模态创新模型1. GPT-4o音频引擎 模型类型 延迟指标 适用场景 技术特性 实时交互版 &lt;200ms 智能客服/同声传译 语音流式处理 音频生成版 异步处理 有声书制作/多媒体内容生成 高保真语音合成 技术突破： 语音识别准确率达98.7%（行业基准96.2%） 支持128k token音频上下文记忆 2. DALL-E视觉引擎python 复制 123456789# 典型图像生成流程from azure.ai import openairesponse = openai.Image.create( prompt=&quot;赛博朋克风格的城市夜景，霓虹灯光雨中的仿生人&quot;, size=&quot;1024x1024&quot;, quality=&quot;hd&quot;, style=&quot;vivid&quot;) 版本对比： DALL-E 3：4K超分辨率/语义理解增强 DALL-E 2：快速原型设计/成本优化 （四）企业级支持模型1. 嵌入模型体系 text-embedding-3-large：1536维高精度向量 text-embedding-3-small：高效检索优化 向量空间对比： 复制 12数学问题 → [0.87, -0.23, ..., 0.45]文学分析 → [-0.12, 0.78, ..., -0.09] 2. Whisper语音模型 支持93种语言实时转写 行业术语识别准确率提升40% 三、模型选型决策框架（一）四维评估体系 计算复杂度：o系列 &gt; GPT-4 &gt; GPT-3.5 响应延迟：o1-mini（50ms）&lt; GPT-4o（120ms）&lt; DALL-E（2s） 多模态需求： 文本+图像：GPT-4o 语音交互：GPT-4o Audio 跨模态检索：嵌入模型 （二）成本优化策略 场景 推荐模型 TCO节省比例 日常对话系统 GPT-3.5 Turbo 45% 技术文档分析 o1-mini 32% 跨国会议转录 Whisper-large 28% 四、企业集成实践（一）混合部署架构1234用户终端 → Azure API网关 → 模型路由层 ├─ GPT-4o（复杂推理） ├─ o1-mini（实时计算） └─ 嵌入模型（语义检索） （二）性能监控指标 推理准确性：BARTScore评估 响应稳定性：P99延迟监控 资源利用率：GPU内存消耗跟踪 五、技术演进展望 量子计算融合：预计2025年实现千亿参数模型实时推理 神经符号系统：将逻辑推理能力提升300% 自我进化机制：模型自优化周期缩短至72小时 通过深度整合Azure云原生能力与OpenAI前沿技术，该服务持续重塑企业智能化转型的技术范式，为各行业提供从基础感知到决策支持的完整AI解决方案。开发者应根据具体业务场景的需求特征，建立动态的模型评估与迭代机制，充分释放生成式AI的商业价值。","link":"/2025/01/13/Azure%20OpenAI%E6%9C%8D%E5%8A%A1%E5%85%A8%E8%A7%A3%E6%9E%90%EF%BC%9A%E4%BB%8EGPT-4%E5%88%B0DALL-E%E7%9A%84%E6%A8%A1%E5%9E%8B%E7%94%9F%E6%80%81/"},{"title":"Azure无服务器GPU实战：低成本运行多模态大模型","text":"随着多模态大模型（如视觉-语言模型、文本-音频生成模型等）的快速发展，企业对高效、低成本的算力需求日益迫切。Azure 无服务器 GPU 服务结合其弹性扩展和按需付费的特性，为开发者提供了部署多模态大模型的理想平台。本文将从实战角度，探讨如何基于 Azure 无服务器 GPU 基础设施，低成本运行多模态大模型。 。 无服务器 GPU 的核心优势解析 弹性计算与精细化成本管理Azure 无服务器 GPU（以 Azure Container Apps 的无服务器 GPU 功能为代表）通过创新的资源调度机制，实现了计算资源的智能动态弹性。系统可根据实时工作负载需求，在秒级时间粒度内完成从零到数百个 GPU 实例的横向扩展，并在任务完成后立即释放计算资源。这种按需供给模式有效解决了传统 GPU 实例常驻模式下的资源空置问题，特别适用于具有明显波峰波谷特征的 AI 工作流（如批量图像生成、视频流实时分析、周期性模型训练等）。以典型应用场景为例，当处理 Stable Diffusion 图像生成任务时，系统可自动调用 NVIDIA T4（推理优化型）或 A100（高性能计算型）实例集群，执行结束后即释放资源，采用每秒千分之一核的精确计费方式，相较传统包年包月模式可降低 60%-80% 的长期持有成本。 高性能即时响应体系针对行业普遍关注的冷启动延迟问题，Azure 构建了多层级的加速引擎：在基础设施层，通过预置容器热池技术将常用框架镜像（如 PyTorch、TensorRT 等）预加载至高速缓存；在调度层，采用基于强化学习的预测算法，提前分配处于休眠状态的”暖实例”；在运行时层，则通过 GPU 内存分页共享技术实现计算状态的快速恢复。这套组合方案将冷启动时间压缩至 200 毫秒以内，较传统云 GPU 服务提升 15 倍响应速度。以部署百亿参数级大模型 VisualGLM-6B 为例，即使在零请求的闲置状态下，系统仍能保持 500ms 以内的首帧响应时间，完美支持突发性推理请求场景（如电商大促期间的实时推荐系统）。 全栈式 AI 开发生态Azure AI Foundry 构建了覆盖模型全生命周期的服务体系，形成从数据处理、分布式训练到生产部署的完整工具链。其核心优势体现在：预集成超过 1,800 个经深度优化的开源及专有模型（涵盖文本生成、多模态理解、代码生成等 32 个垂直领域），包括前沿的 NExT-GPT（多轮对话专家）、DreamLLM（长文本生成）等明星模型。开发者可通过统一控制台实现跨模态模型的即插即用，例如直接调用视觉-语言联合 API 处理复杂场景（如医疗影像报告自动生成），而无需从零搭建多模型协作框架。平台还提供自动化的模型蒸馏服务，支持将千亿参数模型压缩至原体积的 1/10 并保持 98% 的精度，显著降低推理成本。 环境配置与模型部署优化方案 弹性GPU计算集群构建（无服务器模式）在Azure Machine Learning服务中，可通过智能资源调度实现GPU资源的弹性供给。推荐使用NCv3系列虚拟机（建议选择Standard_NC6s_v3型号，搭载NVIDIA Tesla V100 GPU），该配置在计算性能与成本效益间取得平衡，特别适合大模型推理场景。 配置操作流程： 1234567891011121314151617181920212223242526pythonCopy Codefrom azureml.core import Workspacefrom azureml.core.compute import AmlComputefrom azureml.core.compute_target import ComputeTargetException# 创建工作区连接ws = Workspace.from_config()try: # 检查计算目标是否存在 compute_target = AmlCompute(ws, 'gpu-cluster')except ComputeTargetException: # 高级配置参数 provisioning_config = AmlCompute.provisioning_configuration( vm_size='Standard_NC6s_v3', # 配备16GB显存的V100 GPU vm_priority='lowpriority', # 成本优化模式 idle_seconds_before_scaledown=300, # 5分钟无任务后缩容 min_nodes=0, # 无服务器模式核心参数 max_nodes=8, # 根据业务峰值设定 remote_login_port_public_access='Disabled' # 安全加固 ) # 创建弹性集群（约需10分钟） compute_target = ComputeTarget.create(ws, 'gpu-cluster', provisioning_config) compute_target.wait_for_completion(show_output=True) 关键技术特性： 智能伸缩策略：支持0-8节点的动态扩缩，冷启动时间优化至90秒内 成本控制模式：采用Spot实例可降低80%计算成本，支持任务优先级设置 资源监控：集成Azure Monitor实现GPU利用率、显存消耗的实时监控 安全防护：默认启用虚拟网络隔离和托管身份认证 多模态大模型部署实践（VisualGLM-6B优化版） 环境依赖与模型优化 123456789101112bashCopy Code# 创建conda虚拟环境conda create -n visualglm python=3.8 -yconda activate visualglm# 安装核心依赖库（使用清华镜像源加速）pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 -f https://download.pytorch.org/whl/torch_stable.htmlpip install SwissArmyTransformer==0.4.5 bitsandbytes==0.39.1 flash-attn==1.0.7 --index-url=https://pypi.tuna.tsinghua.edu.cn/simple# 模型量化工具pip install git+https://github.com/IST-DASLab/gptq@main 模型优化策略： 4-bit量化：采用QLoRA技术将原始FP32模型压缩至4-bit精度，显存需求从32GB降至8GB 梯度检查点：通过activation checkpointing技术降低50%显存占用 动态分块加载：实现大图像输入的分块处理，支持最高4096x4096分辨率输入 模型推理服务化 12345678910111213141516171819202122232425262728293031323334353637383940414243pythonCopy Codefrom model import VisualGLMModel, AutoTokenizerimport argparsefrom PIL import Image# 量化模型加载def load_quantized_model(): args = argparse.Namespace( fp16=True, quant=&quot;4bit&quot;, device_map=&quot;auto&quot;, warmup_steps=50 ) model = VisualGLMModel.from_pretrained( &quot;THUDM/visualglm-6b&quot;, args=args, torch_dtype=torch.float16, low_cpu_mem_usage=True ) tokenizer = AutoTokenizer.from_pretrained( &quot;THUDM/visualglm-6b&quot;, trust_remote_code=True ) return model, tokenizer# 带缓存机制的推理服务model, tokenizer = None, Nonedef predict(image_path: str, question: str, max_length=512): global model, tokenizer if model is None: model, tokenizer = load_quantized_model() image = Image.open(image_path).convert(&quot;RGB&quot;) response, history = model.chat( image=image, text=question, tokenizer=tokenizer, max_length=max_length, temperature=0.8, top_p=0.95 ) return {&quot;response&quot;: response, &quot;history&quot;: history} 无服务器生产部署方案 容器化封装 12345678910111213141516171819202122dockerfileCopy Code# Dockerfile.prodFROM nvcr.io/nvidia/pytorch:22.12-py3# 系统级优化RUN apt-get update &amp;&amp; \\ apt-get install -y libgl1 libglib2.0-0 &amp;&amp; \\ rm -rf /var/lib/apt/lists/*# 构建优化后的虚拟环境COPY requirements.txt .RUN pip install -r requirements.txt --no-cache-dir# 模型预下载（约节省冷启动时间60s）RUN python -c &quot;from model import VisualGLMModel; VisualGLMModel.from_pretrained('THUDM/visualglm-6b')&quot;WORKDIR /appCOPY app.py .CMD [&quot;gunicorn&quot;, &quot;app:api&quot;, &quot;--timeout&quot;, &quot;300&quot;, &quot;--workers&quot;, &quot;2&quot;] Azure服务集成通过Azure Container Apps实现自动扩缩容： 构建并推送镜像至ACR（Azure Container Registry） 创建容器应用服务，配置GPU加速实例 设置HTTP Scale规则： 请求队列长度阈值：50 扩缩容响应时间：30秒 启用应用洞察（Application Insights）进行性能监控 该方案已通过Azure AI模型认证，支持： 自动弹性伸缩：根据请求量动态分配GPU资源 多实例协同：实现模型分片并行推理 安全审计：符合ISO 27001数据安全标准 成本监控：提供细粒度资源消耗报表 注：实际部署时建议启用Azure Bastion进行安全接入，并配置VNet对等互连实现混合云部署。 成本优化策略深度解析及实施路径 动态批处理与混合精度优化方案技术原理：基于NVIDIA Triton推理服务器的动态批处理机制，通过请求队列管理模块（Sequence Batcher）实现多请求智能聚合。采用时间窗口（max_queue_delay_microseconds）和批处理策略（Direct/Ragged）的协同控制，动态调整批次规模（4-128个样本），实现GPU计算单元利用率最大化。 执行流程： 请求预处理阶段：输入数据经CPU预处理后存入共享内存池 动态调度阶段：Triton调度器根据预设策略（优先级/延迟敏感度）进行请求分组 混合精度执行：采用FP16矩阵乘运算配合FP8梯度计算，通过NVIDIA Transformer Engine自动优化计算图 结果分发阶段：输出解批处理器将结果拆分返回客户端 硬件协同：在Blackwell架构GPU上，结合第三代张量核心的稀疏计算特性，通过Triton的模型分析器（Model Analyzer）自动选择最优批尺寸。实测数据显示，ResNet-152模型推理吞吐量提升42.3%，P99延迟降低34.7%（对比A100平台）111。 模型量化与显存管理技术体系量化实施方案： 训练后量化（PTQ）：采用bitsandbytes的LLM.int8()方案，对线性层进行8-bit分块量化，保留0.01%关键权重为FP32 量化感知训练（QAT）：使用NVIDIA的Quantization Toolkit，在BERT-large训练中插入Q/DQ节点，实现4-bit混合精度量化 自适应量化策略：根据层敏感度分析（使用Hessian跟踪法），对注意力机制层保持FP16，前馈网络层实施4-bit量化 显存优化技术栈： 分层交换策略：通过Mbapp（Memory-bandwidth Aware Partial Paging）技术，将激活张量按计算依赖关系分阶段交换至： L4 GPU缓存：高频访问数据（&lt;10ms保留期） 主内存：通过GPUDirect RDMA实现80GB/s传输带宽 邻近GPU：利用NVLink 4.0的600GB/s互连带宽 实时压缩：采用DeepSpeed的ZERO-Offload技术，对梯度张量实施Snappy实时压缩（压缩比1:3） 实测效果：LLaMA-13B模型显存占用从26GB降至6.2GB，推理速度保持原始性能的92%9。在A100 80GB平台实现同时运行4个量化模型实例2。 智能任务调度与资源监控体系监控系统架构： 数据采集层：Azure Monitor Agent每500ms采集： GPU指标：SM利用率、显存带宽、NVLink误码率 模型指标：批处理效率、推理错误率、队列深度 分析层：使用Prometheus时序数据库存储，采样周期动态调整（空闲期30s/高峰期1s） 弹性扩缩容策略： 横向扩展条件：当同时满足： 请求队列深度 &gt; 50（5秒滑动窗口平均值） GPU利用率 &gt; 85%持续120秒 P95延迟 &gt; SLA阈值20%自动触发扩展到最大128节点（NVIDIA HGX B200集群） 纵向伸缩机制：根据模型优先级动态分配计算资源： 关键任务：独占整卡资源（QoS等级0） 批量任务：共享GPU（通过MIG技术划分2个14GB实例） 冷启动优化：预置10%的”温热”节点（模型预加载+显存预热），使扩容延迟从120s降至8.3s。采用渐进式缩容策略，在负载降至阈值50%后，每300秒缩减25%节点直至基础配置。 典型应用场景深度解析 实时图像描述生成与营销自动化基于Azure AI Search的RAG（检索增强生成）架构，构建多模态智能交互系统。通过集成NExT-GPT等视觉-语言融合模型，实现像素级图像理解与语义生成联动。以电商场景为例，当用户上传产品图像（如3C电子产品）至系统时，模型首先进行细粒度视觉特征提取（包括产品形态、材质纹理、品牌标识等），随后在Azure Cognitive Search构建的知识库中检索关联信息（如产品规格、市场定位），最终生成具备营销价值的多版本文案（如社交媒体短文案、产品详情页长描述）。整套流程依托Azure Kubernetes Service（AKS）的无服务器GPU资源池，通过动态算力分配与CUDA核心优化，实现端到端响应时间压缩至300ms以内，配合内容审核API自动过滤违规表述，形成安全可靠的AIGC工作流。 跨模态内容理解与智能分析利用Azure AI Content Understanding服务的多引擎解析框架，构建非结构化数据处理中枢。系统采用级联模型架构：前端通过Media Services完成音视频流解码与关键帧采样（视频处理支持H.264/H.265编码，音频支持48kHz采样率解析），中台部署多任务学习模型同步执行语音识别（WER≤5%）、视觉OCR（中文识别准确率98.7%）、场景分类等操作，后端通过图神经网络进行跨模态特征对齐，最终输出结构化知识图谱。以客户服务场景为例，系统在解析通话录音时，不仅生成逐字稿文本（支持8种方言识别），还能关联对话时序信息生成交互热力图，结合DeBERTa-v3模型的情感分析模块（细粒度至7种情绪分类），自动生成包含客户诉求摘要、服务评分（1-5星）、改进建议的三维分析报告，大幅提升服务质量评估效率。 大规模模型分布式训练体系针对MoE-LLaVA等混合专家模型的高效训练需求，Azure NDv6虚拟机集群提供异构计算解决方案。硬件层面采用AMD EPYC 7V12处理器与NVIDIA A100 80GB GPU的混合架构，通过NVSwitch实现GPU间600GB/s的超高带宽互联。软件栈集成DeepSpeed框架与Megatron-LM并行库，采用三级混合并行策略：数据并行（32节点分片）处理百亿级样本集，流水线并行（8阶段划分）优化千亿参数模型内存占用，张量并行（4维切分）加速注意力机制计算。训练过程中通过自动梯度压缩技术降低75%的跨节点通信负载，配合Azure CycleCloud的弹性资源调度，实现训练任务动态扩展（支持秒级扩容2000个计算节点）。实际测试表明，该方案在训练1.6万亿参数的视觉-语言模型时，较传统架构减少63%的训练耗时，同时通过Spot虚拟机竞价策略降低61.8%的TCO（总拥有成本）。 硬件升级与生态整合2025年，微软Azure将完成对NVIDIA Blackwell Ultra GPU的全面适配，预计于2025年第四季度全面部署‌12。该芯片采用全新FP8精度架构，在保持模型精度的同时将计算密度较上一代提升2.3倍，并支持动态精度切换技术，可针对多模态模型训练任务（如图文联合建模、视频语义解析）自动选择最优计算模式‌7。其配备的HBM4显存架构带宽峰值达1.8TB/s，结合Azure AI服务基础设施的分布式缓存技术，可支持单集群超万卡规模的并行训练任务，尤其适用于处理4K/8K高分辨率图像和长视频数据‌27。 生态层面，微软通过深度整合NVIDIA NIM微服务与TensorRT-LLM加速框架，在Azure AI Foundry中实现多模态模型的全生命周期管理。开发者可通过统一接口调用包含Llama 4、Mistral Small 3.1在内的超20种基础模型，并利用Blackwell平台的硬件级稀疏计算特性，将Meta Llama 4等模型的推理吞吐量提升50%，端到端延迟降低至毫秒级‌23。 无服务器AI Agent自动化 Azure将于2025年Q3推出AI Agent服务，支持通过自然语言描述定义复杂工作流。例如用户输入“自动收集销售数据生成周报，包含环比图表和竞品分析摘要”，系统将通过语义解析引擎拆解为数据抽取、统计分析、可视化生成等12类标准化任务节点‌23。 该服务依托无服务器GPU架构实现动态资源分配： ‌弹性算力调度‌：基于Blackwell Ultra的FP8稀疏计算能力，在执行多模态任务（如图表生成）时自动扩展至128路GPU并行，闲置时则释放资源至共享池‌27； ‌智能批处理‌：采用NVIDIA TensorRT-LLM的动态批处理技术，将周报生成类轻量化任务聚合处理，单批次可完成800个并发请求，综合成本降低40%‌25； ‌物理-数字联动‌：通过与Azure IoT服务集成，支持从工厂传感器实时获取数据并触发Agent工作流，例如设备故障自动生成维修方案并同步至工程师终端‌36。 这套系统还内置Dynamo分布式推理优化引擎，可根据任务复杂度动态选择Blackwell Ultra GPU实例或RTX PRO 6000服务器版显卡，确保从简单文档处理到4K视频分析的端到端响应时间稳定在5秒以内‌。 Azure 无服务器 GPU 通过弹性资源、精细化成本控制和多模态工具链，为开发者提供了高效运行大模型的解决方案。无论是初创企业还是大型机构，均可通过本文所述的实战方法，快速部署低成本、高性能的多模态 AI 应用。","link":"/2025/02/12/Azure%E6%97%A0%E6%9C%8D%E5%8A%A1%E5%99%A8GPU%E5%AE%9E%E6%88%98%EF%BC%9A%E4%BD%8E%E6%88%90%E6%9C%AC%E8%BF%90%E8%A1%8C%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B/"},{"title":"Blackwell Ultra GPU在Azure AI中的未来展望：万亿参数模型训练","text":"随着生成式AI和大语言模型（LLM）的爆发式发展，模型规模已从千亿级迈向万亿级参数时代。这一趋势对算力基础设施提出了前所未有的挑战：如何高效支持超大规模模型的训练与推理？英伟达最新发布的Blackwell架构GPU与微软Azure AI平台的深度融合，为此提供了革命性的解决方案。本文将从Blackwell Ultra GPU的技术革新、Azure AI的生态系统适配、万亿参数模型训练的具体实践，以及未来技术演进的路径展开分析。 Blackwell Ultra GPU：万亿参数模型的硬件基石1.1 架构设计的突破性创新Blackwell架构通过全栈系统性优化，突破了超大规模AI模型训练的算力天花板。其硬件创新体系由三大核心模块构成： （1）超密度计算单元集群基于台积电4NP工艺打造的Blackwell B200 GPU，单芯片集成量级达到2080亿晶体管，较前代Hopper架构提升近2.3倍。通过革命性的双芯硅中介层封装技术，实现两颗B200 GPU与Grace CPU的异构整合，形成GB200 Grace Blackwell超级芯片。其中NVLink 5互联通道突破性达到1.8TB/s带宽，是PCIe 5.0协议的35倍。这种架构创新使得单颗超级芯片可提供20PFLOPS（2×10^16次/秒）的FP4计算能力，对比Hopper GH100的4.45PFLOPS实现4.5倍跃升。在典型混合精度训练场景下，单机架（含36颗GB200）即可承载GPT-4级别模型的完整训练负载。 （2）超低延时通信网络第五代NVLink技术结合Quantum-X800 InfiniBand网络平台，构建了业界首个全光互连AI计算集群。其创新之处在于： 芯片级：通过3D封装硅桥技术，将GPU间点对点延迟压缩至5ns级别 节点级：每个NVLink交换机支持18个800Gb/s端口，单节点双向带宽达14.4TB/s 集群级：采用自适应路由算法，在576 GPU规模下仍保持1.8TB/s的有效带宽实测数据显示，在1750亿参数的GPT-3模型训练中，Blackwell集群仅需64颗GPU即可达到Hopper架构256颗GPU的训练吞吐量，通信效率提升带来4倍硬件利用率跃升。 （3）智能内存子系统采用三星HBM3E堆叠内存技术，单GPU配备192GB显存，8TB/s带宽较前代提升2.1倍。其创新内存架构包含： 智能缓存分区：支持动态划分训练/推理专用缓存区，MoE模型内存占用降低50% 4位浮点压缩引擎：集成第二代Transformer核心，支持FP4/INT4混合精度计算 错误校正增强：引入RAS（可靠性、可用性、可维护性）架构，软错误率降低至1e-36在Llama 2 700B参数模型推理测试中，该内存系统使得上下文窗口可扩展至128k tokens，同时保持1.2ms/token的推理延迟。 1.2 软件栈的协同加速Blackwell通过软硬协同设计释放硬件潜能，构建了三级加速体系： （1）动态资源调度层NVSwitch 4 ASIC芯片搭载智能任务调度算法，可实时分析计算图特征进行负载均衡。其核心创新包括： MoE模型感知：自动识别专家层结构，动态分配计算单元（如将64个专家层映射到32个计算单元） 流水线并行优化：在512路张量并行训练中，流水线气泡率从22%降至7% 显存智能预取：基于LSTM网络预测内存访问模式，预取准确率达93%在Llama 2 700B微调任务中，该技术实现2.2倍性能提升，训练周期从28天缩短至13天。 （2）稀疏计算加速层新一代张量核心集成结构化稀疏处理单元（SSPU），支持2:4稀疏模式压缩。其技术特点包括： 动态掩码生成：每个时钟周期自动检测并屏蔽50%低权重值计算 稀疏张量重构：通过Epsilon修剪算法保留0.1%重要参数，计算密度提升40% 混合精度支持：在FP8稀疏矩阵运算中保持0.1%的精度损失阈值结合TensorRT-LLM的稀疏化编译器，在BERT-Large训练中实现1.8倍吞吐量提升，能耗比达到58 TFLOPS/W。 （3）全栈优化工具链NVIDIA AI Enterprise 5.0套件提供端到端加速方案： CUDA-X AI编译器：支持自动内核融合，将访存密集型操作延迟降低65% NeMo框架增强：引入动态重计算技术，梯度计算内存占用减少40% Triton推理服务器：集成稀疏量化引擎，INT4推理吞吐量达12,000 queries/sec在GPT-4 1.8T参数推理基准测试中，该软件栈使每秒生成token数达到28,500，较开源方案提升7.3倍。 Azure AI的Blackwell深度集成战略解析与实施路径2.1 云计算基础设施的体系化重构 微软Azure正在构建面向下一代AI的智能计算基座，通过多维度的硬件协同设计实现与Blackwell架构的深度融合。其创新实践包含三个核心维度： 2.1.1 超大规模计算实例创新 基于GB200 NVL72液冷机架系统的技术突破，Azure推出”NDv6-Blackwell”战略级计算实例，该实例采用模块化机架设计实现三大创新： 算力密度跃迁：单节点集成72颗Blackwell GPU核心，通过NVLink-HyperCube互联架构形成1.4 exaflops FP8计算能力，相较前代A100集群提升6.8倍峰值算力 内存架构革新：构建30TB统一内存池，采用动态虚拟内存分片技术（DVMT）实现跨GPU内存的零拷贝数据访问，支持单任务万亿参数模型的完整驻留训练 能效比突破：依托相变液冷技术，PUE指标优化至1.08，在78小时持续负载测试中保持98.7%的算力稳定性 2.1.2 智能网络架构升级 针对超大规模分布式训练场景，Azure打造双平面量子网络架构： Spectrum-X800以太网平面：采用自适应路由算法（ARA）实现800Gb/s带宽，时延控制在0.8μs以内，支持动态带宽分配（DBA） Quantum-X800 InfiniBand平面：部署新型拥塞控制协议（QCCP），在4096颗GPU集群中实现93%的链路利用率 混合网络效能：通过双平面负载均衡技术，在ResNet-5000基准测试中，AllReduce操作耗时从传统架构的2.3秒降至0.39秒，通信效率提升5.1倍 2.1.3 自适应计算精度框架 Azure Machine Learning服务与ONNX Runtime深度整合，构建智能精度调节系统： 动态精度感知器：实时监控模型梯度变化，在FP4/FP8/FP16精度间自主切换 混合精度编译器：采用分层计算图优化技术（LCGO），将高精度计算节点压缩率提升至73% 经济效益转化：在BERT-Large训练案例中，通过动态精度分配实现25倍成本下降，其中存储开销降低18倍，计算能耗减少7倍 2.2 开发者生态体系的智能化演进Azure构建全栈式AI工具链，实现与Blackwell架构的生态级融合： 2.2.1 超大规模模型训练体系 基于DeepSpeed与Megatron框架的协同创新，构建三维并行训练体系： 数据并行维度：采用梯度累积优化算法（GAO），在1024节点规模下保持92%的线性扩展效率 流水线并行维度：开发智能微批处理预测器（MBP），流水线气泡率控制在5%以内 张量并行维度：运用Blackwell的TMA（Tensor Memory Accelerator）特性，实现跨GPU张量运算零同步开销 通信优化成果：在GPT-4规模模型训练中，通过混合通信策略（HCS）将通信开销压缩至总耗时的12%，相较传统方案提升3.4倍训练速度 2.2.2 可信AI计算体系 深度整合Blackwell安全引擎，构建硬件级隐私计算解决方案： 数据安全层：采用物理不可克隆函数（PUF）技术，对训练数据实施量子加密存储 计算安全层：通过可信执行环境（TEE）实现模型权重的实时动态加密，支持SGX/SEV多协议验证 审计合规层：内置HIPAA/GDPR合规性验证模块，提供可验证计算证明（VCP） 医疗应用案例：在合作伙伴Mayo Clinic的基因组模型训练中，实现PHI数据全程加密处理，通过监管审计的时间缩短83% 该技术体系已形成完整的AI开发生命周期支持，从底层硬件加速到上层应用合规，构建起面向企业级AI落地的全栈解决方案。通过Blackwell架构的深度集成，Azure在超大规模模型训练场景实现P90成本下降40%，模型迭代速度提升5-8倍的显著突破。 万亿参数模型训练实践：挑战与突破3.1 典型应用场景与行业赋能 Blackwell架构与Azure云平台的深度融合，正在重塑人工智能与科学计算的范式，其应用场景已突破传统边界。以下为三大前沿领域的典型实践案例： 多模态认知智能演进以OpenAI的GPT-5为代表的新一代认知模型，依托Blackwell架构的10万亿参数承载能力，开创了跨模态联合建模的新纪元。通过创新设计的异构数据融合编码器，系统可同步处理文本、图像、视频、点云等多维数据流。关键技术突破在于其分布式张量计算单元（DTU）与高带宽显存（HBM3E）的协同优化，使跨模态推理延迟从传统架构的秒级（3-5秒）降低至毫秒级（&lt;200ms），在Azure智能内容审核系统中实现实时多模态内容理解，处理通量达每分钟1200个复合型请求。 超大规模科学仿真在Azure Quantum量子计算平台上，Blackwell架构正重新定义分子动力学模拟的边界。通过硬件加速的量子-经典混合算法，单次仿真可精确建模1亿原子体系的相互作用，相较传统CPU集群（基于Intel Xeon Platinum 8480+架构），计算效能提升达100倍。在最近的蛋白质折叠预测基准测试中，系统仅用8小时便完成传统集群需34天的计算任务，能量势场计算精度误差控制在0.05kcal/mol以内。这种突破性进展为新材料研发和药物发现提供了原子级精度的数字孪生平台。 自主决策系统进化特斯拉新一代自动驾驶系统的训练实践，展示了Blackwell架构在实时决策领域的变革力量。通过动态场景泛化引擎与参数动态稀疏化技术的结合，系统训练周期从传统架构的3个月缩短至14天，每日数据吞吐能力突破1PB。关键技术突破在于其可扩展的梯度同步协议（GSPv2），在4096个B200 GPU集群中实现98.7%的通信效率，使复杂城市场景的模型迭代速度提升17倍。在实际道路测试中，极端工况（如突发障碍物避让）的决策响应时间缩短至230ms，较上一代系统提升45%。 3.2 技术挑战与创新突破 尽管Blackwell架构展现出革命性性能，万亿级模型训练仍面临多重技术壁垒，相关解决方案体现着工程创新的智慧： 显存墙突破：参数动态分片技术面对单GPU显存（192GB HBM3E）与万亿参数模型间的数量级差距，Azure研发团队提出”分级分片缓存”架构。该方案包含三个创新层： 近存计算层：利用GPU显存部署高频参数子集（约12%） 分布式缓存层：通过NVLink连接的GPU间构建参数交换网格 存储分级层：将冷参数存储在Azure Blob存储的压缩格式中（采用FP8量化压缩算法）配合改进型ZeRO-Offload 3.0策略，实现参数动态迁移的智能预取机制。在实际训练中，显存占用降低72%，同时保持98.3%的计算效率。在GPT-5训练任务中，参数交换带宽达3.2TB/s，延迟控制在7μs以内。 能效优化：智能液冷系统针对B200 GPU 1200W的功耗特性，Azure设计第六代沉浸式液冷解决方案，包含： 相变冷却模块：采用氟化液（3M Novec 7100）的二级相变系统 智能配电单元：基于强化学习的动态功耗调度算法（DL-PowerSched） 余热回收系统：与微软芝加哥数据中心的热能存储装置联动该方案使整体PUE降至1.05，较传统风冷系统节能43%。在持续训练负载下，单机架（含40个B200）散热功耗从78kW降至18kW，同时支持95℃的高温冷却水循环。 可靠性工程：预测式容错机制面对长达数月的持续训练任务，Blackwell的可靠性增强子系统（RAS 2.0）包含三大创新： 硬件健康度预测模型：基于500+传感器参数的LSTM预测网络，提前8小时预判故障 增量式检查点技术：采用差异快照算法，检查点存储开销降低89% 量子加密校验：对梯度参数进行实时量子密钥分发（QKD）验证在Azure的B200超算集群中，系统MTBF提升至5200小时，任务自动迁移成功率99.998%。在最近的连续90天训练任务中，仅发生2次非计划中断，相较上一代架构提升20倍可靠性。 从万亿到十万亿的跨越：技术革命与社会责任的双重变奏4.1 技术演进的三维突破路径（1）三维异构集成革命（2026-2028）英伟达”Blackwell Next”架构将颠覆传统芯片制造范式，采用TSMC 3DFabric先进封装技术，通过硅中介层实现6颗B200 GPU的垂直堆叠。每个计算单元将集成： 12个HBM4E内存堆栈（单颗容量48GB） 384个Tensor Core第三代张量核心 硅光子收发模块（800Gbps/通道）关键技术突破体现在：1）晶圆级键合精度达0.5μm，热阻系数降低至0.15°C/W2）通过自适应电压调节技术，能耗比提升至75PFLOPS/W3）芯粒间互连带宽突破256TB/s，延迟压缩至50psAMD MI300X的实测数据显示，3D封装使AI训练吞吐量提升270%，验证了该技术路线的可行性。 （2）光子计算网络重构（2027-2030）微软Azure正在构建全球首个全光AI超算架构”Project Photon”，其核心技术特征包括： 采用Intel 1.6T硅光引擎的CPO交换机 基于氮化硅波导的片上光网络（密度达8Tb/s/mm²） 分布式相干光传输系统（Q因子&gt;10dB）在悉尼数据中心进行的早期测试中： 实现0.78μs端到端延迟（相比传统RDMA降低83%） 光链路误码率&lt;1E-18，可靠性提升3个数量级 每机架功耗下降42%（从25kW降至14.5kW）该架构将支持跨三大洲8个区域的无缝模型训练，理论最大扩展规模达128k GPU集群。 （3）量子-神经混合范式（2028-）微软Quantum团队开发的拓扑量子比特系统（Majorana费米子体系）将与Blackwell架构深度耦合： 在128量子比特原型机上，成功优化BERT-large的损失曲面 通过变分量子本征求解器（VQE），将梯度下降迭代次数从1E5降至8E3 量子辅助的注意力机制使Transformer推理速度提升6.8倍关键技术里程碑包括：1）量子比特相干时间突破1ms（液氦温区）2）量子门保真度达99.995%（表面码纠错）3）经典-量子混合编译器延迟&lt;5μs（基于Azure Sphere MCU） 4.2 技术伦理的范式重构（1）算法公平性治理体系Azure Responsible AI Dashboard构建了五维评估框架：1）群体公平性：AUC差异&lt;0.02（医疗诊断场景）2）个体反事实公平：特征扰动敏感度&lt;5%3）动态偏差监测：实时检测107种潜在偏见模式4）可解释性引擎：SHAP值可视化权重分布5）道德约束模块：植入Asimov三大定律逻辑层在乳腺癌筛查模型中，通过对抗性去偏技术： 不同族裔的假阴性率差异从18.7%降至2.3% 低收入群体检出率提升27个百分点 模型决策可解释路径达医疗诊断标准ISO 22600-3 （2）可持续计算生态微软环境科学团队开发的AI碳足迹模型显示： Blackwell单卡训练周期（90天）碳排放为12.3tCO2e 通过模型压缩技术（包括： 参数稀疏化（95%权重剪枝） 8位浮点量化（熵保持率99.7%） 知识蒸馏（教师-学生模型Δacc&lt;0.5%）可将能耗强度降至0.34kgCO2e/PFLOPS可再生能源布局： 在冰岛部署地热供电数据中心（基线负载240MW） 挪威海上风电项目年供电量达5.2TWh 钙钛矿光伏幕墙技术（转化效率31.5%）覆盖82%园区建筑 Blackwell Ultra GPU与Azure AI的融合，标志着超大规模AI模型进入工业化生产阶段。从硬件架构的颠覆性创新，到云平台的全栈优化，这一组合不仅突破了万亿参数训练的算力极限，更重新定义了AI基础设施的构建范式。未来，随着光计算、量子混合架构等技术的成熟，十万亿级模型的训练将成为可能，而Azure与英伟达的持续合作，将确保这一进程始终以效率、安全与责任为核心。对于开发者而言，掌握Blackwell与Azure的协同技术栈，将是解锁下一代AI潜能的关键。","link":"/2025/03/15/Blackwell%20Ultra%20GPU%E5%9C%A8Azure%20AI%E4%B8%AD%E7%9A%84%E6%9C%AA%E6%9D%A5%E5%B1%95%E6%9C%9B%EF%BC%9A%E4%B8%87%E4%BA%BF%E5%8F%82%E6%95%B0%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83/"},{"title":"Golang Context包 详解","text":"context 主要用来在 goroutine 之间传递上下文信息，包括：取消信号、超时时间、截止时间、k-v 等。 context 用来解决 goroutine 之间退出通知、元数据传递的功能。 控制并发有两种经典的方式，一种是WaitGroup，另外一种就是Context Value函数并没有任何保证，编译器不会检查传进来的参数是否是合理。 Context 接口Context接口定义 123456789type Context interface { Deadline() (deadline time.Time, ok bool) Done() &lt;-chan struct{} Err() error Value(key any) any} Context 核心方法Context 接口中有四个核心方法：Deadline()、Done()、Err()、Value()。 Deadl() Deadline() (deadline time.Time, ok bool) 方法返回 Context 的截止时间，表示在这个时间点之后，Context 会被自动取消。如果 Context 没有设置截止时间，该方法返回一个零值 time.Time 和一个布尔值 false。 123456deadline, ok := ctx.Deadline()if ok { // Context 有截止时间} else { // Context 没有截止时间} Done() Done() 方法返回一个只读通道，当 Context 被取消时，该通道会被关闭。可以通过监听这个通道来检测 Context 是否被取消。如果 Context 永不取消，则返回 nil。 123456select {case &lt;-ctx.Done(): // Context 已取消default: // Context 尚未取消} Err() Err() 方法返回一个 error 值，表示 Context 被取消时产生的错误。如果 Context 尚未取消，该方法返回 nil。 123if err := ctx.Err(); err != nil { // Context 已取消，处理错误} Value() Value(key any) any 方法返回与 Context 关联的键值对，一般用于在 Goroutine 之间传递请求范围内的信息。如果没有关联的值，则返回 nil。 1234value := ctx.Value(key)if value != nil { // 存在关联的值} 添加值 context.WithValue() 1ctx := context.WithValue(parentCtx, &quot;username&quot;, &quot;Rolle&quot;) 取消Context context.WithCancel() context.WithCancel(parent Context) (ctx Context, cancel CancelFunc) 函数接收一个父 Context，返回一个新的子 Context 和一个取消函数，当取消函数被调用时，子 Context 会被取消，同时会向子 Context 关联的 Done() 通道发送取消信号，届时其衍生的子孙 Context 都会被取消。这个函数适用于手动取消操作的场景。 12ctx, cancelFunc := context.WithCancel(parentCtx) defer cancelFunc() 取消原因 context.WithCancelCause() 与 context.Cause() context.WithCancelCause(parent Context) (ctx Context, cancel CancelCauseFunc) 函数是 Go 1.20 版本才新增的，其功能类似于 context.WithCancel()，但是它可以设置额外的取消原因，也就是 error 信息，返回的 cancel 函数被调用时，需传入一个 error 参数。 12ctx, cancelFunc := context.WithCancelCause(parentCtx)defer cancelFunc(errors.New(&quot;原因&quot;)) context.Cause(c Context) error 函数用于返回取消 Context 的原因，即错误值 error。如果是通过 context.WithCancelCause() 函数返回的取消函数 cancelFunc(myErr) 进行的取消操作，我们可以获取到 myErr 的值。否则，我们将得到与 c.Err() 相同的返回值。如果 Context 尚未被取消，将返回 nil。 1err := context.Cause(ctx) context.WithDeadline() context.WithDeadline(parent Context, d time.Time) (Context, CancelFunc) 函数接收一个父 Context 和一个截止时间作为参数，返回一个新的子 Context。当截止时间到达时，子 Context 其衍生的子孙 Context 会被自动取消。这个函数适用于需要在特定时间点取消操作的场景。 123deadline := time.Now().Add(time.Second * 2)ctx, cancelFunc := context.WithTimeout(parentCtx, deadline)defer cancelFunc() context.WithTimeout() context.WithTimeout(parent Context, timeout time.Duration) (Context, CancelFunc) 函数和 context.WithDeadline() 函数的功能是一样的，其底层会调用 WithDeadline() 函数，只不过其第二个参数接收的是一个超时时间，而不是截止时间。这个函数适用于需要在一段时间后取消操作的场景。 12ctx, cancelFunc := context.WithTimeout(parentCtx, time.Second * 2)defer cancelFunc() Context 的使用场景传递共享数据编写中间件函数，用于向 HTTP 处理链中添加处理请求 ID 的功能。 123456789101112131415161718192021type key intconst ( requestIDKey key = iota)func WithRequestId(next http.Handler) http.Handler { return http.HandlerFunc(func(rw http.ResponseWriter, req *http.Request) { // 从请求中提取请求ID和用户信息 requestID := req.Header.Get(&quot;X-Request-ID&quot;) // 创建子 context，并添加一个请求 Id 的信息 ctx := context.WithValue(req.Context(), requestIDKey, requestID) // 创建一个新的请求，设置新 ctx req = req.WithContext(ctx) // 将带有请求 ID 的上下文传递给下一个处理器 next.ServeHTTP(rw, req) })} 传递取消信号、结束任务启动一个协程，接受到取消信号就停止工作 123456789101112131415161718192021222324252627282930package mainimport ( &quot;context&quot; &quot;fmt&quot; &quot;time&quot;)func main() { ctx, cancelFunc := context.WithCancel(context.Background()) go Working(ctx) time.Sleep(3 * time.Second) cancelFunc() // 等待一段时间，以确保工作协程接收到取消信号并退出 time.Sleep(1 * time.Second)}func Working(ctx context.Context) { for { select { case &lt;-ctx.Done(): fmt.Println(&quot;done...&quot;) return default: fmt.Println(&quot;ing...&quot;) } }} 在上面的示例中，创建了一个 Working 函数，它会不断执行工作任务。使用 context.WithCancel 创建了一个上下文 ctx 和一个取消函数 cancelFunc。然后，启动了一个工作协程，并将上下文传递给它。在主函数中，需要等待一段时间（3 秒）模拟业务逻辑的执行。然后，调用取消函数 cancelFunc，通知工作协程停止工作。工作协程在每次循环中都会检查上下文的状态，一旦接收到取消信号，就会退出循环。最后，等待一段时间（1 秒），以确保工作协程接收到取消信号并退出。 超时控制模拟耗时操作，超时控制 123456789101112131415161718192021222324252627package mainimport ( &quot;context&quot; &quot;fmt&quot; &quot;time&quot;)func main() { // 使用 WithTimeout 创建一个带有超时的上下文对象 ctx, cancel := context.WithTimeout(context.Background(), 3*time.Second) defer cancel() // 在另一个 goroutine 中执行耗时操作 go func() { // 模拟一个耗时的操作，例如数据库查询 time.Sleep(5 * time.Second) cancel() }() select { case &lt;-ctx.Done(): fmt.Println(&quot;操作已超时&quot;) case &lt;-time.After(10 * time.Second): fmt.Println(&quot;操作完成&quot;) }} 执行结果 1操作已超时 在上面的例子中，首先使用 context.WithTimeout() 创建了一个带有 3 秒超时的上下文对象 ctx, cancel := context.WithTimeout(ctx, 3*time.Second)。接下来，在一个新的 goroutine 中执行一个模拟的耗时操作，例如等待 5 秒钟。当耗时操作完成后，调用 cancel() 方法来取消超时上下文。最后，在主 goroutine 中使用 select 语句等待超时上下文的完成信号。如果在 3 秒内耗时操作完成，那么会输出 “操作完成”。如果超过了 3 秒仍未完成，超时上下文的 Done() 通道会被关闭，输出 “操作已超时”。 同时启动多个 goroutine 进行任务处理时，可以使用 Context 来控制这些 goroutine 的执行。在每个 goroutine 中，都可以检测 Context 对象是否被取消，如果是，则退出 goroutine 的执行，否则继续执行。 12345678910111213141516171819202122232425262728293031package mainimport ( &quot;context&quot; &quot;fmt&quot; &quot;sync&quot;)func worker(ctx context.Context, wg *sync.WaitGroup) { defer wg.Done() for { select { default: fmt.Println(&quot;work&quot;) case &lt;-ctx.Done(): return } }}func main() { parent := context.Background() ctx, cancel := context.WithCancel(parent) var wg sync.WaitGroup for i := 0; i &lt; 3; i++ { wg.Add(1) go worker(ctx, &amp;wg) } cancel() wg.Wait()} 什么是WaitGroup它是一种控制并发的方式，它的这种方式是控制多个goroutine同时完成。 12345678910111213141516func main() { var wg sync.WaitGroup wg.Add(2) go func() { time.Sleep(2*time.Second) fmt.Println(&quot;first&quot;) wg.Done() }() go func() { time.Sleep(2*time.Second) fmt.Println(&quot;second&quot;) wg.Done() }() wg.Wait() fmt.Println(&quot;all done&quot;)} 一定要例子中的2个goroutine同时做完，才算是完成 可能会有这么一种场景：需要我们主动的通知某一个goroutine结束。比如开启一个后台goroutine一直做事情，比如监控，定时任务等现在不需要了，就需要通知这个goroutine结束 1234567891011121314151617181920212223func main() { stop := make(chan bool) go func() { for { select { case &lt;-stop: fmt.Println(&quot;break&quot;) return default: fmt.Println(&quot;watch ing&quot;) time.Sleep(1 * time.Second) } } }() time.Sleep(5 * time.Second) fmt.Println(&quot;stop&quot;) stop &lt;- true fmt.Println(5 * time.Second)} 定义一个stop的chan，通知他结束后台goroutine。实现也非常简单，在后台goroutine中，使用select判断stop是否可以接收到值，如果可以接收到，就表示可以退出停止了；如果没有接收到，就会执行default里的监控逻辑，继续监控，只到收到stop的通知。有了以上的逻辑，就可以在其他goroutine种，给stop chan发送值了，例子中是在main goroutine中发送的，控制让这个监控的goroutine结束。 如果有一层层的无穷尽的goroutine，不太好控制 1234567891011121314151617181920func main() { ctx, cancel := context.WithCancel(context.Background()) go func(ctx context.Context) { for { select { case &lt;-ctx.Done(): fmt.Println(&quot;stop,break...&quot;) return default: fmt.Println(&quot;goroutine watching...&quot;) time.Sleep(2 * time.Second) } } }(ctx) time.Sleep(10 * time.Second) fmt.Println(&quot;all done&quot;) cancel() // 为了检测监控过是否停止，如果没有监控输出，就表示停止了 time.Sleep(5 * time.Second)} 重写，就是把原来的chan stop 换成Context，使用Context跟踪goroutine，以便进行控制，比如结束等。context.Background() 返回一个空的Context，这个空的Context一般用于整个Context树的根节点。然后使用context.WithCancel(parent)函数，创建一个可取消的子Context，然后当作参数传给goroutine使用，这样就可以使用这个子Context跟踪这个goroutine。在goroutine中，使用select调用&lt;-ctx.Done()判断是否要结束，如果接受到值的话，就可以返回结束goroutine了；如果接收不到，就会继续进行监控。那么是如何发送结束指令的呢？这就是示例中的cancel函数啦，它是我们调用context.WithCancel(parent)函数生成子Context的时候返回的，第二个返回值就是这个取消函数，它是CancelFunc类型的。我们调用它就可以发出取消指令，然后我们的监控goroutine就会收到信号，就会返回结束。 Context控制多个goroutine1234567891011121314151617181920212223func main() { ctx, cancel := context.WithCancel(context.Background()) go watch(ctx,&quot;【监控1】&quot;) go watch(ctx,&quot;【监控2】&quot;) go watch(ctx,&quot;【监控3】&quot;) time.Sleep(10 * time.Second) fmt.Println(&quot;可以了，通知监控停止&quot;) cancel() // 为了检测监控过是否停止，如果没有监控输出，就表示停止了 time.Sleep(5 * time.Second)}func watch(ctx context.Context, name string) { for { select { case &lt;-ctx.Done(): fmt.Println(name,&quot;监控退出，停止了...&quot;) return default: fmt.Println(name,&quot;goroutine监控中...&quot;) time.Sleep(2 * time.Second) } }} 启动了3个监控goroutine进行不断的监控，每一个都使用了Context进行跟踪，当使用cancel函数通知取消时，这3个goroutine都会被结束。这就是Context的控制能力，它就像一个控制器一样，按下开关后，所有基于这个Context或者衍生的子Context都会收到通知，这时就可以进行清理操作了，最终释放goroutine，这就优雅的解决了goroutine启动后不可控的问题。 如果Context取消的时候，我们就可以得到一个关闭的chan，关闭的chan是可以读取的，所以只要可以读取的时候，就意味着收到Context取消的信号了，以下是这个方法的经典用法。 12345678910111213func Stream(ctx context.Context, out chan&lt;- Value) error { for { v, err := DoSomething(ctx) if err != nil { return err } select { case &lt;-ctx.Done(): return ctx.Err() case out &lt;- v: } } } WithValue传递元数据1234567891011121314151617181920212223242526var key string=&quot;name&quot;func main() { ctx, cancel := context.WithCancel(context.Background()) // 附加值 valueCtx:=context.WithValue(ctx,key,&quot;【监控1】&quot;) go watch(valueCtx) time.Sleep(10 * time.Second) fmt.Println(&quot;可以了，通知监控停止&quot;) cancel() // 为了检测监控过是否停止，如果没有监控输出，就表示停止了 time.Sleep(5 * time.Second)}func watch(ctx context.Context) { for { select { case &lt;-ctx.Done(): // 取出值 fmt.Println(ctx.Value(key),&quot;监控退出，停止了...&quot;) return default: // 取出值 fmt.Println(ctx.Value(key),&quot;goroutine监控中...&quot;) time.Sleep(2 * time.Second) } }} 通过传递参数的方式，把name的值传递给监控函数。在这个例子里，我们实现一样的效果，但是通过的是Context的Value的方式。可以使用context.WithValue方法附加一对K-V的键值对，这里Key必须是等价性的，也就是具有可比性；Value值要是线程安全的。这样我们就生成了一个新的Context，这个新的Context带有这个键值对，在使用的时候，可以通过Value方法读取ctx.Value(key)。记住，使用WithValue传值，一般是必须的值，不要什么值都传递。 1234567891011121314151617181920212223package mainimport ( &quot;context&quot; &quot;fmt&quot;)func main() { ctx := context.Background() process(ctx) ctx = context.WithValue(ctx, &quot;traceId&quot;, &quot;rolle&quot;) process(ctx)}func process(ctx context.Context) { traceId, ok := ctx.Value(&quot;traceId&quot;).(string) if ok { fmt.Printf(&quot;process over. trace_id=%s\\n&quot;, traceId) } else { fmt.Printf(&quot;process over. no trace_id\\n&quot;) }} 运行结果 12process over. no trace_idprocess over. trace_id=rolle Context 使用原则 不要把Context放在结构体中，要以参数的方式传递 以Context作为参数的函数方法，应该把Context作为第一个参数，放在第一位。 给一个函数方法传递Context的时候，不要传递nil，如果不知道传递什么，就使用context.TODO Context的Value相关方法应该传递必须的数据，不要什么数据都使用这个传递 Context是线程安全的，可以放心的在多个goroutine中传递 超时控制 通过context的WithTimeout设置一个有效时间为800毫秒的context。 该context会在耗尽800毫秒后或者方法执行完成后结束，结束的时候会向通道ctx.Done发送信号。 有人可能要问，你这里已经设置了context的有效时间，为什么还要加上这个time.After呢？ 这是因为该方法内的context是自己申明的，可以手动设置对应的超时时间，但是在大多数场景，这里的ctx是从上游一直传递过来的，对于上游传递过来的context还剩多少时间，我们是不知道的，所以这时候通过time.After设置一个自己预期的超时时间就很有必要了。注意，这里要记得调用cancel()，不然即使提前执行完了，还要傻傻等到800毫秒后context才会被释放。总结 上面的超时控制是搭配使用了ctx.Done和time.After。Done通道负责监听context啥时候完事，如果在time.After设置的超时时间到了，你还没完事，那我就不等了，执行超时后的逻辑代码。 12345678910111213141516func AsyncCall() { ctx, cancel := context.WithTimeout(context.Background(), time.Duration(time.Millisecond*800)) defer cancel() go func(ctx context.Context) { // 发送HTTP请求 }() select { case &lt;-ctx.Done(): fmt.Println(&quot;call successfully!!!&quot;) return case &lt;-time.After(time.Duration(time.Millisecond * 900)): fmt.Println(&quot;timeout!!!&quot;) return }} 使用通道123456789101112131415161718func AsyncCall() { ctx := context.Background() done := make(chan struct{}, 1) go func(ctx context.Context) { // 发送HTTP请求 done &lt;- struct{}{} }() select { case &lt;-done: fmt.Println(&quot;call successfully!!!&quot;) return case &lt;-time.After(time.Duration(800 * time.Millisecond)): fmt.Println(&quot;timeout!!!&quot;) return }} 这里主要利用通道可以在协程之间通信的特点，当调用成功后，向done通道发送信号。 监听Done信号，如果在time.After超时时间之前接收到，则正常返回，否则走向time.After的超时逻辑，执行超时逻辑代码。 这里使用的是通道和time.After组合，也可以使用通道和time.NewTimer组合。 子父context1234567891011121314151617181920212223242526package mainimport ( &quot;context&quot; &quot;fmt&quot; &quot;time&quot;)func main() { ctx := context.Background() before := time.Now() preCtx, _ := context.WithTimeout(ctx, 100*time.Millisecond) go func() { childCtx, _ := context.WithTimeout(preCtx, 300*time.Millisecond) select { case &lt;-childCtx.Done(): after := time.Now() fmt.Println(&quot;child during:&quot;, after.Sub(before).Milliseconds()) } }() select { case &lt;-preCtx.Done(): after := time.Now() fmt.Println(&quot;pre during:&quot;, after.Sub(before).Milliseconds()) }} 举一个例子来说明一下 Context 中的级联退出。下面的代码中 childCtx 是 preCtx 的子 Context，其设置的超时时间为 300ms。但是 preCtx 的超时时间为 100 ms，因此父 Context 退出后，子 Context 会立即退出，实际的等待时间只有 100ms。 当把 preCtx 的超时时间修改为 500ms 时： 1preCtx ,_:= context.WithTimeout(ctx,500*time.Millisecond) 从新的输出中可以看出，子协程的退出不会影响父协程的退出。 从上面这个例子可以看出，父 Context 的退出会导致所有子 Context 的退出，而子 Context 的退出并不会影响父 Context。 参考link","link":"/2021/03/11/Golang%20Context%E5%8C%85%20%E8%AF%A6%E8%A7%A3/"},{"title":"Azure无服务器GPU实战：低成本运行多模态大模型","text":"理解Azure的全球基础设施布局微软Azure构建的全球化智能基础设施体系，是其稳居云服务行业领导地位的关键竞争力。依托分布式区域部署策略，Azure在全球范围内形成以合规性为基础、低时延为导向的服务矩阵。截至目前，该平台已实现横跨五大洲140余个国家的业务覆盖，构建起由超过60个区域（Region）组成的服务网络，且规模仍在持续扩展。每个区域均由若干高可用性数据中心集群构成，通过智能流量调度和冗余架构设计，为不同地理位置的用户提供合规可靠、响应迅捷的云端服务。 1.1 Azure区域核心解析 在Azure云架构中，”区域（Region）”定义为由互联数据中心集群构成的逻辑服务单元，每个区域集成独立的数据存储、计算资源与网络基础设施。其全球化布局设计聚焦三大核心价值： 业务连续性保障：依托区域内多数据中心冗余架构，支持自动故障转移与负载均衡，确保服务高可用性（SLA高达99.99%）。 性能优化体验：借助微软全球骨干网与边缘节点，实现用户就近接入，显著降低网络延迟（典型场景延迟&lt;50ms）。 跨域灾备能力：支持跨区域数据同步与业务容灾，构建多层级灾难恢复体系（RPO/RTO可定制化配置）。 1.1.1 区域差异化竞争力 基础设施韧性：最低双数据中心部署标准，关键区域采用3+可用区设计（如东南亚区域含新加坡、香港等多可用区）。 智能网络架构：部署SDN（软件定义网络）与ExpressRoute专线，确保区域间传输带宽&gt;100Gbps。 合规驱动创新：各区域同步更新当地数字主权政策（如欧盟区域的GDPR合规数据中心认证）。 1.2 地理位置（Geography）战略架构Azure的”地理位置”是覆盖多区域的战略单元，其设计深度融合地缘政治与市场特性，主要解决： 数据主权约束：在法律敏感地区（如瑞士、阿联酋）设立独立地理位置，确保数据物理边界符合当地法规。 市场定制化服务：针对特定经济区（如东盟、非洲自由贸易区）提供本地化解决方案套件。 地缘风险隔离：通过地理位置间的物理隔离（如美国与欧洲地理位置间距&gt;1000公里），规避区域性灾难的级联影响。 1.2.1 全球布局全景图 北美板块： 商业云集群：美国东部/西部等8大区域，支持FedRAMP中等/高等合规 政府专用云：DoD IL5/IL6认证区域，满足CMMC 2.0标准 新兴市场枢纽： 巴西圣保罗区域（拉丁美洲唯一Tier IV数据中心） 南非约翰内斯堡/开普敦双区域布局，覆盖整个非洲大陆 亚太经济走廊： 中国节点（由世纪互联运营，通过等保2.0三级认证） 日本东京/大阪区域（取得ISMAP政府云资质） 1.3 数据治理体系Azure构建了分级数据治理框架，通过技术+合规双重机制保障： 1.3.1 主权数据控制层 物理隔离方案：在德国采用”数据受托人”模式，微软无法直接访问客户数据 加密主权保障：支持客户自持密钥（BYOK）及HSM硬件安全模块（FIPS 140-2 Level 3验证） 跨境传输机制：通过欧盟标准合同条款（SCCs）规范跨地理位置数据传输 1.3.2 合规认证矩阵 行业专项认证： 金融业：PCI DSS v3.2.1、SWIFT CSP 医疗健康：HIPAA/HITECH、HITRUST CSF 国际标准体系： 信息安全管理：ISO 27001/27017/27018三重认证 云安全联盟：STAR Attestation金牌认证 动态合规引擎：实时监控全球150+合规要求变更（如沙特阿拉伯的PDPL、泰国的PDPA），通过Azure Policy自动实施配置管控 此架构设计使企业能够根据业务场景灵活选择部署模式： 敏感数据场景：采用主权云架构（如Azure China由本土运营商独立运营） 全球化业务场景：利用区域对（Region Pair）实现异步复制（复制距离&gt;300英里） 混合云需求：通过Azure Stack HCI实现边缘位置与公有云区域的统一管理 Azure全球部署战略2.1 区域选择策略在Azure云环境部署中，区域选择是构建高效云架构的核心决策要素，直接影响服务性能、运营成本、合规适配性及业务连续性。 2.1.1 战略评估维度实施部署前需进行多维度评估： 性能优化策略 就近部署原则：选择与目标用户群体地理距离最近的Azure区域，可有效降低网络延迟（通常可减少30-50ms） 网络拓扑优化：利用Azure全球骨干网络优势，优先选择具备Premium Tier网络连接的区域 合规与法律适配 数据主权要求：GDPR合规区域（如西欧）、HIPAA适用区域（美国中部）等特殊合规区域筛选 行业认证资质：金融行业优先选择PCI DSS认证区域，政府机构选择FedRAMP授权区域 成本控制模型 区域性价差分析：对比计算型资源（如Dv3系列VM）在不同区域的每小时费率差异 流量成本预测：评估跨区域数据传输可能产生的egress费用 业务连续性保障 配对区域选择：遵循Azure预设的区域配对机制（如美国东部2与美国中部） SLA等级匹配：关键业务系统优先选择提供99.99% SLA的可用区支持区域 2.1.2 业务场景化选型指南 本土化业务：用户集中单一地区时采用”主区域+本地冗余存储”架构（如日本关东区域部署） 全球化业务：实施多活架构（如西欧+东南亚双区域部署），配合流量管理器实现智能路由 混合云场景：优先选择与本地数据中心建立ExpressRoute直连的区域 2.2 全球化基础设施布局Azure构建了业界领先的全球基础设施网络，覆盖60+区域及190+国家，形成三级服务节点体系： 核心战略区域集群 北美双枢纽：美国东部（弗吉尼亚）、美国西部2（华盛顿）构成Tier-1骨干节点 欧洲双中心：荷兰AMS、德国FFT区域组成GDPR合规核心区 亚太黄金三角：日本东京、新加坡、澳大利亚东南部形成环太平洋服务圈 新兴市场前沿部署 非洲战略节点：南非约翰内斯堡、阿联酋迪拜区域提供北非/撒哈拉以南覆盖 南美双引擎：巴西圣保罗（南美最大互联点）、墨西哥克雷塔罗区域 主权云专属区域 中国独立运营区：由世纪互联运营的北京/上海区域 政府云专区：US Gov Virginia、Germany Black Forest等政府专属区域 区域服务能力矩阵 区域类型 典型代表 核心能力 全球核心区域 美国东部 全服务可用区支持 国家云区域 中国北部2 本地合规定制化 前沿区域 卡塔尔中部 新兴市场优先接入 实践应用与最佳实践3.1 云资源部署策略优化 在Azure云环境部署中，建议企业采用以下策略实现服务效能与稳定性的双重提升： 业务驱动型区域规划• 需求导向部署：通过用户地域分布分析、合规性审查（数据主权/隐私法规）及业务连续性要求评估，建立区域选址矩阵• 智能灾备架构：构建跨区域多活部署体系，集成Azure Site Recovery实现分钟级RTO，确保99.99%业务可用性• 成本效能平衡：运用Azure Pricing Calculator进行跨区域TCO模拟，结合预留实例与冷热数据分层存储策略优化成本 3.2 全链路性能增强方案 地理拓扑优化• 边缘计算部署：基于Azure全球60+区域布局，采用Traffic Manager实现用户请求的智能地理路由• 多活服务网格：通过Azure Front Door建立跨区域应用网关，构建智能故障转移与蓝绿发布机制• 动态资源编排：运用Azure Autoscale配合Application Gateway实现七层负载均衡，构建弹性计算资源池 智能运维保障• 全栈监控体系：集成Azure Monitor与Application Insights，建立服务等级目标(SLO)的可视化仪表盘• 预测性优化：利用Azure Machine Learning进行历史流量模式分析，实现容量规划的AI预配置• 混沌工程实践：通过Azure Chaos Studio定期执行故障注入测试，验证跨区域容灾有效性 微软Azure以构建全球化智能云矩阵为目标，通过覆盖140+国家/地区的60+区域节点打造数字神经中枢。其基础设施采用”区域+地域+可用区”三级架构，形成具备智能冗余的网格化服务体系。这种多层次部署策略既满足欧盟GDPR等120+合规认证要求，又通过动态扩展架构实现计算资源毫秒级响应。凭借AI驱动的资源编排引擎和混合云互联技术，企业可精准实现工作负载的全球拓扑优化，在数据主权保障、延迟敏感型应用部署及绿色能效管理间获得平衡支点。 常见问题解答（FAQ） Q: Azure区域和地理位置有什么区别？ A: 区域是数据中心的集合，而地理位置是包含多个区域的更大范围市场单位，主要考虑数据驻留和合规性要求。 Q: 为什么要选择特定的Azure地理位置？ A: 选择特定地理位置主要是为了满足数据主权、合规性要求和确保最佳服务性能。 Q: Azure如何确保数据驻留合规性？ A: 通过地理位置边界限制，确保数据始终存储在指定地理范围内。 Q: 美国政府专用区域有什么特殊之处？ A: 这是专门为美国政府机构设计的独立环境，普通用户无法访问。 Q: Azure如何在全球范围内确保服务质量？ A: 通过战略性地部署数据中心、实施冗余机制和持续监控来保证服务质量。","link":"/2024/12/14/Azure%20%E4%BA%91%E8%AE%A1%E7%AE%97%E5%9F%BA%E7%9F%B3%EF%BC%9A%E5%85%A8%E7%90%83%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD%E6%9E%B6%E6%9E%84%E5%85%A8%E6%99%AF%E9%80%8F%E8%A7%86/"},{"title":"记一次MySQL死锁排查过程","text":"背景大概说一下业务场景，需要定时计算一些数据，从其他系统、接口拉取达到的数据比较多，然后经计算后的值存储在本系统中。拉取的数据量可能有万左右，然后以主键存在的数据是需要更新的。不存在则插入。每次做全量更新/插入。 起因最开始采用的方法是先查询，数据存在则更新数据，不存在则插入数据。但是数据要求的时效性比较高。于是定时任务在做任务处理的时候频率就比较高了。就出现了单位时间内对数据库的读写高，于是就换了一个方法。用INSERT … ON DUPLICATE KEY UPDATE。 对数据库的读写次数虽然比之前少了，但是又引发了一个新的问题，因为更新、插入的数据量多，所以导致与一条INSERT … ON DUPLICATE KEY UPDATE的执行时间有点长，大概5s，去研究了下。实际上一次批量插入几千条条数据。为了解决这个问题，就分组批量查询，分为了每 50 条数据一组，这样每条sql 执行的时间也就短了。 随之又出现了另外一个问题，随着数据量的增加，一次循环拉取的数据经计算,写入,更新等。时间大于定时任务所处理的时间，这样就导致与上一个定时任务还没处理完的时候，下一个定时任务又进来处理数据了。 于是选择的做法是在每处理一组数据的时候，把 redis 的key 延长一点时间。然后整组数据处理完的时候，再删除 redis 的 值。等下一次定时任务抢到锁了再进来处理。 想到的第二个方案是，直接插入数据库，主键冲突就抛错，根据指定的错来更新值","link":"/2019/09/11/MySQL%E6%AD%BB%E9%94%81/"},{"title":"count(*) 的实现方式","text":"InnoDB引擎在执行count(*)的时候，需要把数据一行一行地从引擎里面读出来，然后累积计数 MyISAM 引擎把一个表的总行数存在了磁盘上，因此执行 count(*) 的时候会直接返回这个数，效率很高； 那为什么 InnoDB 不跟 MyISAM 一样，也把数字存起来呢？这是因为即使是在同一个时刻的多个查询，由于多版本并发控制（MVCC）的原因，InnoDB 表“应该返回多少行”也是不确定的。这里，用一个算 count(*) 的例子来为你解释一下。 假设表 t 中现在有 10000 条记录，我们设计了三个用户并行的会话。 会话 A 先启动事务并查询一次表的总行数； 会话 B 启动事务，插入一行后记录后，查询表的总行数； 会话 C 先启动一个单独的语句，插入一行记录后，查询表的总行数。 我们假设从上到下是按照时间顺序执行的，同一行语句是在同一时刻执行的。 会话A 会话B 会话C begin select count (*) from t insert into t (写入一行数据) begin insert into t(写入一行数据) select count(*) from t(返回10000) select count(*) from t(返回10002) select count(*) from t(返回10001) 在最后一个时刻，三个会话A，B，C会同时查询表t的总行数，但拿到的结果却不同 这和 InnoDB 的事务设计有关系，可重复读是它默认的隔离级别，在代码上就是通过多版本并发控制，也就是 MVCC 来实现的。每一行记录都要判断自己是否对这个会话可见，因此对于 count(*) 请求来说，InnoDB 只好把数据一行一行地读出依次判断，可见的行才能够用于计算“基于这个查询”的表的总行数。 InnoDB 是索引组织表，主键索引树的叶子节点是数据，而普通索引树的叶子节点是主键值。所以，普通索引树比主键索引树小很多。对于 count(*) 这样的操作，遍历哪个索引树得到的结果逻辑上都是一样的。因此，MySQL 优化器会找到最小的那棵树来遍历。在保证逻辑正确的前提下，尽量减少扫描的数据量，是数据库系统设计的通用法则之一。 如果你用过 show table status 命令的话，就会发现这个命令的输出结果里面也有一个 TABLE_ROWS 用于显示这个表当前有多少行，这个命令执行挺快的，那这个 TABLE_ROWS 能代替 count(*) 吗？ 索引统计的值是通过采样来估算的。实际上，TABLE_ROWS 就是从这个采样估算得来的，因此它也很不准。有多不准呢，官方文档说误差可能达到 40% 到 50%。所以，show table status 命令显示的行数也不能直接使用。 总结一下 MyISAM 表虽然 count(*) 很快，但是不支持事务； show table status 命令虽然返回很快，但是不准确； InnoDB 表直接 count(*) 会遍历全表，虽然结果准确，但会导致性能问题。 对于 count(主键 id) 来说，InnoDB 引擎会遍历整张表，把每一行的 id 值都取出来，返回给 server 层。server 层拿到 id 后，判断是不可能为空的，就按行累加。 对于 count(1) 来说，InnoDB 引擎遍历整张表，但不取值。server 层对于返回的每一行，放一个数字“1”进去，判断是不可能为空的，按行累加。 对于 count(字段) 来说： 如果这个“字段”是定义为 not null 的话，一行行地从记录里面读出这个字段，判断不能为 null，按行累加； 如果这个“字段”定义允许为 null，那么执行的时候，判断到有可能是 null，还要把值取出来再判断一下，不是 null 才累加。 按照效率排序的话，count(字段)&lt;count(主键id)&lt;count(1)≈&lt;count(*) 尽量使用count(*) 参考极客时间MySQL实战45讲","link":"/2018/03/11/MySQL%E7%9A%84count%E8%A7%A3%E8%AF%BB/"},{"title":"String、StringBuffer、StringBuilder三者之间的区别","text":"吧啦吧啦，今天在公司做算法题的时候，不仅就想写下了 String是不可变类，所以任何对String的操作都将引发新的String对象的生成。但是StringBuffer是可变类，任何对StringBuffer所指代的字符串改变都不会产生新的对象。 新引入的StingBuilder类不是线程安全，但其在单线程中的性能比StringBuffer高。 下面是一点小例子 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103import java.util.ArrayList;import java.util.Iterator;import java.util.List;/** * 从JDK1.5中,有了StringBuilder。 */public class DifferenceStringBufferAndStringBuilder { private static final String base = &quot;String&quot;; private static final int count = 3000000; public static void main(String[] args) { stringTest(); stringBufferTest(); stringBuilderTest(); addToStringBuilder(); addToStringBuffer(); } /** * string执行性能测试 */ public static void stringTest() { long begin, end; begin = System.currentTimeMillis(); String test = new String(base); // 在这里为什么要缩150，因为其实时间是很长的 for (int i = 0; i &lt; count / 150; i++) { test = test + &quot;add&quot;; } end = System.currentTimeMillis(); System.out.println((end - begin) + &quot;millis has elapsed when used String&quot;); } /** * stringBuffer */ public static void stringBufferTest() { long begin, end; begin = System.currentTimeMillis(); StringBuffer stringBuffer = new StringBuffer(base); for (int i = 0; i &lt; count; i++) { stringBuffer.append(&quot;add&quot;); } end = System.currentTimeMillis(); System.out.println((end - begin) + &quot;millis has elapsed when used StringBuffer&quot;); } /** * stingBuilder */ public static void stringBuilderTest() { long begin, end; begin = System.currentTimeMillis(); StringBuilder stringBuilder = new StringBuilder(base); for (int i = 0; i &lt; count; i++) { stringBuilder.append(&quot;add&quot;); } end = System.currentTimeMillis(); System.out.println((end - begin) + &quot;mills has elapsed when used StringBuilder&quot;); } /** *转换为StringBuilder */ public static String appendItemsToStringBuilder(List list){ StringBuilder stringBuilder = new StringBuilder(); for (Iterator i = list.iterator();i.hasNext();){ stringBuilder.append(i.next()).append(&quot;&quot;); } return stringBuilder.toString(); } public static void addToStringBuilder(){ List list = new ArrayList(); list.add(&quot;l&quot;); list.add(&quot;y&quot;); list.add(&quot;z&quot;); System.out.println(DifferenceStringBufferAndStringBuilder.appendItemsToStringBuilder(list)); } public static String appendItemsToStringBuffer(List list){ StringBuffer stringBuffer = new StringBuffer(); for (Iterator i = list.iterator();i.hasNext();){ stringBuffer.append(i.next()).append(&quot;&quot;); } return stringBuffer.toString(); } public static void addToStringBuffer(){ List list = new ArrayList(); list.add(&quot;l&quot;); list.add(&quot;y&quot;); list.add(&quot;z&quot;); System.out.println(DifferenceStringBufferAndStringBuilder.appendItemsToStringBuffer(list)); }} 最后输出的是 123451127millis has elapsed when used String86millis has elapsed when used StringBuffer35mills has elapsed when used StringBuilderlyzlyz 所以根据结果来看，采用String对象时，哪怕是次数是其他对象的1/150,执行时间上也比其他对象高很多，而采用StringBuffer对象和采用StringBuilder对象也有明显的差距。所以如果是在单线程下运行，就不必考虑到线程同步的问题，优先采用StringBuilder类，当然，如果是要保证线程安全的话，就要考虑到StringBuffer了。 除了对多线程的支持不一样的话，其实这两个类没啥区别的，上面不就很好的说明了嘛。","link":"/2017/11/23/String%E3%80%81StringBuffer%E3%80%81StringBuilder%E4%B8%89%E8%80%85%E4%B9%8B%E9%97%B4%E7%9A%84%E5%8C%BA%E5%88%AB/"},{"title":"2020年总结","text":"终于、拖到了现在来整理年终总结（其实早就写得差不多）了。原计划是在年前换完工作然后就写这篇总结的，但是又有一些小插曲，所以也不得不将选择的时机放在年后了。大概也有前年出差的那段时间令我不是很愉快，具体的事情也就没有必要在这里写了。成年人的世界里没有容易的，需要做的事情就是要不断让自己成长。有一天能够有更多的主动选择权，而不是被动的选择。 读书本来立的flag，阅读量是要有50本的。但实际读过的书比去年都还少了，不管是技术类的还是非技术类的书籍。这一点有一小部分原因是加班居多 ？但是终于把2019年给2020年立的flag坚持读英语&amp;&amp;背英语单词，完成度90%差不多吧，还是有慢慢在坚持（但是2021开端又停下来了，摸狗头）原本计划的是练一练口语，出国玩的时候也会方便一些（不如找一个英语还可以的女朋友，是真的对英语好的人莫名有好感，做梦？）。以及在看一些技术文档的时候，可以更好的看原汁原味的信息。 关于健康从最开始的疫情笼罩下，开始自己做饭。也解锁了不少菜谱。详情可以点击这里 香糯软滑的香菇鸡肉粥是真的超级香。比起2019年在吃(点外卖)这一块儿的账单，确实是少了不少，还是坚信自己动手丰衣足食。 健康不单单是吃这一块儿。良好的身体素质也会显得额外重要。不然怎么修福报(996)，2020年期间，犹豫上半年是基本上没咋锻炼(老生常谈的话题了，疫情)，在5月起就开始了户外锻炼。还是以跑步为主，隔几个周去奥森。以及平时晚上的时候在公司的健身区那一块儿锻炼。慢慢，5km也终于能够破20min了，10km也能破44min。我也不知道这水平是啥样，反正没去对比。还是想的是有一天能够参加半马就可以了，成绩也不至于太难看就行了。全马我就不了，这玩意没必要追求这么高。为忙碌的工作之余加强身体的锻炼就行了。 关于家人原本是计划在2020年的4月或者是10月的样子带家人来北京玩一趟的，因为趁我还在北京，以及我对朋友说过，老舍先生的《四世同堂》里说过“春天要住杭州 ，夏天住济南 ，秋天住北平”，然后想到今年不太可能，我就计划在国庆的时候回家看一看家人，以及把侄女和我妈带到成都去逛了一圈。大概是吧，工作了后发现时间大部分都不太算属于自己的，特别是工作的属性带有“一线城市”&amp;&amp;“互联网”这两个属性。尽管还有点小，很多东西仅局限于表面。看到的一些东西可能就是花花绿绿的。并不知道代表的是啥。但是我还是对我侄女保持“我知道的东西我会带她们玩的过程中科普给她们”，一方面是有个简单的初步认识，另外一个方面万一也能激起其兴趣呢。 技能陆陆续续阅读了部分源代码，大部分都是go语言相关的，当然还有一部分框架相关的。已经有提过pr（不过还是规范上的问题），希望在这一块儿，能在2021继续进行下去。主要是觉得一毕业后的简历，都要抹平大学竞赛所获得过的奖。所以自从成为了打工人后的简历尤其的平常（说好的为开源做贡献和打比赛，我承认太难了，而我又太菜了）而且源代码又有太多值得学习的地方 开源精神良好 弥补在技能上的不足 作为一个coder的兴趣 希望提升自身的 reputation（任重而道远） 不过这一块儿的水还是很深，需要花的时间太多，而又没有在自己blog上好好写下一些技术文章，主要也是觉得目前所掌握的不够深度，2021慢慢写起来吧。 选择身边的人陆续有去发展迅猛的公司，也有人选择离开北京去成都生活。勇于去改变的生活是极好的。前不久和朋友去北京天文馆(这是2021，主要是觉得比较符合这里提到的，所以额外加了一点)。看到宇宙之浩大“无论是否相信平行宇宙的存在，我们能看到的始终只有一条时间线，当我们选择了 B后，我们就永远不会知道 A 的结果是什么。同时，比起承认自身能力的不足，我们更倾向于把失利归咎于关键节点上错误的选择，那么客观的 review 选择就更是难上加难，只能进行简单的分析来做一些预测。” 关于工作部门领导，以及同事。这些实在是换的勤，人员流失大。先不管这些，主要也是对物流行业确实提不上兴趣，从内心来看还是倾向于去做c端的东西。以及预料到了会在2020年下半年加班比较多，倒也不是排斥加班，主要也是要有成长吧。很多时候我周末如果不去约同学吃饭，以及不出去玩。大概有一半的时间我会选择去公司学习，一方面是因为公司的气氛好一些。到了下半年就果然无止境的加班了，基本上从6月起就是单休的样子了，这也为我积攒了部分假期。还是期望后期去一个真的想去的公司和方向吧。 失败与展望今年感觉过得挺失败的，部分焦虑与部分乐观依然并存。这里的失败，指的是一些行动力的低下。比如每年结束&amp;&amp;第二年开始的时候都会展望比较好的flag，唯一可能就是保持一年2次回家看家人和经常关注侄女的成长。眼看毕业3年就快过去了，但是能拿得手的东西其实没啥。倒是有在年初的时候计划未来要去哪一个城市并且位置付出了行动。这篇文章中不会列出详细计划，因为觉得主要要是要加强自我行动力。关于2021，希望可以在修福报的同时还可以解锁一些其他的事情，生活并不是只有工作。","link":"/2021/01/01/2020%E5%B9%B4%E6%80%BB%E7%BB%93/"},{"title":"合适以及为何使用最少使用(LFU)缓存与Golang中的实现","text":"[译]合适以及为何使用最少使用(LFU)缓存与Golang中的实现在过去的这些年，参与计算机科学和工程师的人们一直在努力优化各种性质。我们生活在一个资源有限的世界里，人们一直致力于优化成本和速度的方法。 在软件工程方面而言，我认为，最流行的改善性能的就是缓存了。在许多app都有缓存，依赖于软件方面的存储，缓存背后的想法非常简单。为了加载较快，存储数据经常被用到。 事实上，缓存必须在两个方面很快 确保尽可能多的文件请求(缓存命中)，而不是通过网络或者主内存(没有命中) 使用它的开销应该比较小，测试人员决定何时更换文件 在这篇文章中，我们将会关注第二部分。对最不常用的缓存采取特定的实现方法，并使成员资格测试和驱逐算法具有良好的性能。并且，我们还将介绍基础知识并探究这种缓存方案可用的地方。 基础LFU是一种缓存算法。只要达到缓存的容量限制，就会删除缓存中最不常用项。这意味着对于缓存中的每个项目，我们必须跟踪它的使用频率。一旦超过了容量，讲运用驱逐算法，从缓存中挑选和过期（移除）项目。 如果你之前实现过LFU缓存，你可能已经考虑使用最小堆数据结构。因为它对数时间复杂度处理插入，删除和更新。在这篇文章中，我们将介绍另一种实现它的方法。 但在我们进入实施之前，让我们看看LFU在哪些情况下比替代品更好。 LFU闪耀点想象一下CDN上的资产缓存，其中资产根据使用模式进行缓存。因此，当用户在网页上请求加载一些图片时，此CDN会将其添加到缓存中，以便其他用户更快获取它。 例如,一个这样的图像(资产)是网站的标志，你能想象一天有多少次谷歌的标识被要求在他们的所有产品上。我真的很想找到这个数字，但就目前而言，我们可能会认同这个数字是庞大的。 这种资产缓存是LFU缓存的完美用例。LFU缓存逐出算法永远不会驱逐频繁访问的资产。事实上，在这样的缓存中，谷歌的微标几乎将永远缓存，相比之下。如果由于Reddit，Slashdot和Hackernews首页上的新产品的新登录页面而有任何图像将会访问，一旦超级风暴过去，资产将被驱逐得更快，因为访问频率将急剧下降，尽管在过去几天他们已经被访问过很多次。 正如你可能已经注意到的那样，在缓存对象的访问模式不经常更改的情况下。这种缓存逐出的方法非常有效。虽然LRU缓存将驱逐最近无法访问的资产，但LFU驱逐方法将在炒作结束后逐出不再需要的资产。 实现LFU缓存现在，让我们来了解它，如我们之前所说的。我们不是将min-heap视为可能支持LFU缓存的可能数据结构，而是考虑采用更好的方法。 事实上，在2010年，一组研究人员Ketan Shah教授，Anirban Mitra和Dhruv Matani发表了一篇题为“用户实现LFU缓存驱逐方案的O(1)算法”的文章（你可以点击这里查看），在这文章中他们解释LFU缓存的实现，其运行的时间复杂度为O（1）,用于其所有操作，包括插入，访问，和删除(驱逐)。 在此，我将向你展示如何实现此缓存并引导你完成实现。 数据结构不，它不会是某种科学怪人的红黑树，事实上，它是两个双向链表和一个哈希表。是的，就是这样。 为了能够理解LFU实现的基本原理，让我们将链表和哈希表看做图形。在我们查看实际图形之前，我们需要了解如何使用哈希表和链接列表。 哈希表将使用通过哈希算法处理的密匙存储所有项目(为了我们的目的，我们 可以保持简单)，值将是实际项目。 链表有点复杂，第一个将是”频率列表“，它将具有所有访问频率。此列表中的每一个节点都有一个项目列表。该列表将包含已使用相应频率访问的所有项目。此外，项目列表中的每一个项目都会在频率列表中指向其祖先。 如果我们查看上面的图形例子，我们可以注意到项A，B，C和D已被访问过一次。E和F项已被访问过4次，依据类推。蓝线是项列表中的每个项都与频率列表中的祖先有关的指针。 那么，如果再次访问项E会发生会发生什么？让我们完成以下步奏：1. 从哈希表中检索项很容易（并且很好地扩展）O（1）。 2. 我们将访问项的frequencyParent指针，从中我们可以检查列表中的下一个频率是什么。3. 如果存在新频率(列如8)，我们将其作为频率节点8下的项目列表的第一项。4. 如果新频率不存在，我们将创建频率节点8并将节点8添加E到项列表中. 就是这样，检索项并刷新项的频率是O（1）,在我们开始实现访问算法前，让我们首先建立我们需要的基本类型。 类型如我们之前所说，我们需要对所需的类型进行建模，这些类型将成为我们缓存的主干。 第一个结构将是CacheItem,这将是将存储在缓存中的实际项目。 1234567type CacheItem struct { key string // Key of entry value interface{} // Value of item frequencyParent *list.Element // Pointer to parent in cacheList} 它包含我们可以在哈希表中查找它的键，值是实际的缓存项，以及指向频率列表中的frequencyParent指针。 下一个结构是FrequencyItem，它表示频率列表中的每一个项。它包含一组条目，这些条目将是一组CacheItem指针，我们将使用map来存储它，以便我们可以将其视为一个集合，它只包含唯一的项。 123456type FrequencyItem struct { entries map[*CacheItem]byte // Set of entries freq int // Access frequency} 我们需要具有平滑运行缓存的最后一个结构就是Cache本身。 12345678type Cache struct { bykey map[string]*CacheItem // Hashmap containing *CacheItems for O(1) access freqs *list.List // Linked list of frequencies capacity int // Max number of items size int // Current size of cache} Cache将包含hash键，称为bykey(命名来自上面链接的文件)，频率列表称为freqs，缓存的最大容量称为容量，缓存的大小表示任何给定缓存的项目数时刻。 New, set &amp; get让我们看看使缓存工作所需的前三个函数。第一个是一个小构造函数： 1234567891011func New() *Cache { cache := new(Cache) cache.bykey = make(map[string]*CacheItem) cache.freqs = list.New() cache.size = 0 cache.capacity = 100 return &amp;c} 构造函数New将创建一个新的Cache结构，并将所有默认值设置为它。如果你想知道list.New（）是如何工作的：对于频率列表，我们将使用Go的容器/列表包，其中包含一个整洁的链表实现。你可以查看其文档以获取更多详细信息。 将在Cache上实现的第二个函数是Set函数： 12345678910111213141516func (cache *Cache) Set(key string, value interface{}) { if item, ok := cache.bykey[key]; ok { item.value = value // Increment item access frequency here } else { item := new(CacheItem) item.key = key item.value = value cache.bykey[key] = item cache.size++ // Eviction, if needed // Increment item access frequency }} 该函数将缓存键和实际值/项缓存为参数。然后，它检查项目是否已经缓存。如果它被缓存，它只会更新项目的值。否则，它将创建一个新的CacheItem，它将封装实际值，它将设置密钥，它将把项添加到bykey哈希表，它将增加缓存的大小。 现在，在两个逻辑分支中，我为缺失的部分添加了一些注释：1。缓存必须知道如何增加aCacheItem的访问频率，但我们还没有实现它; 2.如果大小达到容量，缓存必须知道如何根据访问频率逐出项目。 我们将保留这些注释，直到我们实现增量和逐出函数。 Cache将接收的第三个函数是Get - 通过哈希表中的键访问项目并返回它： 12345678910func (cache *Cache) Get(key string) interface{} { if e, ok := cache.bykey[key]; ok { // Increment acess frequency here return e.value } return nil} 这里也没有魔法 - 我们检查bykey散列表是否包含带有key参数的值，如果存在则返回它。否则，我们返回零。在这里，就像在Set中一样，我们将保留占位符注释，一旦我们实现它就必须添加频率增量函数调用。 更新访问频率正如我们已经看到的，对于缓存的每个访问操作，我们必须更新所访问项的访问频率。 让我们看一下我们的Increment函数必须采取的步骤。首先，对于要过期的项目，我们将不得不决定该项目是否已经是哈希表和频率列表的一部分。如果是，我们将不得不在频率列表中找到它的新频率值和下一个频率位置（节点）。 其次，我们必须弄清楚对于新频率，频率列表中是否已经存在节点。如果有，我们将不得不将该项添加到其条目列表中并分配其新的访问频率（即当前访问频率+ 1）。如果没有，我们将不得不在频率列表中创建一个新的频率节点（并设置其所有合理的默认值），然后将该项添加到其条目列表中 第三，一旦我们检测到FrequencyParent，我们的函数就必须将新的父项设置为正在递增的项，并将其添加到父项的列表中。 作为最后一步，增量函数将从旧频率节点（frequencyParent）的条目中删除该项目。 这是Golang代码： 1234567891011121314151617181920212223242526272829303132func (cache *Cache) increment(item *CacheItem) { currentFrequency := item.frequencyParent var nextFrequencyAmount int var nextFrequency *list.Element if currentFrequency == nil { nextFrequencyAmount = 1 nextFrequency = cache.freqs.Front() } else { nextFrequencyAmount = currentFrequency.Value.(*FrequencyItem).freq + 1 nextFrequency = currentFrequency.Next() } if nextFrequency == nil || nextFrequency.Value.(*FrequencyItem).freq != nextFrequencyAmount { newFrequencyItem := new(FrequencyItem) newFrequencyItem.freq = nextFrequencyAmount newFrequencyItem.entries = make(map[*CacheItem]byte) if currentFrequency == nil { nextFrequency = cache.freqs.PushFront(newFrequencyItem) } else { nextFrequency = cache.freqs.InsertAfter(newFrequencyItem, currentFrequency) } } item.frequencyParent = nextFrequency nextFrequency.Value.(*FrequencyItem).entries[item] = 1 if currentFrequency != nil { cache.remove(currentFrequency, item) }} 让我们回顾一下频率和条目列表的原始图表，并逐步增加E项目。 我们的增量函数将采用的第一步是分配一个指向节点4（frequencyParent）及其值（即4）的指针。由于节点4存在于列表中，它将在频率列表中找到下一个节点，在我们的例子中是节点7。 一旦它确定E节点的新频率应为5而不是7，它将在节点4和7之间的列表中追加一个新的频率节点： 将5节点添加到列表后，该函数将设置节点正常运行所需的默认值。然后它会将E的指针设置为新的frequencyParent（5节点）： 作为最后一步，它将采用具有指针* CacheItem类型的项目，并将其添加到条目列表，同时从先前的frequencyParent的条目列表中删除它： 让我们看看从FrequencyItem的条目列表中删除CacheItem的步骤是什么。 删除条目一旦我们知道列表中我们想要删除它的节点，我们就可以从条目列表中删除该项，如果条目变空，还可以从频率列表中完全删除FrequencyItem： 123456789func (cache *Cache) Remove(listItem *list.Element, item *CacheItem) { frequencyItem := listItem.Value.(*FrequencyItem) delete(frequencyItem.entries, item) if len(frequencyItem.entries) == 0 { cache.freqs.Remove(listItem) }} 驱逐拼图的最后一部分是逐出，或者换句话说，一旦缓存达到其最大容量，就删除最不常用的项目。 我们必须知道我们想要驱逐多少项。通常，我们的缓存将具有低限和高限，因此当达到上限时，我们将删除项目直到下限。在我们的例子中，我们将驱逐任意数量的项目，Evict函数将作为参数。 该功能将从开始到结束开始“遍历”频率列表。由于频率列表是按升序排列的，因此它将开始从第一个频率节点开始删除条目，直到它删除与传入的任意数字一样多的项目。 如果频率节点由于逐出而不包含条目，则Evict函数也必须从频率列表中移除频率节点。它将通过调用Remove函数来实现。这样，驱逐就不会留下任何垃圾。 这是我们上面描述的代码： 12345678910111213141516func (cache *Cache) Evict(count int) { for i := 0; i &lt; count; { if item := cache.freqs.Front(); item != nil { for entry, _ := range item.Value.(*FrequencyItem).entries { if i &lt; count { delete(cache.bykey, entry.key) cache.Remove(item, entry) cache.size-- i++ } } } }} 回到Set and Get在本文开头，我们实现了Set和Get函数。那时我们没有的东西是Evict和increment函数，所以我们可以相应地使用它们。让我们添加他们的调用。 增加访问频率在Get函数中，如果我们在bykey哈希表中找到一个项目，我们需要在继续返回其值之前增加它的访问频率： 12345678910111213141516171819202122232425262728293031func (cache *Cache) Get(key string) interface{} { if e, ok := cache.bykey[key]; ok { cache.increment(e) return e.value } return nil}``通过此更改，Cache将在返回之前增加该特定项的频率。但是，我们忘了什么吗？此外，Set函数在实际缓存它们时访问缓存的项目。这意味着当一个项被缓存时，它将立即被添加到频率列表中，值为1的节点下：```gofunc (cache *Cache) Set(key string, value interface{}) { if item, ok := cache.bykey[key]; ok { item.value = value cache.increment(item) } else { item := new(CacheItem) item.key = key item.value = value cache.bykey[key] = item cache.size++ // Eviction, if needed cache.increment(item) }} 在驱逐后Set函数允许我们的LFU Cache用户在其中缓存更多项目。任何缓存的一个关键组件是，当新项目添加到缓存时，它应该知道如何逐出项目（释放空间）。对于LFU缓存，当缓存达到容量时，需要删除最不常用的项。 让我们首先添加一个函数，如果缓存达到其最大容量，它将返回一个bool： 12345func (cache *Cache) atCapacity() bool { return cache.size &gt;= cache.capacity} 功能很简单：检查Cache的当前大小是大于还是等于容量。 现在，让我们在Set函数中使用它。一旦我们在缓存中设置了新项目，我们就必须检查缓存是否已达到其容量，然后从中删除多个项目。 为简单起见，我们每次达到最大容量时只会删除10个项目： 123456789101112131415161718func (cache *Cache) Set(key string, value interface{}) { if item, ok := cache.bykey[key]; ok { item.value = value cache.increment(item) } else { item := new(CacheItem) item.key = key item.value = value cache.bykey[key] = item cache.size++ if cache.atCapacity() { cache.Evict(10) } cache.increment(item) }} 通过此更改，如果在任何时候添加项目达到缓存的容量，缓存将驱逐最不常用的项目。 通过此更改，如果在任何时候添加项目达到缓存的容量，缓存将驱逐最不常用的项目。 如果您想查看本文的完整代码，可以查看这 关于缩放和时间复杂性的评论LFU是一个有趣的驱逐计划，特别是与LRU相比，在我看来，由于其非常规性质。虽然其应用受到限制，但由于该方法的扩展能力，本文中使用的论文中解释的算法和后备数据结构非常吸引人。 如果我们重新阅读本文开头提到的论文，我们将看到虽然LFU不是新闻，但它传统上是使用min-heap实现的，它具有插入，查找和删除的对数时间。有趣的是，在本文中，作者解释说，他们提出的方法对于每个操作（插入，查找和删除）都具有O（1）时间复杂度，因为操作基于哈希表。此外，链接列表不会增加任何时间复杂度，因为我们不会在任何时候遍历列表 - 我们只是在需要时添加或删除其中的节点（这是一个O（1）操作）。 总结在本文中，我们了解了LFU缓存的基础知识。我们确定了最重要的绩效指标（命中率，成员资格和驱逐速度）。我们看到虽然它不是最广泛使用的缓存方案，但在某些用例中肯定会非常高效。 然后我们继续实施它，使用一种在时间复杂度方面可以很好地扩展的方法。我们看到了实施驱逐和频率增量算法的复杂性。最后，我们进一步探讨了我们用于实现它的方法如何扩展。 如果您想阅读有关该主题的更多信息，请参阅以下几个链接，以丰富您对LFU缓存和缓存的了解： “An O(1) algorithm for implementing the LFU cache eviction scheme”- Prof. Ketan Shah, Anirban Mitra, Dhruv Matani “Caching in theory and practice”- Pavel Panchekha “LFU (Least Frequently Used) Cache Implementation”- Geeks for Geeks 本文翻译自–原文","link":"/2019/06/04/%5B%E8%AF%91%5D%E5%90%88%E9%80%82%E4%BB%A5%E5%8F%8A%E4%B8%BA%E4%BD%95%E4%BD%BF%E7%94%A8%E6%9C%80%E5%B0%91%E4%BD%BF%E7%94%A8(LFU)%E7%BC%93%E5%AD%98%E4%B8%8EGolang%E4%B8%AD%E7%9A%84%E5%AE%9E%E7%8E%B0/"},{"title":"elasticsearch社区分享会","text":"在前段时间加班的时候错过了两场我想去的技术会，这次终于没落空了。大佬也多，涨了不少姿势。特此记录一下分享，由于全凭记忆叙述，可能就没啥顺序而言的还原出之前的收获。 确实目前项目中目前涉及到了elasticsearch不多，索引都才几个。看到别人分享的都是2千，4-5千的索引量。而且数据量大的话才更能体现出elasticsearch的作用。 周金阳 果壳网/在行 算法工程师 算法果然是大佬，让es与深度学习结合起来在搜索这块已经走在很多公司的前面了吧。 使用 ES 来构建一个简易却行之有效的个性化推荐系统，以及一些高级搜索排序的实践。 搜索排序主要是分享一些机器学习工具与 ES 配合的实践心得。 思考一个问题，如果是这样的你会选择怎么排序 1234{ &quot;title&quot;:&quot;引力波&quot; &quot;content&quot;:&quot;引力波引力波引力波&quot;} 1234{ &quot;title&quot;:&quot;引力波,一个世纪的求索&quot; &quot;content&quot;:&quot;在物理学中，引力波是指时空弯曲中的涟漪，通过波的形式从辐射源向外传播，这种波以引力辐射的形式传输能量。在1916年，爱因斯坦基于广义相对论预言了引力波的存在。引力波的存在是广义相对论洛伦兹不变性的结果，因为它引入了相互作用的传播速度有限的概念。相比之下，引力波不能够存在于牛顿的经典引力理论当中，因为牛顿的经典理论假设物质的相互作用传播是速度无限的。&quot;} 若输入的值和被检索到的结果呈线性变化g(q,x)很明显，第一个是用户测试的或者是胡乱写的，当用户输入“引力波”的时候，如何控制类似于这种情况让正常的显示在前。这种情况，就可以加一些其他的限制条件f(x),比如1得出来的期望值为15.42,2得出来的期望值为87.93，这样关于g(q,x) -&gt; f(x)*g(q,x) 当然如果要做的好的话需要优化的还有很多，比如用BiLSTM+CNN 期望后期会用到这些吧，毕竟我觉得这是偏离业务而且是和大数据接轨的之一。 其中，在使用es的时候有一些规范和约束， 业务索引尽量自定义id，数据敏感业务自备插入修改时间 一个索引一个type 控制单次搜索结果条数，总条数由es限制。控制请求超时时间 关于es的使用也有在调用链日志 一个节点一个主分片，0副本， 批量写入，控制单批写入字节数 在生产阶段，调用链日志写入慢，kafka会出现大量堆积等现象，关于如何解决。有以下方案， 索引写入时会伴随着id校验，请求体解析，分词等操作，都会带来一定的cpu开销。原先的索引结构中存在部分多余字段，无需进行分词，取消后可以减轻cpu压力。 使用es自动生成id，省去id检查步骤。调整translog合并时间，半小时一次，防止过多merge任务导致cpu开销过大。 在业务索引随着场景变化，写入量逐渐增多，集群cpu load变高，原来单个主分片写入出现瓶颈遇到这种情况 可以重建索引，主分片改为2个，分别分布在两台机器，达到负载均衡效果，数据需要迁移。 在提及到es时，不得不说也是和spark相关。这里就不展开讲了，下次深入了解的时候再学习。 番外大公司都是搜索是一个团队，虽然我业务写的也不好，但是我更倾向于这种方向。分享者都很强，有开发相关的以及运维，技术演变快，找准自己的一个兴趣点，专研下去。","link":"/2018/09/11/elasticsearch%E7%A4%BE%E5%8C%BA%E5%88%86%E4%BA%AB%E4%BC%9A/"},{"title":"golang-defer","text":"defer的使用特点其实其中一点特性我理解起来就有点像java中的finally的用法 关于官方解释 123A defer statement defers the execution of a function until the surrounding function returns.The deferred call's arguments are evaluated immediately, but the function call is not executed until the surrounding function returns. 这里提到了defer调用的参数会立即计算，但在周围函数返回之前不会执行函数调用。 以及延迟函数调用被压入堆栈。当函数返回时，其延迟调用以后进先出顺序执行。 它有如何特点 所在的函数中，它在 return 或 panic 或 执行完毕 后被调用 多个 defer，它们的被调用顺序，为栈的形式。先进后出，先定义的后被调用 看下面几个例子： 在计算defer语句时，将计算延迟函数的参数。在此示例中，在延迟Println调用时计算表达式“i”。函数返回后，延迟调用将打印“0”。 123456func a() { i := 0 defer fmt.Println(i) i++ return} 在周围函数返回后，延迟函数调用以后进先出顺序执行。 12345func b() { for i := 0; i &lt; 4; i++ { defer fmt.Print(i) }} //将会打印3210 然后不免在使用过程中会遇到这些坑 坑1. defer在匿名返回值和命名返回值函数中的不同表现 12345678910111213141516func returnValues() int { var result int defer func() { result++ fmt.Println(&quot;defer&quot;) }() return result}func namedReturnValues() (result int) { defer func() { result++ fmt.Println(&quot;defer&quot;) }() return result} &nbsp;&nbsp;上面的方法会输出0，下面的方法输出1。上面的方法使用了匿名返回值，下面的使用了命名返回值，除此之外其他的逻辑均相同，为什么输出的结果会有区别呢？ &nbsp;&nbsp;要搞清这个问题首先需要了解defer的执行逻辑，defer语句在方法返回“时”触发，也就是说return和defer是“同时”执行的。以匿名返回值方法举例，过程如下。 将result赋值给返回值（可以理解成Go自动创建了一个返回值retValue，相当于执行retValue = result） 然后检查是否有defer，如果有则执行 返回刚才创建的返回值（retValue） 在这种情况下，defer中的修改是对result执行的，而不是retValue，所以defer返回的依然是retValue。在命名返回值方法中，由于返回值在方法定义时已经被定义，所以没有创建retValue的过程，result就是retValue，defer对于result的修改也会被直接返回。 坑2. 判断执行没有err之后，再defer释放资源 一些获取资源的操作可能会返回err参数，我们可以选择忽略返回的err参数，但是如果要使用defer进行延迟释放的的话，需要在使用defer之前先判断是否存在err，如果资源没有获取成功，即没有必要也不应该再对资源执行释放操作。如果不判断获取资源是否成功就执行释放操作的话，还有可能导致释放方法执行错误。 正确做法 1234567resp, err := http.Get(url)// 先判断操作是否成功if err != nil { return err}// 如果操作成功，再进行Close操作defer resp.Body.Close() 坑3. 调用os.Exit时defer不会被执行当发生panic时，所在goroutine的所有defer会被执行，但是当调用os.Exit()方法退出程序时，defer并不会被执行。 123456func deferExit() { defer func() { fmt.Println(&quot;defer&quot;) }() os.Exit(0)} 上面的defer并不会输出。 坑4.非引用传参给defer调用的函数，且为非闭包函数，值不会受后面的改变影响 1234567func defer0() { a := 1 // a 作为演示的参数 defer fmt.Println(a) // 非引用传参，非闭包函数中，a 的值 不会 受后面的改变影响 a = a + 2}// 控制台输出 1 坑5. 传递引用给defer调用的函数，即使不使用闭包函数，值也会受后面的改变影响 1234567891011func myPrintln(point *int) { fmt.Println(*point) // 输出引用所指向的值}func defer1() { a := 3 // &amp;a 是 a 的引用。内存中的形式： 0x .... ---&gt; 3 defer myPrintln(&amp;a) // 传递引用给函数，即使不使用闭包函数，值 会 受后面的改变影响 a = a + 2}// 控制台输出 5 坑6. 传递值给defer调用的函数，且非闭包函数，值不会受后面的改变影响 1234567891011func p(a int) { fmt.Println(a)}func defer2() { a := 3 defer p(a) // 传递值给函数，且非闭包函数，值 不会 受后面的改变影响 a = a + 2}// 控制台输出： 3 坑7. defer调用闭包函数，且内调用外部非传参进来的变量，值会受后面的改变影响 12345678910// 闭包函数内，事实是该值的引用func defer3() { a := 3 defer func() { fmt.Println(a) // 闭包函数内调用外部非传参进来的变量，事实是该值的引用，值 会 受后面的改变影响 }() a = a + 2 // 3 + 2 = 5}// 控制台输出： 5 坑8. defer调用闭包函数，若内部使用了传参参数的值。使用的是值 123456789101112131415func defer5() { a := []int{1,2,3} for i:=0;i&lt;len(a);i++ { // 闭包函数内部使用传参参数的值。内部的值为传参的值。同时引用是不同的 defer func(index int) { // index 有一个新地址指向它 fmt.Println(a[index]) // index == i }(i) // 后进先出，3 2 1 }}// 控制台输出： // 3// 2// 1 坑9. defer所调用的非闭包函数，参数如果是函数，会按顺序先执行（函数参数） 123456789101112131415161718192021func calc(index string, a, b int) int { ret := a + b fmt.Println(index, a, b, ret) return ret}func defer6() { a := 1 b := 2 // calc 充当了函数中的函数参数。即使在 defer 的函数中，它作为函数参数，定义的时候也会首先调用函数进行求值 // 按照正常的顺序，calc(&quot;10&quot;, a, b) 首先被调用求值。calc(&quot;122&quot;, a, b) 排第二被调用 defer calc(&quot;1&quot;, a, calc(&quot;10&quot;, a, b)) defer calc(&quot;12&quot;,a, calc(&quot;122&quot;, a, b))}// 控制台输出：/**10 1 2 3 // 第一个函数参数122 1 2 3 // 第二个函数参数12 1 3 4 // 倒数第一个 calc1 1 3 4 // 倒数第二个 calc*/ 注意 defer 不影响 return的值 参考1 参考2","link":"/2019/01/11/defer/"},{"title":"MySQL基础","text":"基础知识 mysql默认的引擎是InnoDB(MySQL5.5.5版本起) 三大范式 属性具有原子性，不可再分解； 不存在部分依赖； 不存在传递依赖； MySQL InnoDB 概览InnoDB的数据存储在表空间，表空间是由InnoDB管理的一个黑盒子,由一系列的数据文件组成。InnoDB采用MVCC来支持高并发 MySQL执行 Server 层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖 MySQL 的大多数核心服务功能，以及所有的内置函数（如日期、时间等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。 连接器 先连接上数据库，这个时候接待的就是连接器，连接器负责根客户端建立连接，获取权限，维持和管理连接 查询缓存 MySQL拿到一个查询请求后，会先到查询缓存看看，之前是不是执行过这条语句。之前执行过的语句及其结果可能会以 key-value 对的形式，被直接缓存在内存中。key 是查询的语句，value 是查询的结果。如果你的查询能够直接在这个缓存中找到 key，那么这个 value 就会被直接返回给客户端。 可以将参数 query_cache_type 设置成 DEMAND，这样对于默认的 SQL 语句都不使用查询缓存。(MySQL8.0没有查询缓存) 分析器 分析语法是否正确 优化器 优化器是在表里面有多个索引的时候，决定使用哪个索引；或者在一个语句有多表关联（join）的时候，决定各个表的连接顺序。比如你执行下面这样的语句，这个语句是执行两个表的 join： select * from t1 join t2 using(ID) where t1.c=10 and t2.d=20; 既可以先从表 t1 里面取出 c=10 的记录的 ID 值，再根据 ID 值关联到表 t2，再判断 t2 里面 d 的值是否等于 20。 也可以先从表 t2 里面取出 d=20 的记录的 ID 值，再根据 ID 值关联到 t1，再判断 t1 里面 c 的值是否等于 10。 执行器 如检查权限等，执行SQL IO成本 IO成本就是寻址时间和上线文切换所需要的时间，最主要是用户态和内核态的上下文切换。用户态是无法直接访问磁盘等硬件上的数据的，只能通过操作系统去调内核态的接口，用内核态的线程去访问。 这里的上下文切换指的是同进程的线程上下文切换，所谓上下文就是线程运行需要的环境信息。首先，用户态线程需要一些中间计算结果保存CPU寄存器，保存CPU指令的地址到程序计数器（执行顺序保证），还要保存栈的信息等一些线程私有的信息。 然后切换到内核态的线程执行，就需要把线程的私有信息从寄存器，程序计数器里读出来，然后执行读磁盘上的数据。读完后返回，又要把线程的信息写进寄存器和程序计数器。 切换到用户态后，用户态线程又要读之前保存的线程执行的环境信息出来，恢复执行。这个过程主要是消耗时间资源。 MySQL更新当有一条记录需要更新的时候，InnoDB 引擎就会先把记录写到 redo log里面，并更新内存，这个时候更新就算完成了。同时，InnoDB 引擎会在适当的时候，将这个操作记录更新到磁盘里面，而这个更新往往是在系统比较空闲的时候做 分析SQL 123set @@profiling=1;select ....show profiles; 查看当前会话所产生的所有 profiles的耗时","link":"/2019/01/11/MySQL%E5%9F%BA%E7%A1%80/"},{"title":"gin源码阅读","text":"整体结构认识 gin框架处理请求的入口函数ServeHTTP： 12345678910111213// gin.gofunc (engine *Engine) ServeHTTP(w http.ResponseWriter, req *http.Request) { // 这里使用了对象池 c := engine.pool.Get().(*Context) // Get对象后做初始化 c.writermem.reset(w) c.Request = req c.reset() engine.handleHTTPRequest(c) // 处理HTTP请求的函数 engine.pool.Put(c) // 处理完请求后将对象放回池子} 为减少gc重复回收， 这里使用sync.pool管理自定义Context对象 将请求reqeust数据copy到Context对象中， 通过Context进行管理 调用engine.handleHTTPRequest 进行路由分发 在这里引入自定义的Context对象， 其主要是用来管理数据流转过程时的，上下文数据， 比如response， request， 请求参数params,路径fullpath, 查询缓存, 错误管理， 主要的目的是:避免重复复制数据。 保证数据的一致性。这是gin最重要的数据结构体 处理 handleHTTPRequest 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162func (engine *Engine) handleHTTPRequest(c *Context) { httpMethod := c.Request.Method rPath := c.Request.URL.Path unescape := false if engine.UseRawPath &amp;&amp; len(c.Request.URL.RawPath) &gt; 0 { rPath = c.Request.URL.RawPath unescape = engine.UnescapePathValues } if engine.RemoveExtraSlash { rPath = cleanPath(rPath) } // Find root of the tree for the given HTTP method // 根据请求方法找到对应的路由树 t := engine.trees for i, tl := 0, len(t); i &lt; tl; i++ { if t[i].method != httpMethod { continue } root := t[i].root // Find route in tree 在路由树中根据path查找 value := root.getValue(rPath, c.params, unescape) if value.params != nil { c.Params = *value.params } if value.handlers != nil { // 更新Context对象属性，将路由地址管理的多个路由函数都交给Context管理 c.handlers = value.handlers c.fullPath = value.fullPath // 执行函数链条,递归执行。 这里的Next特别有意思 c.Next() c.writermem.WriteHeaderNow() return } if httpMethod != &quot;CONNECT&quot; &amp;&amp; rPath != &quot;/&quot; { if value.tsr &amp;&amp; engine.RedirectTrailingSlash { redirectTrailingSlash(c) return } if engine.RedirectFixedPath &amp;&amp; redirectFixedPath(c, root, engine.RedirectFixedPath) { return } } break } if engine.HandleMethodNotAllowed { for _, tree := range engine.trees { if tree.method == httpMethod { continue } if value := tree.root.getValue(rPath, nil, unescape); value.handlers != nil { c.handlers = engine.allNoMethod serveError(c, http.StatusMethodNotAllowed, default405Body) return } } } c.handlers = engine.allNoRoute serveError(c, http.StatusNotFound, default404Body)} 核心代码 12345// 根据路径，请求参数，找到对应的 路由处理函数value := root.getValue(rPath, c.Params, unescape)// 递归的执行关联的handler方法c.Next() c.Next()方法，这个方法的核心，主要是方便接入中间件(Middleware)，使得代码模块化操作。 看下Next的具体实现 12345678func (c *Context) Next() { c.index++ for c.index &lt; int8(len(c.handlers)) { // 执行关联的中间件方法或者 实际路由处理函数 c.handlers[c.index](c) c.index++ }} 这里的Next设计非常有意思。以下是我给出的一个例子 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657package mainimport &quot;fmt&quot;// 洋葱模型type Context struct { handles []func(c *Context) index int8 // 代表上面func的索引}func (this *Context) Use(f func(c *Context)) { this.handles = append(this.handles, f)}func (this *Context) Get(path string, f func(c *Context)) { this.handles = append(this.handles, f)}func (this *Context) Next() { this.index++ this.handles[this.index](this)}func (this *Context) Run() { this.handles[0](this) // 执行第一个函数}func Middleware1() func(c *Context) { return func(c *Context) { fmt.Println(&quot;middleware start&quot;) c.Next() fmt.Println(&quot;middleware end&quot;) }}func Middleware2() func(c *Context) { return func(c *Context) { fmt.Println(&quot;middleware2 start&quot;) c.Next() fmt.Println(&quot;middleware2 end&quot;) }}func main() { c := &amp;Context{} c.Use(Middleware1()) c.Use(Middleware2()) c.Get(&quot;/&quot;, func(c *Context) { fmt.Println(&quot;Get handle func &quot;) }, ) c.Run()} 创建一个Context结构体 调用Use方法，将中间件函数添加到handles中 调用Get方法，将路由函数添加到handles中 调用Run方法，执行第一个函数 执行第一个函数，打印middleware start，然后调用Next方法 Next方法中，index++，此时index为1，然后执行handles[1]，也就是第二个函数 执行第二个函数，打印middleware2 start，然后调用Next方法 Next方法中，index++，此时index为2，然后执行handles[2]，也就是第三个函数 执行第三个函数，打印Get handle func 执行完毕，返回到第二个函数，打印middleware2 end 返回到第一个函数，打印middleware end 其调用关系实现了Next方法的伪代码，加深理解： 处理函数有先后执行关系， 并且处理函数可以通过调用Abort方法， 提前返回，不用递归调用到实际处理函数。这些中间件，可以方便的使我们的业务代码接入权限校验auth，日志管理等其他功能模块。 路由匹配路由匹配是由节点的 getValue方法实现的。getValue根据给定的路径(键)返回nodeValue值，保存注册的处理函数和匹配到的路径参数数据。 gin框架涉及中间件相关有4个常用的方法，它们分别是c.Next()、c.Abort()、c.Set()、c.Get()。 中间件的注册 gin框架中的中间件设计很巧妙，从最常用的r := gin.Default()的Default函数开始看，它内部构造一个新的engine之后就通过Use()函数注册了Logger中间件和Recovery中间件： 123456func Default() *Engine { debugPrintWARNINGDefault() engine := New() engine.Use(Logger(), Recovery()) // 默认注册的两个中间件 return engine} Use() 函数 123456func (engine *Engine) Use(middleware ...HandlerFunc) IRoutes { engine.RouterGroup.Use(middleware...) // 实际上还是调用的RouterGroup的Use函数 engine.rebuild404Handlers() engine.rebuild405Handlers() return engine} 注册中间件其实就是将中间件函数追加到group.Handlers中： 12345// Use adds middleware to the group, see example code in GitHub.func (group *RouterGroup) Use(middleware ...HandlerFunc) IRoutes { group.Handlers = append(group.Handlers, middleware...) return group.returnObj()} 而我们注册路由时会将对应路由的函数和之前的中间件函数结合到一起： 123456func (group *RouterGroup) handle(httpMethod, relativePath string, handlers HandlersChain) IRoutes { absolutePath := group.calculateAbsolutePath(relativePath) handlers = group.combineHandlers(handlers) // 将处理请求的函数与中间件函数结合 group.engine.addRoute(httpMethod, absolutePath, handlers) return group.returnObj()} 12345678910111213package mainimport &quot;github.com/gin-gonic/gin&quot;func main() { r := gin.Default() r.GET(&quot;/ping&quot;, func(c *gin.Context) { c.JSON(200, gin.H{ &quot;message&quot;: &quot;success&quot;, }) }) r.Run() // listen and serve on 0.0.0.0:8080} 所以其实gin的中间件，其实就是Gin定义的一个HandlerFunc先看r.Run() 12345678func (engine *Engine) Run(addr ...string) (err error) { defer func() { debugPrintError(err) }() address := resolveAddress(addr) debugPrint(&quot;Listening and serving HTTP on %s\\n&quot;, address) err = http.ListenAndServe(address, engine) return} Gin 提供了gin.BasicAuth 生成基本认证的中间件 12345r := gin.Default()r.Use(gin.BasicAuth(gin.Accounts{ &quot;admin&quot;: &quot;123456&quot;,})) 比如访问的时候 需要用户名和密码 也可对特定的URL进行认证，也就是像这样 123456789101112func main() { r := gin.Default() r.GET(&quot;/&quot;, func(c *gin.Context) { c.JSON(200, &quot;首页&quot;) }) adminGroup := r.Group(&quot;/admin&quot;) adminGroup.Use(gin.BasicAuth(gin.Accounts{ &quot;admin&quot;: &quot;123456&quot;, }))}","link":"/2022/05/05/gin%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/"},{"title":"2018年总结","text":"帝都的生活&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;依稀记得在18年初的时候，毅然决定从重庆离职，离开这个舒适区，因为我明白，在这边待越久，差距就会越来越大，等到过完年来帝都搬砖。来到这边才感知和重庆的天壤之别。每天早上13号线挤得不要不要的，但是9点后出门的话人就会少很多。正是因为这边上班晚，所以下班就迟了。加班也是常态。其实正常的工作日，加上通勤的时间，自己的时间久真的不太多。周末，和朋友们去过故宫，是挺大挺庄严的。但其实这些建筑之类的，去过一次也就够了。相比于去过的圆明园，更加喜欢这种自然风的恬静。北方是山也不算高，从故宫后面的景山看整个故宫挺震撼的。以及珍藏奇珍异宝的国家博物馆，逛了足足半天。只能说中外文物意义非凡，而且看着古代匠人的艺术感挺强的，因为不懂技术的我其实很想多认识一些“艺术”朋友。当深夜行走在奥体公园的时候，雨滴在五彩斑斓的灯光勾勒下仿佛也有一种彩虹的错觉。团建的时候开过的卡丁卡，到现在也还记得速度下的刺激。盛夏时节去十渡也有一种暴晒的感觉，当然对比重庆来说还是要好很多。在帝都工作是很忙，忙里偷闲也不缺乏去看看其他不一样的世界。由于一天都是坐办公室内，再加上每天良好的伙食不知不觉中就长胖了，意识到每天不到3000的步数会导致我的身体体能下降。本着在学校练出的毅力办了健身卡，3个月就再也没去过了。很大一部分原因是公司项目真的很忙吧。4月的时候竟然还在北京见到了雪，虽然不足为奇，但感受到南北方的气候的初步差异。不久后就到了5月，答辩的日子。也没有留恋学校的日子， 所以呆了10天就走了。在这期间很感谢我的导师zy。毕业不是再见，是更好的遇见。北京的空气是真的干，动不动就是静电电你。还有就是抗寒能力不错的我在刺骨的冬天也真的觉得冷哇，不敢拿身体开玩笑。 帝都的工作&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;从到这边来就感受到了和重庆的技术有这天壤之别，技术层出不穷，但就国内而言前沿的技术还是先在北上广这些城市试用。互联网的人来人往，流动性大。但其实对于加班这点来看，确实会因为菜的原因加班，再就是就算不加班，也不喜欢早走，因为还可以在公司继续学习，可能氛围比在家好些。因为各种原因，在10月的时候换到了现在这家公司。说实话挺感谢我对面的同事zjf,虽然在技术成长的方面靠自己，但是在其他事情上，比如业务还是很耐心的给我讲，挺好的一个人。以及旁边同事就比我大一届，如果我和他是同届也被他甩了几条街了吧。当然要学习的还有很多很多，在工作中也有很多时候效率低下，开发相关知识肯定是基础，然后也一个人的思维能力可能就真的是发展的天花板。这点还要不断加强练习。很快就在这边呆了一年了，还是先干好手头上的事情。来这边你会知道，优秀的人比你还努力很多倍。不过好在现在终于算稳定了吧，从实习开始主语言就从c#换成后面的Java和js并行。然后换到了现在的golang。 其他原本搭建这个博客是为了更好地将自己的吸收转为输出，但写着写着回过头来看就觉得是太简单的，没啥必要记录。以及自己掌握的东西还不够，怕误导其他人。所以就先决定再沉淀一段时间。回头看看，今年好像就做了这些数字相关的吧。 参加3场技术沙龙 在帝都的这一年，参加了3场感兴趣的技术沙龙，但也有其他原因错失了我想去的。参加这些技术分享会的动机很单纯，就是去学习其他前沿技术。有以下几点好处 认识一些周末不打游戏，不陪妹子的人。对技术充满热情的人。 不同公司会有一定得局限性，比如上家公司我做的那个项目es索引都才几个，其他公大公司都是搜索一个团队，对于海量的数据，如何如何处理。也正是我去了es社区的分享后很想做es方面的工作。但是现在这个机器人团队也挺好的。 因为目前很多知识都还是属于懵懂阶段，对眼界的开阔的一定帮助。 认识一些大牛，其实并不是为了大牛能够帮你解决疑难问题，有的时候你加上别人，看看别人的朋友圈，说不定也能掌握业内的一些动向。 读完14本书 只能说通勤的时间很充裕 看完28部电影 当然几乎都是电脑上看的。相比去年的110部很满足了。 明年争取减少到10部","link":"/2018/12/31/2018%E6%80%BB%E7%BB%93/"},{"title":"深入理解go map","text":"哈希函数哈希查找表一般会存在“碰撞”的问题，就是说不同的 key 被哈希到了同一个 bucket。一般有两种应对方法：链表法和开放地址法。链表法将一个 bucket 实现成一个链表，落在同一个 bucket 中的 key 都会插入这个链表。开放地址法则是碰撞发生后，通过一定的规律，在数组的后面挑选“空位”，用来放置新的 key。 1装载因子 := 元素数量 / 桶数量 与开放地址法一样，拉链法的装载因子越大，哈希的读写性能就越差，在一般情况下使用拉链法的哈希表装载因子都不会超过 1，当哈希表的装载因子较大时就会触发哈希的扩容，创建更多的桶来存储哈希中的元素，保证性能不会出现严重的下降。如果有 1000 个桶的哈希表存储了 10000 个键值对，它的性能是保存 1000 个键值对的 1/10，但是仍然比在链表中直接读写好 1000 倍。 Golang 使用的哈希算法 Golang 选择哈希算法时，根据 CPU 是否支持 AES 指令集进行判断 ，如果 CPU 支持 AES 指令集，则使用 Aes Hash，否则使用 memhash。 AES 指令集全称是高级加密标准指令集（或称英特尔高级加密标准新指令，简称AES-NI），是一个 x86指令集架构的扩展，用于 Intel 和 AMD 处理器。 利用 AES 指令集实现哈希算法性能很优秀，因为它能提供硬件加速。 查看 CPU 是否支持 AES 指令集： 123cat /proc/cpuinfo | grep aesflags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl eagerfpu pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch fsgsbase bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt 处理哈希冲突 Golang 及多数编程语言都使用链地址法处理哈希冲突。 链地址法 链地址法链地址法是处理哈希冲突最常见的方法，它的实现比开放地址法稍微复杂一些，但平均查找长度较短，用于存储节点的内存是动态申请的，可以节省较多内存。 要将一个键值对 (Key6, Value6) 写入哈希表，需要经过两个步骤： 键值对中的键 Key6 先经过 Hash 算法计算，返回的哈希值定位到一个桶，选择桶的方式是对哈希值取模： 1index := hash(&quot;Key6&quot;) % array.len 遍历当前桶中的链表，在遍历链表的过程中会遇到以下两种情况： 121. 找到键相同的键值对，则更新键对应的值；2. 没有找到键相同的键值对，则在链表的末尾追加新键值对。 只有可比较的类型才能够作为Map中的key 数据结构Go中Map是一个KV对集合。底层使用hash table，用链表来解决冲突，出现冲突时，不是每一个Key都申请一个结构通过链表串起来，而是以bmap为最小粒度挂载，一个bmap可以放8个kv。 在哈希函数的选择上，会在程序启动时，检测 cpu 是否支持 aes，如果支持，则使用aes hash，否则使用memhash。 每个map的底层结构是hmap，是有若干个结构为bmap的bucket组成的数组。每个bucket底层都采用链表结构。 Go 语言运行时同时使用了多个数据结构组合表示哈希表，其中使用 hmap 结构体来表示哈希 12345678910111213141516171819202122232425262728293031323334type hmap struct { count int // 元素个数 flags uint8 // 用来标记状态 B uint8 // 扩容常量相关字段B是buckets数组的长度的对数 2^B noverflow uint16 // noverflow是溢出桶的数量，当B&lt;16时，为精确值,当B&gt;=16时，为估计值 hash0 uint32 // 是哈希的种子，它能为哈希函数的结果引入随机性，这个值在创建哈希表时确定，并在调用哈希函数时作为参数传入 buckets unsafe.Pointer // 桶的地址 oldbuckets unsafe.Pointer // 旧桶的地址，用于扩容 nevacuate uintptr // 搬迁进度，扩容需要将旧数据搬迁至新数据，这里是利用指针来比较判断有没有迁移 extra *mapextra // 用于扩容的指针}type mapextra struct { overflow *[]*bmap oldoverflow *[]*bmap // nextOverflow holds a pointer to a free overflow bucket. nextOverflow *bmap}// A bucket for a Go map.type bmap struct { tophash [bucketCnt]uint8 // tophash用于记录8个key哈希值的高8位，这样在寻找对应key的时候可以更快，不必每次都对key做全等判断}//实际上编辑期间会动态生成一个新的结构体type bmap struct { topbits [8]uint8 keys [8]keytype values [8]valuetype pad uintptr overflow uintptr} bmap 就是常说的“桶”，桶里面会最多装 8 个 key，这些 key之所以会落入同一个桶，是因为它们经过哈希计算后，哈希结果是“一类”的，在桶内，又会根据key计算出来的hash值的高8位来决定 key到底落入桶内的哪个位置（一个桶内最多有8个位置)。 当map的key和value都不是指针，并且 size都小于128字节的情况下，会把bmap标记为不含指针，这样可以避免gc时扫描整个hmap。 但是，bmap其实有一个overflow的字段，是指针类型的，破坏了 bmap 不含指针的设想，这时会把overflow移动到 hmap的extra 字段来。 如果初始化时生成了溢出桶,会放置到map的extra字段里去 12345678910111213141516171819func makemap(t *maptype, hint int, h *hmap) *hmap { ... B := uint8(0) for overLoadFactor(hint, B) { B++ } h.B = B if h.B != 0 { var nextOverflow *bmap h.buckets, nextOverflow = makeBucketArray(t, h.B, nil) if nextOverflow != nil { h.extra = new(mapextra) h.extra.nextOverflow = nextOverflow } } return h} 123456type mapextra struct { overflow *[]*bmap oldoverflow *[]*bmap // nextOverflow holds a pointer to a free overflow bucket. nextOverflow *bmap} 当 make 的 hint &lt;= 8 时，会直接在栈上分配一个 bucket，一个 bucket 可以存储8对 KV 当 make 的 hint &gt; 8 &amp;&amp; hint &lt;= 52 时，会在堆上分配 bucket，此时不会分配 overflow bucket 当 make 的 hint &gt; 52 时，会在堆上分配 bucket 和 overflow bucket bmap 是存放 k-v 的地方key 和 value 是各自放在一起的，并不是 key/value/key/value/... 这样的形式。源码里说明这样的好处是在某些情况下可以省略掉 padding 字段，节省内存空间。减少内存对齐的内存消耗 如果按照 key/value/key/value/... 这样的模式存储，那在每一个 key/value 对之后都要额外 padding 7 个字节；而将所有的 key，value 分别绑定到一起，这种形式 key/key/.../value/value/...，则只需要在最后添加 padding。 每个 bucket 设计成最多只能放 8 个 key-value 对，如果有第 9 个 key-value 落入当前的 bucket，那就需要再构建一个 bucket ，通过 overflow 指针连接起来。 这样随着哈希表存储的数据逐渐增多，会扩容哈希表或者使用额外的桶存储溢出的数据，不会让单个桶中的数据超过 8 个，不过溢出桶只是临时的解决方案，创建过多的溢出桶最终也会导致哈希的扩容。 插入过程12345678910111213141516func maplit(n *Node, m *Node, init *Nodes) { a := nod(OMAKE, nil, nil) a.Esc = n.Esc a.List.Set2(typenod(n.Type), nodintconst(int64(n.List.Len()))) litas(m, a, init) entries := n.List.Slice() if len(entries) &gt; 25 { ... return } // Build list of var[c] = expr. // Use temporaries so that mapassign1 can have addressable key, elem. ...} 当哈希表中的元素数量少于或者等于 25 个时，编译器会将字面量初始化的结构体转换成以下的代码，将所有的键值对一次加入到哈希表中： 1234hash := make(map[string]int, 3)hash[&quot;1&quot;] = 2hash[&quot;3&quot;] = 4hash[&quot;5&quot;] = 6 一旦哈希表中元素的数量超过了 25 个，编译器会创建两个数组分别存储键和值，这些键值对会通过如下所示的 for 循环加入哈希： 123456hash := make(map[string]int, 26)vstatk := []string{&quot;1&quot;, &quot;2&quot;, &quot;3&quot;, ... ， &quot;26&quot;}vstatv := []int{1, 2, 3, ... , 26}for i := 0; i &lt; len(vstak); i++ { hash[vstatk[i]] = vstatv[i]} 对 key 计算 hash 值，找到要赋值的位置（可能是插入新 key，也可能是更新老 key），对相应位置进行赋值。 函数首先会检查 map 的标志位 flags。如果 flags 的写标志位此时被置 1 了，说明有其他协程在执行“写”操作，进而导致程序 panic。这也说明了 map 对协程是不安全的。 不过因为 Go 语言哈希的扩容不是一个原子的过程，所以**mapassign**** 还需要判断当前哈希是否已经处于扩容状态，避免二次扩容造成混乱。** 在 B 不为 0 的情况下，会调用 makeBucketArray 函数初始化桶。 当 B &lt; 4 的时候，初始化 hmap 只会生成 8 个桶，不生成溢出桶，因为数据少几乎不可能用到溢出桶； 当 B &gt;= 4 的时候，会额外创建 2^(B−4) 个溢出桶。 Map有多种初始化的方式,如果指定了长度N,在初始化时会生成桶。桶的数量为log2(N).如果map的长度大于了2^4，则会在初始化的时候生成溢出桶。溢出桶的大小为2^(b-4),b为桶的大小。 遍历过程map 遍历的核心在于理解 2 倍扩容时，老 bucket 会分裂到 2 个新 bucket 中去。而遍历操作，会按照新 bucket 的序号顺序进行，碰到老 bucket 未搬迁的情况时，要在老 bucket 中找到将来要搬迁到新 bucket 来的 key。 查找keykey 经过 Hash 计算后得到 64 位哈希值（64位机器）； 用哈希值最后 B 个 bit 位计算它落在哪个桶； 用哈希值高 8 位计算它在桶中的索引位置。 删除过程mapdelete 函数。它首先会检查 h.flags 标志，如果发现写标位是 1，直接 panic，因为这表明有其他协程同时在进行写操作。 计算 key 的哈希，找到落入的 bucket。检查此 map 如果正在扩容的过程中，直接触发一次搬迁操作。 将 count 值减 1，将对应位置的 tophash 值置成 Empty。 同样需要计算出hash的前8位、指定的桶等。 如果在删除期间遇到了哈希表的扩容，就会分流桶中的元素，分流结束之后会找到桶中的目标元素完成键值对的删除工作。 如果查找到了指定的key,则会清空数据，hash位设置为emptyOne. 如果发现后面没有元素，则会设置为emptyRest,并循环向上检查前一个元素是否为空。 扩容过程1234567// 一个桶里最多可以装载的键值对数量：8对bucketCntBits = 3bucketCnt = 1 &lt;&lt; bucketCntBits// 触发扩容操作的装载因子临界值是：13 / 2 = 6.5loadFactorNum = 13loadFactorDen = 2 1loadFactor := count / (2^B) # count 就是 map 的元素个数，2^B 表示 bucket 数量。 触发 map 扩容的时机：在向 map 插入新 key 的时候，会进行条件检测，符合下面这 2 个条件，就会触发扩容： 装载因子超过阈值，源码里定义的阈值是 6.5。 翻倍扩容 哈希使用了太多溢出桶（造成这种现象的原因是不停地插入、删除元素） , overflow 的 bucket 数量过多：当 B 小于 15，也就是 bucket 总数 2^B 小于 2^15 时，如果 overflow 的 bucket 数量超过 2^B；当 B &gt;= 15，也就是 bucket 总数 2^B 大于等于 2^15，如果 overflow 的 bucket 数量超过 2^15。map的扩容不是一个原子的扩容，所以他虽然具备扩容条件，而先要判断该map是否处在扩容状态。 等量扩容 sameSizeGrow，等量扩容创建的新桶数量只是和旧桶一样，该函数中只是创建了新的桶，并没有对数据进行拷贝和转移 当溢出桶的数量过多,则会进行等量重建。新桶会会存储到buckets字段,旧桶会存储到oldbuckets字段。 map中extra字段的溢出桶也同理的进行了转移。 因为 Go 语言哈希的扩容不是一个原子的过程，所以 [runtime.mapassign](https://draveness.me/golang/tree/runtime.mapassign) 还需要判断当前哈希是否已经处于扩容状态，避免二次扩容造成混乱。 扩容的入口是 [runtime.hashGrow](https://draveness.me/golang/tree/runtime.hashGrow)： 1234567891011121314151617181920func hashGrow(t *maptype, h *hmap) { bigger := uint8(1) if !overLoadFactor(h.count+1, h.B) { bigger = 0 h.flags |= sameSizeGrow } oldbuckets := h.buckets newbuckets, nextOverflow := makeBucketArray(t, h.B+bigger, nil) h.B += bigger h.flags = flags h.oldbuckets = oldbuckets h.buckets = newbuckets h.nevacuate = 0 h.noverflow = 0 h.extra.oldoverflow = h.extra.overflow h.extra.overflow = nil h.extra.nextOverflow = nextOverflow} 哈希在扩容的过程中会通过 [runtime.makeBucketArray](https://draveness.me/golang/tree/runtime.makeBucketArray) 创建一组新桶和预创建的溢出桶，随后将原有的桶数组设置到 oldbuckets 上并将新的空桶设置到 buckets 上，溢出桶也使用了相同的逻辑更新 哈希在存储元素过多时会触发扩容操作，每次都会将桶的数量翻倍，扩容过程不是原子的，而是通过 [runtime.growWork](https://draveness.me/golang/tree/runtime.growWork) 增量触发的，在扩容期间访问哈希表时会使用旧桶，向哈希表写入数据时会触发旧桶元素的分流。除了这种正常的扩容之外，为了解决大量写入、删除造成的内存泄漏问题，哈希引入了 sameSizeGrow 这一机制，在出现较多溢出桶时会整理哈希的内存减少空间的占用。 扩容是渐进式的，如果 map 处在扩容的过程中，那么当 key 定位到了某个 bucket 后，需要确保这个 bucket 对应的老 bucket 完成了迁移过程。即老 bucket 里的 key 都要迁移到新的 bucket 中来（分裂到 2 个新 bucket），才能在新的 bucket 中进行插入或者更新的操作。 现在到了定位 key 应该放置的位置了，所谓找准自己的位置很重要。准备两个指针，一个（inserti）指向 key 的 hash 值在 tophash 数组所处的位置，另一个(insertk)指向 cell 的位置（也就是 key 最终放置的地址），当然，对应 value 的位置就很容易定位出来了。这三者实际上都是关联的，在 tophash 数组中的索引位置决定了 key 在整个 bucket 中的位置（共 8 个 key），而 value 的位置需要“跨过” 8 个 key 的长度。 在循环的过程中，inserti 和 insertk 分别指向第一个找到的空闲的 cell。如果之后在 map 没有找到 key 的存在，也就是说原来 map 中没有此 key，这意味着插入新 key。那最终 key 的安置地址就是第一次发现的“空位”（tophash 是 empty）。 如果这个 bucket 的 8 个 key 都已经放置满了，那在跳出循环后，发现 inserti 和 insertk 都是空，这时候需要在 bucket 后面挂上 overflow bucket。当然，也有可能是在 overflow bucket 后面再挂上一个 overflow bucket。这就说明，太多 key hash 到了此 bucket。 在正式安置 key 之前，还要检查 map 的状态，看它是否需要进行扩容。如果满足扩容的条件，就主动触发一次扩容操作。 这之后，整个之前的查找定位 key 的过程，还得再重新走一次。因为扩容之后，key 的分布都发生了变化。最后，会更新 map 相关的值，如果是插入新 key，map 的元素数量字段 count 值会加 1；在函数之初设置的 hashWriting 写标志出会清零。 再来看一下扩容具体是怎么做的。由于 map 扩容需要将原有的 key/value 重新搬迁到新的内存地址，如果有大量的 key/value 需要搬迁，会非常影响性能。因此 Go map 的扩容采取了一种称为“渐进式”地方式，原有的 key 并不会一次性搬迁完毕，每次最多只会搬迁 2 个 bucket。 hashGrow() 函数实际上并没有真正地“搬迁”，它只是分配好了新的 buckets，并将老的 buckets 挂到了 oldbuckets 字段上。真正搬迁 buckets 的动作在 growWork() 函数中，而调用 growWork() 函数的动作是在 mapassign 和 mapdelete 函数中。也就是插入或修改、删除 key 的时候，都会尝试进行搬迁 buckets 的工作。先检查 oldbuckets 是否搬迁完毕，具体来说就是检查 oldbuckets 是否为 nil。 hashGrow() 函数所做的工作，再来看具体的搬迁 buckets 是如何进行的。 12345678910111213141516171819202122232425262728293031func hashGrow(t *maptype, h *hmap) { // B+1 相当于是原来 2 倍的空间 bigger := uint8(1) // 对应条件 2 if !overLoadFactor(int64(h.count), h.B) { // 进行等量的内存扩容，所以 B 不变 bigger = 0 h.flags |= sameSizeGrow } // 将老 buckets 挂到 buckets 上 oldbuckets := h.buckets // 申请新的 buckets 空间 newbuckets, nextOverflow := makeBucketArray(t, h.B+bigger) flags := h.flags &amp;^ (iterator | oldIterator) if h.flags&amp;iterator != 0 { flags |= oldIterator } // 提交 grow 的动作 h.B += bigger h.flags = flags h.oldbuckets = oldbuckets h.buckets = newbuckets // 搬迁进度为 0 h.nevacuate = 0 // overflow buckets 数为 0 h.noverflow = 0 // ……} 主要是申请到了新的 buckets 空间，把相关的标志位都进行了处理：例如标志 nevacuate 被置为 0， 表示当前搬迁进度为 0。 真正执行搬迁工作的 growWork() 函数。 123456789func growWork(t *maptype, h *hmap, bucket uintptr) { // 确认搬迁老的 bucket 对应正在使用的 bucket evacuate(t, h, bucket&amp;h.oldbucketmask()) // 再搬迁一个 bucket，以加快搬迁进程 if h.growing() { evacuate(t, h, h.nevacuate) }} map为什么是无序的map 在扩容后，会发生 key 的搬迁，原来落在同一个 bucket 中的 key，搬迁后，有些 key 就要远走高飞了（bucket 序号加上了 2^B）。而遍历的过程，就是按顺序遍历 bucket，同时按顺序遍历 bucket 中的 key。搬迁后，key 的位置发生了重大的变化，有些 key 飞上高枝，有些 key 则原地不动。这样，遍历 map 的结果就不可能按原来的顺序了。 在遍历 map 时，并不是固定地从 0 号 bucket 开始遍历，每次都是从一个随机值序号的 bucket 开始遍历，并且是从这个 bucket 的一个随机序号的 cell 开始遍历。这样，即使你是一个写死的 map，仅仅只是遍历它，也不太可能会返回一个固定序列的 key/value 对了。 “迭代 map 的结果是无序的”这个特性是从 go 1.0 开始加入的。 1234567891011...// decide where to startr := uintptr(fastrand())if h.B &gt; 31-bucketCntBits { r += uintptr(fastrand()) &lt;&lt; 31}it.startBucket = r &amp; bucketMask(h.B)it.offset = uint8(r &gt;&gt; h.B &amp; (bucketCnt - 1))// iterator stateit.bucket = it.startBucket 可以边遍历边删除吗map 并不是一个线程安全的数据结构。同时读写一个 map 是未定义的行为，如果被检测到，会直接 panic。 上面说的是发生在多个协程同时读写同一个 map 的情况下。 如果在同一个协程内边遍历边删除，并不会检测到同时读写，理论上是可以这样做的。但是，遍历的结果就可能不会是相同的了，有可能结果遍历结果集中包含了删除的 key，也有可能不包含，这取决于删除 key 的时间：是在遍历到 key 所在的 bucket 时刻前或者后。 一般而言，这可以通过读写锁来解决：sync.RWMutex。 读之前调用 RLock() 函数，读完之后调用 RUnlock() 函数解锁；写之前调用 Lock() 函数，写完之后，调用 Unlock() 解锁。 另外，sync.Map 是线程安全的 map，也可以使用。 可以对map元素取地址么不可以，因为一旦发生扩容，key 和 value 的位置就会改变，之前保存的地址也就失效了。 map是线程安全的吗在查找、赋值、遍历、删除的过程中都会检测写标志，一旦发现写标志置位（等于1），则直接 panic。赋值和删除函数在检测完写标志是复位之后，先将写标志位置位，才会进行之后的操作。 删除掉map中的元素是否会释放内存？ 不会，删除操作仅仅将对应的tophash[i]设置为empty，并非释放内存。若要释放内存只能等待指针无引用后被系统gc 参考 面向信仰编程深入Go的Map使用和实现原理Go中的map的实现map的实现原理","link":"/2020/07/11/go-map/"},{"title":"Go字符串","text":"修改字符串要修改字符串，需要先将其转换成[]rune或[]byte，完成后再转换为string。无论哪种转换，都会重新分配内存，并复制字节数组。 1234567891011func changeString() { s1 := &quot;hello&quot; // 强制类型转换 byteS1 := []byte(s1) byteS1[0] = 'H' fmt.Println(string(byteS1)) s2 := &quot;博客&quot; runeS2 := []rune(s2) runeS2[0] = '狗' fmt.Println(string(runeS2)) } 先将这段内存拷贝到堆或者栈上； 将变量的类型转换成 []byte 后并修改字节数据； 将修改后的字节数组转换回 string； 1234// string is the set of all strings of 8-bit bytes, conventionally but not// necessarily representing UTF-8-encoded text. A string may be empty, but// not nil. Values of string type are immutable.type string string string是8位字节的集合，通常但不一定代表UTF-8编码的文本。string可以为空，但不能为nil。string的值是不能改变的 Go源代码为 UTF-8 编码格式的，源代码中的字符串直接量是 UTF-8 文本。所以Go语言中字符串是UTF-8编码格式的。 123// rune is an alias for int32 and is equivalent to int32 in all ways. It is// used, by convention, to distinguish character values from integer values.type rune = int32 rune是int32的别名，在所有方面都等同于int32，按照约定，它用于区分字符值和整数值。 rune一个值代表的就是一个Unicode字符，因为一个Go语言中字符串编码为UTF-8，使用1-4字节就可以表示一个字符，所以使用int32类型范围可以完美适配。 见坑 字符串拼接 优先使用 strings.Builder 而不是 += 子字符串操作及内存泄露字符串的切分也会跟切片的切分一样，可能会造成内存泄露。例子：有一个handleLog的函数，接收一个string类型的参数log，假设log的前4个字节存储的是log的message类型值，需要从log中提取出message类型，并存储到内存中。下面是相关代码： 12345678func (s store) handleLog(log string) error { if len(log) &lt; 4 { return errors.New(&quot;log is not correctly formatted&quot;) } message := log[:4] s.store(message) // Do something} 使用log[:4]的方式提取出了message，假设参数log是一个包含成千上万个字符的字符串。当使用log[:4]操作时，实际上是返回了一个字节切片，该切片的长度是4，而容量则是log字符串的整体长度。那么实际上存储的message不是包含4个字节的空间，而是整个log字符串长度的空间。所以就有可能会造成内存泄露。如下图所示： 那怎么避免呢？使用拷贝。将uuid提取后拷贝到一个字节切片中，这时该字节切片的长度和容量都是4。如下： 123456789func (s store) handleLog(log string) error { if len(log) &lt; 4 { return errors.New(&quot;log is not correctly formatted&quot;) } message := string([]byte(log[:4])) s.store(message) // Do something} 字符串的长度内建的 len()函数返回byte的数量，而不是像Python中计算好的unicode字符串中字符的数量。要在Go中得到相同的结果，可以使用“unicode/utf8”包中的 RuneCountInString()函数。 1234567package mainimport(&quot;fmt&quot;&quot;unicode/utf8&quot;)func main(){ data :=&quot;♥&quot; fmt.Println(utf8.RuneCountInString(data))//prints: 1} 理论上说 RuneCountInString()函数并不返回字符的数量，因为单个字符可能占用多个rune。","link":"/2019/02/11/go_string/"},{"title":"golang 切片","text":"切片结构12345type slice struct { array unsafe.Pointer len int cap int} 12a = make([]int, 0)unsafe.Sizeof(a) // 24 切片组成元素： 指针：指向底层数组 长度：切片中元素的长度，不能大于容量 容量：指针所指向的底层数组的总容量 初始化方式 使用make12slice := make([]int, 5) // 初始化长度和容量都为 5 的切片slice := make([]int, 5, 10) // 初始化长度为 5, 容量为 10 的切片 使用 make 关键字创建切片时，很多工作都需要运行时的参与；调用方必须在 make 函数中传入一个切片的大小以及可选的容量，[cmd/compile/internal/gc.typecheck1](https://github.com/golang/go/blob/b7d097a4cf6b8a9125e4770b54d33826fa803023/src/cmd/compile/internal/gc/typecheck.go#L327-L2126) 会对参数进行校验： 123456789101112131415161718192021222324252627282930313233func typecheck1(n *Node, top int) (res *Node) { switch n.Op { ... case OMAKE: args := n.List.Slice() i := 1 switch t.Etype { case TSLICE: if i &gt;= len(args) { yyerror(&quot;missing len argument to make(%v)&quot;, t) return n } l = args[i] i++ var r *Node if i &lt; len(args) { r = args[i] } ... if Isconst(l, CTINT) &amp;&amp; r != nil &amp;&amp; Isconst(r, CTINT) &amp;&amp; l.Val().U.(*Mpint).Cmp(r.Val().U.(*Mpint)) &gt; 0 { yyerror(&quot;len larger than cap in make(%v)&quot;, t) return n } n.Left = l n.Right = r n.Op = OMAKESLICE } ... }} 上述函数不仅会检查 len 是否传入，还会保证传入的容量 cap 一定大于或者等于 len，除了校验参数之外，当前函数会将 OMAKE 节点转换成 OMAKESLICE，随后的中间代码生成阶段在 [cmd/compile/internal/gc.walkexpr](https://github.com/golang/go/blob/4d5bb9c60905b162da8b767a8a133f6b4edcaa65/src/cmd/compile/internal/gc/walk.go#L439-L1532) 函数中的 [OMAKESLICE](https://github.com/golang/go/blob/4d5bb9c60905b162da8b767a8a133f6b4edcaa65/src/cmd/compile/internal/gc/walk.go#L1315) 分支依据两个重要条件对这里的 OMAKESLICE 进行转换： 切片的大小和容量是否足够小； 切片是否发生了逃逸，最终在堆上初始化 虽然大多的错误都可以在编译期间被检查出来，但是在创建切片的过程中如果发生了以下错误就会直接导致程序触发运行时错误并崩溃： 内存空间的大小发生了溢出； 申请的内存大于最大可分配的内存； 传入的长度小于 0 或者长度大于容量； [runtime.makeslice](https://draveness.me/golang/tree/runtime.makeslice) 在最后调用的 [runtime.mallocgc](https://draveness.me/golang/tree/runtime.mallocgc) 是用于申请内存的函数，这个函数的实现比较复杂，如果遇到了比较小的对象会直接初始化在 Go 语言调度器里面的 P 结构中，而大于 32KB 的对象会在堆上初始化 为啥是32kb 界限的选择是基于一些性能和内存管理的考虑。 小于等于32KB的对象被认为是比较小的，可以在 P 结构中进行初始化。这样做有以下几个优点： 减少对堆的访问：将对象初始化在 P 结构中可以避免频繁地访问堆，减少内存的分配和释放操作，提高程序的性能。 提高局部性：将对象与对应的 P 结构关联起来，可以提高数据的局部性，减少内存访问的延迟，进一步提升性能。 大于32KB的对象被认为是较大的对象，其内存需求比较高。将这些对象直接初始化在堆上有以下几个优点： 堆的管理更加灵活：堆提供了更加灵活的内存管理机制，可以根据需要动态分配和释放内存，适应各种大小的对象。 避免过度占用 P 结构：将大对象初始化在堆上可以避免过度占用 P 结构的内存空间，保持 P 结构的高效利用。 使用简短定义 1slice := []int{1, 2, 3} 使用数组来初始化切片 123arr := [5]int{1, 2, 3, 4, 5}slice := arr[0:3] // 左闭右开区间，最终切片为 [1,2,3]cap(slice) // 长度为5,更通用的规则是：一个切片的容量可以被看作是透过这个窗口最多可以看到的底层数组中元素的个数。注意，切片代表的窗口是无法向左扩展的。 使用切片来初始化切片12sliceA := []int{1, 2, 3, 4, 5}sliceB := sliceA[0:3] // 左闭右开区间，sliceB 最终为 [1,2,3] 扩容例子 注意点 多个切片共享一个底层数组的情况，对底层数组的修改，将影响上层多个切片的值 多个切片共享一个底层数组的情况，对底层数组的修改，原有的切片发生了扩容 底层数组被重新创建 ，和原来的切片已经没有关系了 扩容的slice还和类型(其实是类型占的字节)有关 1234567891011121314e := []int32{1,2,3} fmt.Println(&quot;cap of e before:&quot;,cap(e)) e = append(e,4) fmt.Println(&quot;cap of e after:&quot;,cap(e)) f := []int{1,2,3} fmt.Println(&quot;cap of f before:&quot;,cap(f)) f = append(f,4) fmt.Println(&quot;cap of f after:&quot;,cap(f)) cap of e before: 3cap of e after: 8cap of f before: 3cap of f after: 6 1234567891011121314151617181920package mainimport ( &quot;fmt&quot;)func main() { slice := []int{1, 2, 3, 4, 5} newSlice := slice[0:3] fmt.Println(&quot;before modifying underlying array:&quot;) fmt.Println(&quot;slice: &quot;, slice) fmt.Println(&quot;newSlice: &quot;, newSlice) fmt.Println() newSlice[0] = 6 // 如果是newSlice append几个元素进去，则slice的值为 6，1，2，3，4，5 fmt.Println(&quot;after modifying underlying array:&quot;) fmt.Println(&quot;slice: &quot;, slice) fmt.Println(&quot;newSlice: &quot;, newSlice)} 以上代码预期输出如下： 1234567before modify underlying array:slice: [1 2 3 4 5]newSlice: [1 2 3]after modify underlying array:slice: [6 2 3 4 5]newSlice: [6 2 3] 使用 copy 方法可以避免共享同一个底层数组 1234567891011121314151617181920package mainimport ( &quot;fmt&quot;)func main() { slice := []int{1, 2, 3, 4, 5} newSlice := make([]int, len(slice)) copy(newSlice, slice) fmt.Println(&quot;before modifying underlying array:&quot;) fmt.Println(&quot;slice: &quot;, slice) fmt.Println(&quot;newSlice: &quot;, newSlice) fmt.Println() newSlice[0] = 6 fmt.Println(&quot;after modifying underlying array:&quot;) fmt.Println(&quot;slice: &quot;, slice) fmt.Println(&quot;newSlice: &quot;, newSlice)} 以上代码预期输出如下： 1234567before modifying underlying array:slice: [1 2 3 4 5]newSlice: [1 2 3 4 5]after modifying underlying array:slice: [1 2 3 4 5]newSlice: [6 2 3 4 5] 扩容分析通过 append 关键字被转换的控制流了解了在切片容量足够时如何向切片中追加元素，但是当切片的容量不足时就会调用 [runtime.growslice](https://github.com/golang/go/blob/440f7d64048cd94cba669e16fe92137ce6b84073/src/runtime/slice.go#L76-L191) 函数为切片扩容，扩容就是为切片分配一块新的内存空间并将原切片的元素全部拷贝过去，我们分几部分分析该方法： 1234567891011121314151617181920func growslice(et *_type, old slice, cap int) slice { // …… newcap := old.cap doublecap := newcap + newcap if cap &gt; doublecap { newcap = cap } else { if old.cap &lt; 1024 { newcap = doublecap } else { for newcap &lt; cap { newcap += newcap / 4 } } } // …… capmem = roundupsize(uintptr(newcap) * ptrSize) newcap = int(capmem / ptrSize)} 后半部分还对 newcap 作了一个内存对齐，这个和内存分配策略相关。进行内存对齐之后，新 slice 的容量是要 大于等于 老 slice 容量的 2倍或者1.25倍。 123456789package mainimport &quot;fmt&quot;func main() { s := []int{1,2} s = append(s,4,5,6) fmt.Printf(&quot;len=%d, cap=%d&quot;,len(s),cap(s))} 运行结果是： 1len=5, cap=6 （如果按照1.25倍的说法就是5，8，但实际上是错误的） 这个函数的参数依次是 元素的类型，老的 slice，新 slice 最小求的容量。 例子中 s 原来只有 2 个元素，len 和 cap 都为 2，append 了三个元素后，长度变为 5，容量最小要变成 5，即调用 growslice 函数时，传入的第三个参数应该为 5。即 cap=5。而一方面，doublecap 是原 slice容量的 2 倍，等于 4。满足第一个 if 条件，所以 newcap 变成了 5。 接着调用了 roundupsize 函数，传入 40。（代码中ptrSize是指一个指针的大小，在64位机上是8） 再看内存对齐，搬出 roundupsize 函数的代码： 1234567891011121314func roundupsize(size uintptr) uintptr { if size &lt; _MaxSmallSize { if size &lt;= smallSizeMax-8 { return uintptr(class_to_size[size_to_class8[(size+smallSizeDiv-1)/smallSizeDiv]]) } else { //…… } } //……}const _MaxSmallSize = 32768const smallSizeMax = 1024const smallSizeDiv = 8 最终返回 1class_to_size[size_to_class8[(size+smallSizeDiv-1)/smallSizeDiv]] 这是 Go 源码中有关内存分配的两个 slice。class_to_size通过 spanClass获取 span划分的 object大小。而 size_to_class8 表示通过 size 获取它的 spanClass。 123var size_to_class8 = [smallSizeMax/smallSizeDiv + 1]uint8{0, 1, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9, 10, 10, 11, 11, 12, 12, 13, 13, 14, 14, 15, 15, 16, 16, 17, 17, 18, 18, 18, 18, 19, 19, 19, 19, 20, 20, 20, 20, 21, 21, 21, 21, 22, 22, 22, 22, 23, 23, 23, 23, 24, 24, 24, 24, 25, 25, 25, 25, 26, 26, 26, 26, 26, 26, 26, 26, 27, 27, 27, 27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 28, 28, 28, 29, 29, 29, 29, 29, 29, 29, 29, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31}var class_to_size = [_NumSizeClasses]uint16{0, 8, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 576, 640, 704, 768, 896, 1024, 1152, 1280, 1408, 1536, 1792, 2048, 2304, 2688, 3072, 3200, 3456, 4096, 4864, 5376, 6144, 6528, 6784, 6912, 8192, 9472, 9728, 10240, 10880, 12288, 13568, 14336, 16384, 18432, 19072, 20480, 21760, 24576, 27264, 28672, 32768} 传进去的 size 等于 40。所以 (size+smallSizeDiv-1)/smallSizeDiv = 5；获取 size_to_class8 数组中索引为 5 的元素为 4；获取 class_to_size 中索引为 4 的元素为 48。最终，新的 slice 的容量为 6： 1newcap = int(capmem / ptrSize) // 6 预估容量（预估”元素个数”） 注意：(官方代码在2020-09-25 换成了 if old.cap &lt; 1024{} ) 如果新申请容量（cap）大于旧容量（old.cap）的两倍，则最终容量（newcap）是新申请的容量（cap）； 如果旧切片的长度小于 1024，则最终容量是旧容量的 2 倍，即“newcap=doublecap”； (注意1.18后时256) 如果旧切片的长度大于或等于 1024，则最终容量从旧容量开始循环增加原来的 1/4，直到最终容量大于或等于新申请的容量为止；(注意1.18后时256，&gt;=512) 如果最终容量计算值溢出，即超过了 int 的最大范围，则最终容量就是新申请容量。 分配内存 = 预估容量 * 元素类型大小 申请分配内存是语言自身实现的内存管理模块向操作系统申请(合适的内存规格:8,16,32,48,64,80,96,112……字节，64位下每个元素占16字节。32位下占8字节,其中查看类型占多少字节用unsafe.Sizeof()来判断，但是又如何得知当前平台是在处于多少为的系统。可以用以下来判断(比如int在64为下占8个字节，string在64为下占10个字节) 132 &lt;&lt; (^uint(0) &gt;&gt; 63) ^uint(0)在32位系统上返回的是0XFFFFFFFF, 也就是2^32, 在64位系统上返回的是0xFFFFFFFFFFFFFFFF, 也就是2^64 申请分配内存会匹配到最接近的规格 确认了最新容量后，则进行内存对齐。通过对元素的大小（et.size）的判断，对了内存对齐。通过数组class_to_size拿到对齐的值。 newCap = 申请分配内存 / 元素类型大小 在1.22版本后切片的扩容机制变更为 初始化变量：函数接受两个参数，newLen 表示切片的新长度，oldCap 表示切片的旧容量。开始时将 newcap 初始化为 oldCap。 判断是否需要直接扩容至新长度：首先计算 doublecap，即旧容量的两倍。如果新长度大于 doublecap，则直接返回新长度，因为此时直接扩容到新长度即可。 阈值判断：定义了一个阈值常量 threshold，其值为 256。如果旧容量小于该阈值，那么新容量直接设置为 doublecap。 循环计算新容量：对于大于等于阈值的旧容量，采用一种新的扩容策略，即每次增加 newcap 的 1.25 倍，直到 newcap 大于等于 newLen。 溢出检查：通过将 newcap 强制转换为 uint 类型进行溢出检查。如果 newcap 溢出，则直接返回新长度。 返回新容量：最后返回新容量，如果新容量小于等于 0，则返回新长度，以防溢出。 这个机制在处理切片扩容时，尤其是针对大容量的切片，可以更加有效地管理内存，避免频繁的内存分配和拷贝操作，从而提高性能。 拷贝切片当我们使用 copy(a, b) 的形式对切片进行拷贝时，编译期间的 [cmd/compile/internal/gc.copyany](https://github.com/golang/go/blob/bf4990522263503a1219372cd8f1ee9422b51324/src/cmd/compile/internal/gc/walk.go#L2980-L3040) 函数也会分两种情况进行处理，如果当前 copy 不是在运行时调用的，copy(a, b) 会被直接转换成下面的代码： 之后，向 Go 内存管理器申请内存，将老 slice 中的数据复制过去，并且将 append 的元素添加到新的底层数组中。最后，向 growslice 函数调用者返回一个新的 slice，这个 slice 的长度并没有变化，而容量却增大了。 1234567n := len(a)if n &gt; len(b) { n = len(b)}if a.ptr != b.ptr { memmove(a.ptr, b.ptr, n*sizeof(elem(a))) } 例子 12345arr := [10]int{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}var sl []int = arr[1:4]var s2 []int = arr[7:]fmt.Println(len(sl),cap(sl)) // 3,9 fmt.Println(len(s2),cap(s2)) // 3,3 一个切片的容量可以被看作是透过这个窗口最多可以看到的底层数组中元素的个数。注意，切片代表的窗口是无法向左扩展的。(前面提到的) 使用技巧12345a = a[:len(a)-1] // 删除尾部1个元素a = a[:len(a)-N] // 删除尾部N个元素a = a[1:] // 删除开头1个元素a = a[N:] // 删除开头N个元素a = append(a[:i], a[j:]...) // cut i ~ j 假设切片里存放的是指针对象，那么下面删除末尾的元素后，被删除的元素依然被切片底层数组引用，从而导致被自动垃圾回收器回收（这要依赖回收器的实现方式）： 保险的方式是先将需要自动内存回收的元素设置为nil，保证自动回收器可以发现需要回收的对象，然后再进行切片的删除操作： 123var a []*int{ ... }a[len(a)-1] = nil // GC回收最后一个元素内存a = a[:len(a)-1] // 从切片删除最后一个元素 同理截掉切片[i,j）之间的元素： 1a = append(a[:i], a[j:]...) 上面的Cut如果元素是指针的话，会存在内存泄露，所以我们要对删除的元素设置nil，等待GC。 12345copy(a[i:], a[j:])for k, n := len(a)-j+i, len(a); k &lt; n; k++ { a[k] = nil // or the zero value of T}a = a[:len(a)-j+i] Delete（GC） 123copy(a[i:], a[i+1:])a[len(a)-1] = nil // or the zero value of Ta = a[:len(a)-1] 切片使用不当对内存的泄露 应该将原切片拷到一个新的切片操作，比如使用切片的前5个slice12345func getMessageType(msg []byte) []byte { msgType := make([]byte, 5) copy(msgType, msg) return msgType} 分组切片 12345678910func chunk(actions []int, batchSize int) [][]int { var batches [][]int for batchSize &lt; len(actions) { actions, batches = actions[batchSize:], append(batches, actions[0:batchSize:batchSize]) } batches = append(batches, actions) return batches} 同时数组可以作为 map 的 k（键），而切片不行 它的大小和类型在编译时就已经确定了。 append函数的常见操作 删除位于索引 i 的元素：a = append(a[:i], a[i+1:]…) 切除切片 a 中从索引 i 至 j 位置的元素：a = append(a[:i], a[j:]…) 为切片 a 扩展 j 个元素长度：a = append(a, make([]T, j)…) 索引 i 的位置插入切片 b 的所有元素：a = append(a[:i], append(b, a[i:]…)…) 并发安全slice 是非协程安全的数据类型，如果创建多个 goroutine 对 slice 进行并发读写，会造成丢失。看一段代码 123456789101112131415161718192021package mainimport ( &quot;fmt&quot; &quot;sync&quot;)func main () { a := make([]int, 0) var wg sync.WaitGroup for i := 0; i &lt; 10000; i++ { wg.Add(1) go func(i int) { a = append(a, i) wg.Done() }(i) } wg.Wait() fmt.Println(len(a))}// 9403 9876 9985 9491 ... 多次执行，每次得到的结果都不一样，总之一定不会是想要的 10000 个。想要解决这个问题，按照协程安全的编程思想来考虑问题可以考虑使用 channel 本身的特性(阻塞)来实现安全的并发读写。 123456789101112131415161718192021func main() { a := make([]int, 0) buffer := make(chan int) go func() { for v := range buffer { a = append(a, v) } }() var wg sync.WaitGroup for i := 0; i &lt; 10000; i++ { wg.Add(1) go func(i int) { buffer &lt;- i wg.Done() }(i) } wg.Wait() fmt.Println(len(a))}// 10000 slice 坑bar 执行了 append 函数之后，最终也修改了 foo 的最后一个元素，这是一个在实践中非常常见的陷阱。 12345foo := []int{0, 0, 0, 42, 100}bar := foo[1:4]bar = append(bar, 99)fmt.Println(&quot;foo:&quot;, foo) // foo: [0 0 0 42 99]fmt.Println(&quot;bar:&quot;, bar) // bar: [0 0 42 99] bar 的 cap 容量会到原始切片的末尾，所以当前 bar 的 cap 长度为 4。 如果要解决这样的问题，其实可以在截取时指定容量： 12345foo := []int{0,0,0,42,100}bar := foo[1:4:4]bar = append(bar, 99)fmt.Println(&quot;foo:&quot;, foo) // foo: [0 0 0 42 100]fmt.Println(&quot;bar:&quot;, bar) // bar: [0 0 42 99] foo[1:4:4] 这里，第三个参数 4 代表 cap 的位置一直到下标 4，但是不包括下标 4。 所以当前 bar 的 Cap 变为了 3，和它的长度相同。当 bar 进行 append 操作时，将发生扩容，它会指向与 foo 不同的底层数据空间。由于bar的容量足够，它将继续使用foo的底层数数组，所以foo也被修改成了[0, 0, 0, 42, 99]。 切片中的三种特殊状态切片的三种特殊状态 —— 「零切片」、「空切片」和「nil 切片」。 空切片和 nil 切片的区别在于，空切片指向的地址不是nil，指向的是一个内存地址，但是它没有分配任何内存空间，即底层元素包含0个元素。 不管是使用 nil 切片还是空切片，对其调用内置函数 append，len 和 cap 的效果都是一样的。 通过 unsafe.Pointer 来转换 Go 语言的任意变量类型。 12345678910111213141516171819var s1 []intvar s2 = []int{}var s3 = make([]int, 0)var s4 = *new([]int)var a1 = *(*[3]int)(unsafe.Pointer(&amp;s1))var a2 = *(*[3]int)(unsafe.Pointer(&amp;s2))var a3 = *(*[3]int)(unsafe.Pointer(&amp;s3))var a4 = *(*[3]int)(unsafe.Pointer(&amp;s4))fmt.Println(a1)fmt.Println(a2)fmt.Println(a3)fmt.Println(a4)---------------------[0 0 0][824634355296 0 0][824634355296 0 0][0 0 0] 其中输出为 [0 0 0] 的 s1 和 s4 变量就是「 nil 切片」，s2 和 s3 变量就是「空切片」。824634199592 这个值是一个特殊的内存地址，所有类型的「空切片」都共享这一个内存地址。 空切片指向的 zerobase 内存地址是一个神奇的地址 「 nil 切片」和 「空切片」在使用上有什么区别么？ 最好办法是不要创建「 空切片」，统一使用「 nil 切片」，同时要避免将切片和 nil 进行比较来执行某些逻辑。这是官方的标准建议。（正确选择 var res []int ） 1234567891011121314151617181920package mainimport &quot;fmt&quot;func main() { var s1 []int // nil 切片 var s2 = []int{} // 空切片 fmt.Println(s1 == nil) fmt.Println(s2 == nil) fmt.Printf(&quot;%#v\\n&quot;, s1) fmt.Printf(&quot;%#v\\n&quot;, s2)}-------truefalse[]int(nil)[]int{} 「空切片」和「 nil 切片」有时候会隐藏在结构体中，这时候它们的区别就被太多的人忽略了，看个例子 123456789101112type Something struct { values []int}var s1 = Something{}var s2 = Something{[]int{}}fmt.Println(s1.values == nil)fmt.Println(s2.values == nil)--------truefalse 「空切片」和「 nil 切片」还有一个极为不同的地方在于 JSON 序列化 1234567891011121314type Something struct { Values []int}var s1 = Something{}var s2 = Something{[]int{}}bs1, _ := json.Marshal(s1)bs2, _ := json.Marshal(s2)fmt.Println(string(bs1))fmt.Println(string(bs2))---------{&quot;Values&quot;:null}{&quot;Values&quot;:[]} 注意，对于切片的判断最好使用len()==0 参考 why-go-vet-report-uint0-might-be-too-small-for-shift-of-63 slice类型存什么？make和new？slice和数组？扩容规则？ 切片(slice)性能及陷阱 切片的容量是怎样增长的 3.2 切片 深度解析 Go 语言中「切片」的三种特殊状态","link":"/2020/10/11/golang%E5%88%87%E7%89%87/"},{"title":"golang 中获取字符串个数","text":"golang 中获取字符串个数在 golang 中不能直接用 len 函数来统计字符串长度，查看了下源码发现字符串是以 UTF-8 为格式存储的，说明 len 函数是取得包含 byte 的个数 123// string is the set of all strings of 8-bit bytes, conventionally but not// necessarily representing UTF-8-encoded text. A string may be empty, but// not nil. Values of string type are immutable. 举个例子，”Hello, 世界“(因为，对比所以用了中文) 123s := &quot;Hello, 世界&quot;fmt.Println(len(s)) // 13fmt.Println([]byte(s)) // [72 101 108 108 111 44 32 228 184 150 231 149 140] 既然是以 byte 存储的，那自然就想到了取 byte 的长度 1234- bytes.Count() - strings.Count() - 将字符串转换为 []runee 后调用 len 函数- 使用 utf8.RuneCountInString() 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667package mainimport ( &quot;bytes&quot; &quot;fmt&quot; &quot;strings&quot; &quot;testing&quot; &quot;unicode/utf8&quot;)/*在 golang 中不能直接用 len 函数来统计字符串长度，查看了下源码发现字符串是以 UTF-8 为格式存储的，说明 len 函数是取得包含 byte 的个数*/func main() { s := &quot;hello, 世界&quot; fmt.Println(len(s)) // 13 fmt.Println([]byte(s)) // [72 101 108 108 111 44 32 228 184 150 231 149 140] fmt.Print(f1(s))}func f1(s string) int { return bytes.Count([]byte(s), nil) - 1}func f2(s string) int { return strings.Count(s, &quot;&quot;) - 1}func f3(s string) int { return len([]rune(s))}func f4(s string) int { return utf8.RuneCountInString(s)}var s = &quot;Hello, 世界&quot;func Benchmark1(b *testing.B) { for i := 0; i &lt; b.N; i++ { f1(s) }}func Benchmark2(b *testing.B) { for i := 0; i &lt; b.N; i++ { f2(s) }}func Benchmark3(b *testing.B) { for i := 0; i &lt; b.N; i++ { f3(s) }}func Benchmark4(b *testing.B) { for i := 0; i &lt; b.N; i++ { f4(s) }} 在 golang ldea配置中我没有看到 benchamark配置，总说包不对，在命令行中输入 1go test stringCount_test.go -bench &quot;.*&quot; 得到以下结果 1234Benchmark1-12 100000000 17.7 ns/opBenchmark2-12 100000000 14.0 ns/opBenchmark3-12 100000000 14.5 ns/opBenchmark4-12 100000000 13.1 ns/op 最快的是utf8.RuneCountInString() 参考","link":"/2018/04/01/golang%20%E4%B8%AD%E8%8E%B7%E5%8F%96%E5%AD%97%E7%AC%A6%E4%B8%B2%E4%B8%AA%E6%95%B0/"},{"title":"golang逃逸分析","text":"堆内存与栈内存Go 程序会在 2 个地方为变量分配内存，一个是全局的堆(heap)空间用来动态分配内存，另一个是每个 goroutine 的栈(stack)空间。与 Java、Python 等语言类似，Go 语言实现垃圾回收(Garbage Collector)机制，因此，Go 语言的内存管理是自动的，通常开发者不需要关心内存分配在栈上，还是堆上。但是从性能的角度出发，在栈上分配内存和在堆上分配内存，性能差异是非常大的。 栈 栈的内存是由编译器自动进行分配和释放的，栈区往往存储着函数参数、局部变量和调用函数帧，它们随着函数的创建而分配，随着函数的退出而销毁。 Go应用程序运行时，每个 goroutine 都维护着一个自己的栈区，这个栈区只能自己使用不能被其他 goroutine 使用。（所以不需要加锁）栈是调用栈（call stack）的简称。一个栈通常又包含了许多栈帧（stack frame），它描述的是函数之间的调用关系 堆 堆区的内存一般由编译器和工程师自己共同进行管理分配，交给 Runtime GC 来释放。在堆上分配时，必须找到一块足够大的内存来存放新的变量数据。后续释放时，垃圾回收器扫描堆空间寻找不再被使用的对象。（所有有时候会有加锁的操作防止数据竞争） 在函数中申请一个对象，如果分配在栈中，函数执行结束时自动回收，如果分配在堆中，则在函数结束后某个时间点进行垃圾回收。 在栈上分配和回收内存的开销很低，在栈上分配内存，消耗的仅是将数据拷贝到内存的时间，而内存的 I/O 通常能够达到 30GB/s，因此在栈上分配内存效率是非常高的。 在堆上分配内存，一个很大的额外开销则是垃圾回收。Go 语言使用的是标记清除算法，并且在此基础上使用了三色标记法和写屏障技术，提高了效率。 函数参数是值传递的，且在调用的时立即执行值拷贝的。所以无论传递什么参数都会被copy到函数的参数变量的内存地址中，堆或者栈上，具体是堆还是栈上涉及到逃逸问题 什么是逃逸分析逃逸分析是编译器用于决定变量分配到堆上还是栈上的一种行为。 确定一个变量是在堆上还是在栈上 ？ 是否有在其他地方（非局部）被引用。只要有可能被引用了，那么它一定分配到堆上。否则分配到栈上。 即使没有被外部引用，但对象过大，无法存放在栈区上。依然有可能分配到堆上。 比如这样的例子 123456789101112func main() { var i int fmt.Printf(&quot;main: %p\\n&quot;, &amp;i) foo(i)}func foo(i int) { fmt.Printf(&quot;foo : %p\\n&quot;, &amp;i)}// 输出的变量地址不一样main: 0xc0000382b0foo : 0xc0000382b8 所以对于复杂结构应该尽量的传递指针减少copy时的开销。 指针传递的同时也带来变量逃逸，和GC压力，也是一把双刃剑，好在大部分情况下不需要特别的对GC进行调优。所以，在make it simple的理念下，在需要时再针对性调优是个不错的选择。 什么时候我们应该传递值，什么时候应该传递指针，这主要取决于copy开销和是否需要在函数内部对变量值进行更改。 指针逃逸指针逃逸应该是最容易理解的一种情况了，即在函数中创建了一个对象，返回了这个对象的指针。这种情况下，函数虽然退出了，但是因为指针的存在，对象的内存不能随着函数结束而回收，因此只能分配在堆上。 12345678910111213141516171819// main.gopackage mainimport &quot;fmt&quot;type Demo struct { name string}func createDemo(name string) *Demo { d := new(Demo) // 局部变量 d 逃逸到堆 d.name = name return d}func main() { demo := createDemo(&quot;demo&quot;) fmt.Println(demo)} 这个例子中，函数 createDemo 的局部变量 d 发生了逃逸。d 作为返回值，在 main 函数中继续使用，因此 d 指向的内存不能够分配在栈上，随着函数结束而回收，只能分配在堆上。 编译时可以借助选项 -gcflags=-m，查看变量逃逸的情况： 1go run -gcflags '-m' main.go 加 -l 了是为了不让Go 编译时自动内敛函数 1go run - gcflags '-m -l' escape . go 12# command-line-arguments./main.go:13:18: moved to heap: userInfo GetUserInfo函数里面的变量 userInfo 逃到堆上了（分配到堆内存空间上了）。GetUserInfo 函数的返回值为 *UserData 指针类型，然后 将值变量userInfo 的地址返回，此时编译器会判断该值可能会在函数外使用，就将其分配到了堆上，所以变量userInfo就逃逸了。 interface{} 动态类型逃逸在 Go 语言中，空接口即 interface{} 可以表示任意的类型，如果函数参数为 interface{}，编译期间很难确定其参数的具体类型，也会发生逃逸。 1234func main() { demo := createDemo(&quot;demo&quot;) fmt.Println(demo)} demo 是 main 函数中的一个局部变量，该变量作为实参传递给 fmt.Println()，但是因为 fmt.Println() 的参数类型定义为 interface{}，因此也发生了逃逸。 对于 Go 语言来说，运行时(runtime) 尝试在 goroutine 需要的时候动态地分配栈空间，goroutine 的初始栈大小为 2 KB。当 goroutine 被调度时，会绑定内核线程执行，栈空间大小最也不会超过操作系统的限制。对 Go 编译器而言，超过一定大小的局部变量将逃逸到堆上，不同的 Go 版本的大小限制可能不一样。 当切片占用内存超过一定大小，或无法确定当前切片长度时，对象占用内存将在堆上分配。 发生逃逸的几种情况 在某个函数中new或字面量创建出的变量，将其指针作为函数返回值，则该变量一定发生逃逸（构造函数返回的指针变量一定逃逸）； 被已经逃逸的变量引用的指针，一定发生逃逸； 被指针类型的slice、map和chan引用的指针，一定发生逃逸；一个典型的例子就是 []*string 。这会导致切片的内容逃逸。尽管其后面的数组可能是在栈上分配的，但其引用的值一定是在堆上。 slice 的背后数组被重新分配了，因为 append 时可能会超出其容量( cap )。 slice 初始化的地方在编译时是可以知道的，它最开始会在栈上分配。如果切片背后的存储要基于运行时的数据进行扩充，就会在堆上分配。 必然不会逃逸 指针被未发生逃逸的变量引用； 仅仅在函数内对变量做取址操作，而未将指针传出； 可能发生逃逸，也可能不会发生逃逸： 将指针作为入参传给别的函数；这里还是要看指针在被传入的函数中的处理过程，如果发生了上边的三种情况，则会逃逸；否则不会逃逸； 一些例子例1 12345678910111213package maintype S struct{}func main() { var x S y := &amp;x _ = *identity(y)}func identity(z *S) *S { return z} 1234 go run -gcflags '-m -l' main.go# command-line-arguments./main.go:10:15: leaking param: z to result ~r0 level=0 第一行是z变量是流经某个函数的意思，仅作为函数的输入，并且直接返回，在 identity()中也没有使用到 z的引用，所以变量没有逃逸。第二行， x在 main()函数中声明，所以是在 main()函数中的栈中的，也没有逃逸。 当然要是上面的例子，打印出 *identity(y) 的返回值，那肯定就是逃逸了。比如 例2 123456789101112131415package mainimport &quot;fmt&quot;type S struct{}func main() { var x S y := &amp;x c := *identity(y) fmt.Println(c)}func identity(z *S) *S { return z} 123456 go run -gcflags '-m -l' main.go# command-line-arguments./main.go:13:15: leaking param: z to result ~r0 level=0./main.go:11:13: ... argument does not escape./main.go:11:14: c escapes to heap 那是否是不引用返回值就不逃逸了呢。不，一样的逃逸的，看下面这个例子 例3 12345678910package maintype S struct{}func main() { var x S _ = *ref(x)}func ref(z S) *S { return &amp;z } 1234 go run -gcflags '-m -l' main.go# command-line-arguments./main.go:9:10: moved to heap: z ref()的参数 z是通过值传递的，所以 z是 main()函数中 x的一个值拷贝，而 ref()返回了 z的引用，所以 z不能放在 ref()的栈中，实际上被分配到了堆上。 例4 12345678910111213package maintype S struct{ M *int }func main() { var i int refStruct(&amp;i)}func refStruct(y *int) (z S) { z.M = y return z} 1234 go run -gcflags '-m -l' main.go# command-line-arguments./main.go:9:16: leaking param: y to result z level=0 这个 y没有逃逸的原因是， main()中带着 i的引用调用了 refStruct()并直接返回了，从来没有超过 main()函数的调用栈 例5 12345678910package maintype S struct{ M *int }func main() { var x S var i int ref(&amp;i, &amp;x)}func ref(y *int, z *S) { z.M = y } 1234# command-line-arguments./main.go:10:10: leaking param: y./main.go:10:18: z does not escape./main.go:7:6: moved to heap: i y和 z没有逃逸很好理解，但问题在于 y还被赋值到函数 ref()的输入 z的成员了，而Go的逃逸分析不能跟踪变量之间的关系，不知道 i变成了 x的一个成员，分析结果说 i是逃逸的，但本质上 i是没逃逸的 例6 interface类型逃逸 123456789package mainimport &quot;fmt&quot;func main() { str := &quot;str&quot; fmt.Printf(&quot;%p&quot;, &amp;str)} 1234# command-line-arguments./main.go:6:2: moved to heap: str./main.go:7:12: ... argument does not escape str也逃逸到了堆上，在堆上进行内存分配，这是因为访问str的地址，因为入参是interface类型，所以变量str的地址以实参的形式传入fmt.Printf后被装箱到一个interface{}形参变量中，装箱的形参变量的值要在堆上分配，但是还要存储一个栈上的地址，也就是str的地址，堆上的对象不能存储一个栈上的地址，所以str也逃逸到堆上，在堆上分配内存。 例7 闭包发生的逃逸 123456789101112func Increase() func() int { n := 0 return func() int { n++ return n }}func main() { in := Increase() fmt.Println(in()) // 1} 123456# command-line-arguments./main.go:6:2: moved to heap: n./main.go:7:9: func literal escapes to heap./main.go:15:13: ... argument does not escape./main.go:15:16: in() escapes to heap 函数也是一个指针类型，所以匿名函数当作返回值时也发生了逃逸，在匿名函数中使用外部变量n，这个变量n会一直存在直到in被销毁，所以n变量逃逸到了堆上。 例8 变量大小不确定以及栈空间不足引发逃逸 先使用ulimit -a查看操作系统的栈空间： 123456789-t: cpu time (seconds) unlimited-f: file size (blocks) unlimited-d: data seg size (kbytes) unlimited-s: stack size (kbytes) 8192-c: core file size (blocks) 0-v: address space (kbytes) unlimited-l: locked-in-memory size (kbytes) unlimited-u: processes 2784-n: file descriptors 256 我的电脑是mac，栈大小是8192 1234567891011121314151617181920212223242526272829303132333435package mainimport ( &quot;math/rand&quot;)func LessThan8192() { nums := make([]int, 8191) // &lt; 64KB for i := 0; i &lt; len(nums); i++ { nums[i] = rand.Int() }}func MoreThan8192(){ nums := make([]int, 8192) // = 64KB for i := 0; i &lt; len(nums); i++ { nums[i] = rand.Int() }}func NonConstant() { number := 10 s := make([]int, number) for i := 0; i &lt; len(s); i++ { s[i] = i }}func main() { NonConstant() MoreThan8192() LessThan8192()} 123456go run -gcflags '-m -l' main.go# command-line-arguments./main.go:8:14: make([]int, 100) does not escape./main.go:15:14: make([]int, 1000000) escapes to heap./main.go:23:11: make([]int, number) escapes to heap 当栈空间足够时，不会发生逃逸，但是当变量过大时，已经完全超过栈空间的大小时，将会发生逃逸到堆上分配内存。同样当我们初始化切片时，没有直接指定大小，而是填入的变量，这种情况为了保证内存的安全，编译器也会触发逃逸，在堆上进行分配内存。 例10 1234567891011121314151617package mainimport &quot;fmt&quot;type A struct { s string}// 这是上面提到的 &quot;在方法内把局部变量指针返回&quot; 的情况func foo(s string) *A { a := new(A) a.s = s return a //返回局部变量a,在C语言中妥妥野指针，但在go则ok，但a会逃逸到堆}func main() { a := foo(&quot;hello&quot;) b := a.s + &quot; world&quot; c := b + &quot;!&quot; fmt.Println(c)} 例11 变量类型不确定发生的逃逸 123456789package mainimport &quot;fmt&quot;func main() { a := 123 fmt.Println(a)} 123456go run -gcflags '-m -l' main.go# command-line-arguments./main.go:8:13: ... argument does not escape./main.go:8:14: a escapes to heap 变量a逃逸到了堆上。但是我们并没有外部引用，为什么也会有逃逸呢？为了看到更多细节，可以在语句中再添加一个-m参数。 12345678910111213 go run -gcflags '-m -m -l' main.go# command-line-arguments./main.go:7:14: a escapes to heap:./main.go:7:14: flow: {storage for ... argument} = &amp;{storage for a}:./main.go:7:14: from a (spill) at ./main.go:7:14./main.go:7:14: from ... argument (slice-literal-element) at ./main.go:7:13./main.go:7:14: flow: {heap} = {storage for ... argument}:./main.go:7:14: from ... argument (spill) at ./main.go:7:13./main.go:7:14: from fmt.Println(... argument...) (call parameter) at ./main.go:7:13./main.go:7:13: ... argument does not escape./main.go:7:14: a escapes to heap a逃逸是因为它被传入了fmt.Println的参数中，这个方法参数自己发生了逃逸。因为fmt.Println的函数参数为interface类型，编译期不能确定其参数的具体类型，所以将其分配于堆上。 源码位置 这里就暂时不贴了，可以链接过去直接看 大概就是说 片段中通过定义 labelState 常量和 func 方法来标记不需要增加循环深度的标签，并且给它们赋予 nonlooping 状态。 paramTag 函数，用于向函数参数添加逃逸分析信息。该函数首先获取参数名称，然后检查是否需要为当前函数生成诊断信息，以及该函数是否包含主体语句。 如果函数没有主体语句，则假定 uintptr 参数必须在调用期间保持活动状态，并设置 pragma 表示此信息。接着，如果参数类型不包含指针，则返回空字符串；否则，创建一个新的泄漏对象（leaks object）来表示参数可能逃逸的位置。如果函数被标记为“noescape”，则将堆位置添加到泄漏对象中；否则，在启用诊断的情况下生成一个警告并将堆位置添加到泄漏对象中。对于具有主体的函数，paramTag 函数从旧位置检索参数的现有逃逸分析信息，优化它，并将其分配给 leaks 变量。如果启用了诊断且参数没有逃逸，则会产生警告。如果参数逃逸到结果参数，则将显示带有逃逸级别的警告。最后，函数将泄漏对象编码为字符串并返回。 所以分析了这么多，函数传递指针真的比传值效率高吗？ 传递指针可以减少底层值的拷贝，可以提高效率，但是如果拷贝的数据量小，由于指针传递会产生逃逸，可能会使用堆，也可能会增加GC的负担，所以传递指针不一定是高效的。 如果想要减少垃圾回收的时间，提高程序性能，那就要尽量避免在堆上分配空间 总结一下 函数返回变量的指针时，这个变量会逃逸 当觉得栈上的空间不够时，会分配在堆上 在切片上存储指针或带指针的值的时候，对应的变量会逃逸 chan里面的元素是指针的时候，也会发生逃逸 map的value是指针的时候，也会发生逃逸 在interface类型上调用方法，也会发生逃逸 当给一个slice分配一个动态的空间容量时，也会发生逃逸 函数或闭包外声明指针，在函数或闭包内分配，也会发生逃逸 函数外初始化变量，函数内使用变量，然后返回函数，也会发生逃逸 被已经逃逸的指针引用的指针，也会发生逃逸 逃逸分析在编译阶段完成的 注意 go run -gcflags ‘-m -m -l’ xx.main 不一定100%对，详情参考 参考 逃逸分析优化性能的论文通过实例理解Go逃逸分析逃逸分析对性能的影响","link":"/2022/06/05/golang%E9%80%83%E9%80%B8%E5%88%86%E6%9E%90/"},{"title":"go规范","text":"1 . 多个 if 语句可以折叠成 switch123456789101112131415161718// NOT BADif foo() { // ...} else if bar == baz { // ...} else { // ...}// BETTERswitch {case foo(): // ...case bar == baz: // ...default: // ...} 2 . 用 chan struct{} 来传递信号, chan bool 表达的不够清楚当你在结构中看到 chan bool 的定义时，有时不容易理解如何使用该值，例如： 123type Service struct { deleteCh chan bool // what does this bool mean? } 但是我们可以将其改为明确的 chan struct {} 来使其更清楚：我们不在乎值（它始终是 struct {}），我们关心可能发生的事件，例如： 123type Service struct { deleteCh chan struct{} // ok, if event than delete something.} 3 . time.Second 比 time.Duration(30) * time.Second 更好你不需要将无类型的常量包装成类型，编译器会找出来。另外最好将常量移到第一位： 12345// BADdelay := time.Second * 60 * 24 * 60// GOODdelay := 24 * 60 * 60 * time.Second 4 . 用 time.Duration 代替 int64 + 变量名12345// BADvar delayMillis int64 = 15000// GOODvar delay time.Duration = 15 * time.Second 5. 按类型分组 const 声明，按逻辑和/或类型分组 var12345678910111213141516171819// BADconst ( foo = 1 bar = 2 message = &quot;warn message&quot;)// MOSTLY BADconst foo = 1const bar = 2const message = &quot;warn message&quot;// GOODconst ( foo = 1 bar = 2)const message = &quot;warn message&quot; 6 要比较时间戳，请使用 time.Before 或 time.After ，不要使用 time.Sub 来获得 duration (持续时间)，然后检查它的值。7. 用 %+v 来打印数据的比较全的信息8. 在 Go 里面要小心使用 range12for i := range a and for i, v := range &amp;a ，都不是 a 的副本但是 for i, v := range a 里面的就是 a 的副本 9. 不要忘记停止 ticker12ticker := time.NewTicker(1 * time.Second)defer ticker.Stop() 10.所有代码都应该通过golint和go vet的检查并无错误。11. 零值 sync.Mutex 和 sync.RWMutex 是有效的。所以指向 mutex 的指针基本是不必要的。12var mu sync.Mutexmu.Lock() 12. Channel 的 size 要么是 1，要么是无缓冲的12c := make(chan int, 1)c := make(chan int) 13. 枚举从 1 开始1234567type Operation intconst ( Add Operation = iota + 1 Subtract Multiply) 14. 使用 time.Time 表达瞬时时间在处理时间的瞬间时使用 [time.Time](https://golang.org/pkg/time/#Time)，在比较、添加或减去时间时使用 time.Time 中的方法。 123func isActive(now, start, stop time.Time) bool { return (start.Before(now) || start.Equal(now)) &amp;&amp; now.Before(stop)} 15. 在尽可能的情况下，在初始化要追加的切片时为make()提供一个容量值。16. 优先使用 strconv 而不是 fmt17. 包命名 全部小写。没有大写或下划线。 大多数使用命名导入的情况下，不需要重命名。 简短而简洁。请记住，在每个使用的地方都完整标识了该名称。 不用复数。例如net/url，而不是net/urls。 不要用“common”，“util”，“shared”或“lib”。这些是不好的，信息量不足的名称。 18. 嵌入式类型（例如 mutex）应位于结构体内的字段列表的顶部，并且必须有一个空行将嵌入式字段与常规字段分隔开。19. 不应明确返回长度为零的切片。应该返回nil 来代替。20. 要检查切片是否为空，请始终使用len(s) == 0。而非 nil。21. time.After() 在某些情况下会发生泄露，替换为使用 Timer。22. 读写磁盘时，使用读写 buffer。23. 内存分配复用内存可以使用 sync.Pool24. 频繁的字符串拼接操作（+=），替换为 StringBuffer 或 StringBuilder25. 并发检测 race","link":"/2019/03/12/go%E8%A7%84%E8%8C%83/"},{"title":"kafka 基本术语","text":"​ Apache Kafka 是一款开源的消息引擎系统，也是分布式流处理平台，使用的是纯二进制的字节序列。 ​ kafka术语​Topic: 发布订阅的对象是主题（Topic） 生产者程序通常持续不断地向一个或多个主题发送消息 Producer: 向主题发布消息的客户端应用程序称为生产者（Producer） Consumer: 订阅这些主题消息的客户端应用程序就被称为消费者（Consumer） Broker: Kafka 的服务器端由被称为 Broker 的服务进程构成，即一个 Kafka 集群由多个 Broker 组成，Broker 负责接收和处理客户端发送过来的请求，以及对消息进行持久化。虽然多个 Broker 进程能够运行在同一台机器上，但更常见的做法是将不同的 Broker 分散运行在不同的机器上，这样如果集群中某一台机器宕机，即使在它上面运行的所有 Broker 进程都挂掉了，其他机器上的 Broker 也依然能够对外提供服务。这其实就是 Kafka 提供高可用的手段之一。分区的leader副本只存在于其中一个broker中 实现高可用的另一个手段就是备份机制（Replication）。备份的思想很简单，就是把相同的数据拷贝到多台机器上，而这些相同的数据拷贝在 Kafka 中被称为副本（Replica）。 副本：Replica。Kafka 中同一条消息能够被拷贝到多个地方以提供数据冗余，这些地方就是所谓的副本。副本是在分区层级下的，即每个分区可配置多个副本实现高可用。 Kafka 定义了两类副本：领导者副本（Leader Replica）和追随者副本（Follower Replica）。前者对外提供服务，这里的对外指的是与客户端程序进行交互；而后者只是被动地追随领导者副本而已，不能与外界进行交互。 副本的工作机制：生产者总是向领导者副本写消息；而消费者总是从领导者副本读消息。至于追随者副本，它只做一件事：向领导者副本发送请求，请求领导者把最新生产的消息发给它，这样它能保持与领导者的同步。通过副本选举实现故障转移。 分区：Partition。一个有序不变的消息序列。每个主题下可以有多个分区。（修改分区数一定要比原有分区数大）一个topic 可以拥有若干个partition（从 0 开始标识partition ），分布在不同的broker 上， 实现发布与订阅时负载均衡。producer 通过自定义的规则将消息发送到对应topic 下某个partition，以offset标识一条消息在一个partition的唯一性。 一个partition拥有多个replica，提高容灾能力。 **Kafka 中的分区机制 **指的是将每个主题划分成多个分区（Partition），每个分区是一组有序的消息日志。生产者生产的每条消息只会被发送到一个分区中​在Kafka中，一个Partition对应物理机器上的一个文件夹，文件夹命名会以Topic名称加序号表示。换句话说，Partition在Broker中以文件夹的形式存在。每个Partition文件夹中会有多个大小相等的日志段文件（Segment File），消息生产者生产的消息发送到Broker后就会以追加到日志文件末尾的方式持久化到Partition中。 如果在Kafka运行时调整Topic的Partition数量，会直接影响Message根据Key的顺序问题。如果调整Replication数量，会给集群带来较大的性能压力，因为涉及到Zookeeper要重新选举Leader一系列操作。 副本如何与这里的分区联系在一起呢？ 实际上，副本是在分区这个层级定义的。每个分区下可以配置若干个副本，其中只能有 1 个领导者副本和 N-1 个追随者副本。生产者向分区写入消息，每条消息在分区中的位置信息由一个叫位移（Offset）的数据来表征。分区位移总是从 0 开始，假设一个生产者向一个空分区写入了 10 条消息，那么这 10 条消息的位移依次是 0、1、2、…、9。 ​消息位移：Offset。表示分区中每条消息的位置信息，是一个单调递增且不变的值。 ​Kafka 的三层消息架构： ​第一层是主题层，每个主题可以配置 M 个分区，而每个分区又可以配置 N 个副本。 ​第二层是分区层，每个分区的 N 个副本中只能有一个充当领导者角色，对外提供服务；其他 N-1 个副本是追随者副本，只是提供数据冗余之用。 第三层是消息层，分区中包含若干条消息，每条消息的位移从 0 开始，依次递增。 ​Kafka体系架构 = M个producer +N个broker +K个consumer+ZK集群 消费者组：Consumer Group。多个消费者实例共同组成的一个组，同时消费多个分区以实现高吞吐。 ​重平衡：Rebalance。消费者组内某个消费者实例挂掉后，其他消费者实例自动重新分配订阅主题分区的过程。Rebalance 是 Kafka 消费者端实现高可用的重要手段。 ​ 一个topic 可以让若干个consumer消费，若干个consumer组成一个 consumer group ，一条消息只能被consumer group 中一个consumer消费，若干个partition 被若干个consumer 同时消费，达到消费者高吞吐量 当创建topic的时候Kafka会保证所有副本均匀地在broker上保存。 ​两种消息模型，即点对点模型（Peer to Peer，P2P）和发布订阅模型。这里面的点对点指的是同一条消息只能被下游的一个消费者消费，其他消费者则不能染指。在 Kafka 中实现这种 P2P 模型的方法就是引入了消费者组（Consumer Group）。所谓的消费者组，指的是多个消费者实例共同组成一个组来消费一组主题。这组主题中的每个分区都只会被组内的一个消费者实例消费，其他消费者实例不能消费它。为什么要引入消费者组呢？主要是为了提升消费者端的吞吐量。多个消费者实例同时消费，加速整个消费端的吞吐量（TPS）。另外这里的消费者实例可以是运行消费者应用的进程，也可以是一个线程，它们都称为一个消费者实例（Consumer Instance）。 消费者组里面的所有消费者实例不仅“瓜分”订阅主题的数据，而且它们还能彼此协助。假设组内某个实例挂掉了，Kafka 能够自动检测到，然后把这个 Failed 实例之前负责的分区转移给其他活着的消费者。 每个消费者在消费消息的过程中必然需要有个字段记录它当前消费到了分区的哪个位置上，这个字段就是消费者位移（Consumer Offset）。注意，这和上面所说的位移完全不是一个概念。上面的“位移”表征的是分区内的消息位置，它是不变的，即一旦消息被成功写入到一个分区上，它的位移值就是固定的了。而消费者位移则不同，它可能是随时变化的，毕竟它是消费者消费进度的指示器嘛。另外每个消费者有着自己的消费者位移，因此一定要区分这两类位移的区别。消息在分区中的位移称为分区位移，而把消费者端的位移称为消费者位移。 ​ ​","link":"/2020/04/11/kafka%E5%9F%BA%E6%9C%AC/"},{"title":"初识Docker","text":"关于dockerdocker是一款以容器虚拟化技术为基础的软件 那么什么是虚拟化技术 ？ 虚拟化技术是一种将计算机物理资源进行抽象、转换为虚拟的计算机资源提供给程序使用的技术。 因为要配置各种环境等，给开发造成了很多困扰。 虚拟化还有一种作用，就是将虚拟化应用于资源管理。 假想一下，你要装mysql，redis等等，跑起一个服务端就比较费资源，虚拟化就可以很好地解决这件事情。就会有一种效果，那就是1+1&lt;2. 虚拟化技术通过资源隔离的方式，无形地也可以把这些程序隔离在不同的虚拟环境中，既然虚拟环境不同，自然运行在不同环境中的程序就不会互相干扰或争抢资源了。 docker的优势 基于容器技术的Docker拥有很高的跨平台性。Docker 的容器能够很轻松的运行在开发者本地的电脑，数据中心的物理机或虚拟机，云服务商提供的云服务器，甚至是混合环境中。 Docker 的轻量性和高可移植性能够很好的帮助我们完成应用的动态伸缩，我们可以通过一些手段近实时的对基于 Docker 运行的应用进行弹性伸缩，这能够大幅提高应用的健壮性。 不管是交付市场时间， 增加开发生产力，提高开发效率，节约基础设施成本，提升运维效率，以及加速问题解决时间。docker都有一个很好的作用。 关于docker的技术实现 Docker的实现，主要归结于三大技术，命令空间，控制组以及联合文件系统。大家可以更深入的去了解下。说到了Docker，就不得不先说说Docker的体系了。它有四个对象：镜像，容器，网络，数据卷。* 镜像：大概可以理解为一个只读的文件包。其中包含了虚拟环境运行最原始文件系统的内容。镜像是对容器运行环境进行持久化存储的结果。* 容器：容器就是用来隔离虚拟环境的基础设施，而在 Docker 里，它也被引申为隔离出来的虚拟环境。如果把镜像理解为编程中的类，那么容器就可以理解为类的实例。镜像内存放的是不可变化的东西，当以它们为基础的容器启动后，容器内也就成为了一个“活”的空间。 用更官方的定义来讲，Docker容器应该有三项内容组成。 一个Docker镜像 一个程序运行环境 一个指令集合 网络 对于大部分程序来说，它们的运行都不会是孤立的，而是要与外界或者更准确的说是与其他程序进行交互的，这里的交互绝大多数情况下指的就是数据信息的交换。网络通讯是目前最常用的一种程序间的数据交换方式了。 在 Docker 中，实现了强大的网络功能，我们不但能够十分轻松的对每个容器的网络进行配置，还能在容器间建立虚拟网络，将数个容器包裹其中，同时与其他网络环境隔离。 利用一些技术，Docker 能够在容器中营造独立的域名解析环境，这使得我们可以在不修改代码和配置的前提下直接迁移容器，Docker 会为我们完成新环境的网络适配。对于这个功能，我们甚至能够在不同的物理服务器间实现，让处在两台物理机上的两个 Docker 所提供的容器，加入到同一个虚拟网络中，形成完全屏蔽硬件的效果。 数据卷 在以往的虚拟机中，我们通常直接采用虚拟机的文件系统作为应用数据等文件的存储位置。然而这种方式其实并非完全安全的，当虚拟机或者容器出现问题导致文件系统无法使用时，虽然我们可以很快的通过镜像重置文件系统使得应用快速恢复运行，但是之前存放的数据也就消失了。 为了保证数据的独立性，我们通常会单独挂载一个文件系统来存放数据。这种操作在虚拟机中是繁琐的，因为我们不但要搞定挂载在不同宿主机中实现的方法，还要考虑挂载文件系统兼容性，虚拟操作系统配置等问题。值得庆幸的是，这些在 Docker 里都已经为我们轻松的实现了，我们只需要简单的一两个命令或参数，就能完成文件系统目录的挂载。","link":"/2018/05/11/%E5%85%B3%E4%BA%8Edocker/"},{"title":"使用Azure人脸API对图片进行人脸识别","text":"人脸识别作为人工智能领域较为成熟的机器学习应用方向，已在多个生产场景中发挥重要作用。从生物特征认证到智能考勤系统，从公共安防监控到商业客流分析，这项技术正持续赋能产业数字化转型。针对中小型企业在技术研发中普遍面临的算法门槛，微软Azure人脸API通过封装先进机器学习模型，提供标准化REST API接口及多语言SDK工具包，有效降低开发者的技术集成难度。 该服务支持对数字图像进行多维度的面部特征解析，可精准返回包括面部轮廓坐标、基础生物特征（性别、年龄）、微表情识别（喜悦、愤怒等情绪状态）、以及眼镜佩戴情况等40余项可视化数据。 使用场景身份核验系统：基于可信人脸图像进行身份比对验证，可实现数字资产与物理空间的智能准入控制。系统采用官方证件（如护照、驾驶证）或现场采集的注册照作为基准数据源，结合生物特征识别技术完成身份核验。关键性的活体检测模块可有效抵御照片翻拍、视频回放、三维面具等欺诈手段，通过分析面部微表情、血液流动特征等生物活性指标，确保验证对象为真实存在的自然人。 反欺诈防护机制：活体检测作为核心安全屏障，采用多模态感知技术（包括但不限于动态虹膜检测、红外成像、三维结构光分析）实时判别用户物理存在性，防范各类伪造生物特征的非法入侵行为。 无感通行解决方案：相较于传统实体凭证（门禁卡、票据等），智能人脸认证系统构建了更安全卫生的数字化访问体系。该方案不仅消除证件遗失、冒用带来的安全隐患，更通过非接触式交互显著提升公共场所通行效率，适用于机场安检、智慧园区、文体场馆、医疗教育机构等场景的智能化升级需求。 隐私增强技术：系统集成实时人脸模糊处理引擎，支持视频流中人脸区域的智能识别与动态脱敏。该技术符合GDPR等数据保护法规要求，通过像素扰动、特征加密等方式实现生物特征数据的合规化处理，在保障安防效能的同时维护个人隐私权益。 人脸检测和分析任务在所有应用场景中，人脸检测都是首要执行的核心步骤。通过调用人脸检测API，系统能够对输入图像进行面部特征分析，并输出检测到的人脸区域坐标（以矩形框形式呈现），同时生成与该人脸特征绑定的唯一标识码。此标识码将作为关键索引，在后续的人脸识别或身份核验等操作中实现数据关联。 除基础定位功能外，该检测技术还可解析多维度的生物特征数据，具体包括：头部空间姿态、年龄预测值、情绪状态评估（如喜悦、平静等）、面部毛发分布特征以及眼镜佩戴情况等重要参数。需要特别说明的是，这些属性分析结果属于基于算法的统计学预测，而非精确的确定性分类。部分关键参数（如面部遮挡检测）可有效保障人脸注册质量：当系统检测到用户注册时佩戴太阳镜，可触发交互提示建议用户调整面部状态；当头部偏转角度超出设定阈值时，可引导用户调整至标准姿态，从而确保录入的人脸特征数据达到系统要求的质量标准。 活体检测任务人脸活体检测是通过分析视频流中人脸的生物特征动态，判别检测对象是真实人体还是伪造媒介的关键技术。作为生物特征认证系统的核心安全模块，该技术能有效抵御通过照片翻拍、视频重播、高仿面具等伪造手段发起的呈现攻击（Presentation Attacks），从而防止非授权用户冒用他人身份非法侵入系统。 该技术的核心价值在于构建”真人验证”机制，确保身份认证过程必须与具备生命体征的实体进行交互。在数字金融、远程办公、智能安防等场景深度应用的背景下，活体检测技术已成为保障数字身份可信性的重要防线。有效的活体检测方案能够识别并抵御多种类型的伪造攻击，包括但不限于：纸质/电子屏显照片、2D/3D数字面具、屏幕重放攻击（如通过手机或电子设备展示预录视频）等。 作为信息安全领域的前沿研究方向，活体检测技术持续经历着攻防对抗的演进升级。面对不断进化的深度伪造（Deepfake）、对抗样本攻击等新型威胁，研究机构通过融合多模态生物特征分析、微表情检测、血流动力学分析等创新手段，持续强化防御体系的鲁棒性。相关技术成果会通过客户端和服务端的持续迭代更新进行部署，形成动态进化的全链路防护能力。 Azure人脸API概述 Azure人脸API是微软认知服务（Cognitive Services）中的核心组件，基于深度学习算法提供以下能力： 人脸检测：定位图片中人脸位置及关键特征点 属性分析：识别年龄、性别、情绪、面部毛发等87种属性 人脸验证：判断两张人脸是否属于同一人 人脸搜索：在大规模人脸库中进行1:N识别 活体检测：防止照片/视频伪造攻击（需v3.1及以上版本） 技术特性： 支持JPEG、PNG、GIF、BMP格式 图像尺寸范围：36x36 - 4096x4096像素 单图最大人脸数：100 响应时间：通常&lt;1秒 环境准备1. 创建Azure资源 登录 Azure Portal 创建Face资源： 选择订阅和资源组 区域选择eastus或westeurope（根据用户位置） 定价层：建议F0（免费，20调用/分钟）用于测试 创建人脸服务 获取密钥和终结点： 12终结点：https://&lt;your-region&gt;.api.cognitive.microsoft.com/密钥1：xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx 也可以使用界面来操作 2. 安装SDKPython环境安装： 1pip install azure-cognitiveservices-vision-face 人脸检测实现身份认证 123456789from azure.cognitiveservices.vision.face import FaceClientfrom msrest.authentication import CognitiveServicesCredentials# 配置认证信息KEY = &quot;your-subscription-key&quot;ENDPOINT = &quot;your-endpoint&quot;# 创建客户端实例face_client = FaceClient(ENDPOINT, CognitiveServicesCredentials(KEY)) 基础人脸检测12345678910111213141516# 本地图片检测with open(&quot;test.jpg&quot;, &quot;rb&quot;) as image_file: detected_faces = face_client.face.detect_with_stream( image=image_file, detection_model='detection_03', # 最新检测模型 recognition_model='recognition_04', # 最高精度识别模型 return_face_attributes=['age', 'gender', 'emotion', 'glasses'] )# 解析结果for face in detected_faces: print(f&quot;Face ID: {face.face_id}&quot;) print(f&quot;Age: {face.face_attributes.age}&quot;) print(f&quot;Gender: {face.face_attributes.gender}&quot;) print(f&quot;Emotion: {face.face_attributes.emotion}&quot;) print(f&quot;Face rectangle: {face.face_rectangle}&quot;) 高级参数配置1234567891011121314# 自定义返回属性return_face_attributes = [ 'age', 'gender', 'headPose', 'smile', 'facialHair', 'glasses', 'emotion', 'hair', 'makeup', 'occlusion', 'accessories', 'blur', 'exposure', 'noise']# 启用landmarks检测detected_faces = face_client.face.detect_with_url( url=&quot;https://example.com/image.jpg&quot;, return_face_landmarks=True, return_face_attributes=return_face_attributes) 典型应用场景人脸比对验证1234567# 获取两张人脸的faceIdface_id1 = detected_faces[0].face_idface_id2 = detected_faces[1].face_id# 执行验证result = face_client.face.verify_face_to_face(face_id1, face_id2)print(f&quot;Is identical: {result.is_identical} (confidence: {result.confidence})&quot;) 人脸库管理（人脸列表）123456789101112131415161718# 创建Person GroupPERSON_GROUP_ID = &quot;employees&quot;face_client.person_group.create(person_group_id=PERSON_GROUP_ID, name=&quot;Employee Database&quot;)# 添加人员person = face_client.person_group_person.create(PERSON_GROUP_ID, name=&quot;John Doe&quot;)# 注册人脸with open(&quot;john_photo1.jpg&quot;, &quot;rb&quot;) as image: face_client.person_group_person.add_face_from_stream( PERSON_GROUP_ID, person.person_id, image)# 训练模型face_client.person_group.train(PERSON_GROUP_ID)# 人脸搜索test_face = face_client.face.detect_with_url(&quot;https://example.com/unknown.jpg&quot;)[0]results = face_client.face.identify([test_face.face_id], PERSON_GROUP_ID) 通过简单的一个wpf的应用演示了如果使用Azure人脸API进行图片中的人脸检测 性能优化建议批量处理：使用detect_in_batch处理多张图片 12image_urls = [&quot;url1&quot;, &quot;url2&quot;, &quot;url3&quot;]responses = face_client.face.detect_in_batch(image_urls) 异步处理：对于大规模识别任务使用异步API 12345678from azure.core.polling import LROPolleroperation = face_client.face.detect_in_batch( image_urls, is_async=True)poller = LROPoller(face_client, operation)results = poller.result() 缓存策略：face_id有效期为24小时，可重复使用 安全与合规 隐私保护措施： 默认不存储用户图片 数据加密传输（HTTPS） GDPR合规性认证 使用建议： 关键业务系统启用活体检测 定期更新识别模型版本 对敏感数据启用Azure私有终结点 进阶 表情识别深度分析： 123emotion = face.face_attributes.emotiondominant_emotion = max(emotion.__dict__.items(), key=lambda x: x[1])[0]print(f&quot;主要情绪：{dominant_emotion} (置信度：{getattr(emotion, dominant_emotion)})&quot;) 3D头部姿态估计： 12head_pose = face.face_attributes.head_poseprint(f&quot;头部姿态 - 俯仰角：{head_pose.pitch}°, 偏航角：{head_pose.yaw}°, 翻滚角：{head_pose.roll}°&quot;) 质量检测： 12345quality = face.face_attributes.qualityif quality.noise.value == 'high': print(&quot;图片噪点过多可能影响识别精度&quot;)if quality.blur.value == 'high': print(&quot;图片模糊度过高&quot;) 成本优化 免费层(F0)限制： 20请求/分钟 30,000次/月 标准层(S0)建议： 按API调用次数计费 10,000次识别≈$1.0（具体因区域而异） 优化建议： 启用请求批处理 使用本地缓存减少重复识别 设置QPS限制避免突发流量","link":"/2025/01/20/%E4%BD%BF%E7%94%A8Azure%E4%BA%BA%E8%84%B8API%E5%AF%B9%E5%9B%BE%E7%89%87%E8%BF%9B%E8%A1%8C%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/"},{"title":"vim技巧","text":"目录简介小技巧启动及关闭教程篇文本编辑文本编辑的高效命令other 简介得益于 vim 的指法，敲起代码来如行云流水。不管是不是写代码，学好vim 指法相当重要，当然最重要的还是为了效率，节省时间做更多其他的事。 小技巧“工欲善其事，必先利其器”。在 Vi/Vim 版本的选择上，原则是“能用 Vim 就不要使用 Vi”。Vim 提供的功能和特性要比 Vi 多得多，如语法加亮着色功能等。就使用效果及效率来说，编辑同样的文件，使用 Vim 更胜一筹；就版本来说，新版的往往会修复旧版的一些缺陷及不足。这就要求我们在可能的情况下一定要使用最新版的 Vim。 启动及关闭 退出 ZQ 无条件退出 q!无条件退出 ZZ 存盘并退出 :wq 存盘并退出 保存部分文件 :m,nw &lt; file&gt;将 m 行到 n 行部分的内容保存到文件 中 :m,nw &gt;&gt; 将 m 行到 n 行的内容添加到文件 的末尾 保存文件 :w 教程篇默认的 vim 是没有显示行数的，可自行在 vim 配置文件里开启(自行Google) Vi/Vim 中操作单位有很多，按从小到大的顺序为（括号内为相应的操作命令）：字符（h、l）→ 单词 (w、W、b、B、e) → 行 (j、k、0、^、$、:n) → 句子（(、)）→ 段落（{、}）→ 屏 (H、M、L) → 页（Ctrl-f、Ctrl-b、Ctrl-u、Ctrl-d) → 文件（G、gg、:0、:$）。 字符 h左移一位,l右移一位 单词 w/W 移动到下一单词的开头 b/B 移动到上一单词的开头 e/E 移动到光标所在单词的末尾 f 快速移动到下一个字符的位置 行 j 下移一行 k 上移一行 0 移到当前行开头 ^ 移到当前行的第一个非空字符 $ 移到当前行末尾 :n 移动到第 n 行 句子 ) 移动到当前句子的末尾 ( 移动到当前句子的开头 段落 } 移动当前段落的末尾 { 移到当前段落的开头 屏 H 移动到屏幕的第一行 M 移动到屏幕的中间一行 L 移动到屏幕的最后一行 页 Ctrl-f 向前滚动一页 Ctrl-b 向后滚动一页 Ctrl-u向前滚动半页 Ctrl-d 向后滚动半页 文件 G 移动到文件末尾 gg 移动到文件开头 :0移动到文件第一行 :$ 移动到文件最后一行 文本编辑 与光标移动一样，Vi/Vim 中关于编辑操作的命令也比较多，但操作单位要比移动光标少得多。按从小到大的顺序为（括号内为相应的操作命令）：字符 （x、c、s、r、i、a）→ 单词 (cw、cW、cb、cB、dw、dW、db、dB) → 行 (dd、d0、d$、I、A、o、O) → 句子（(、)）→ 段落（{、}）。这些操作单位有些可以加操作次数。操作对象的范围计算公式为：操作范围 = 操作次数 * 操作单位。比如：d3w 命令删除三个单词，10dd 命令删除十行。 字符 x 删除光标位置的字符 c 更改当前字符并进入插入模式 s 替换光标位置的字符并进入插入模式 r 替换光标位置的字符但不进入插入模式 i 在当前位置的字符之前进入插入模式 a 在当前位置的字符之后进入插入模式 单词 cw/cW 删除当前单词从光标开始的部分并进入插入模式 cb/cB 删除当前单词从光标所在位置至单词开始的部分并进入插入模式 dw/dW 删除当前单词从光标开始的部分但不进入插入模式 db/dB 删除当前单词从光标所在位置至单词开始的部分但不进入插入模式 行 dd 删除当前行 d0 删除从当前光标开始到行末的内容 d$ 删除从当前光标开始到行末的内容 I 在当前行的行首进入插入模式 A 在当前行的行尾进入插入模式 o 在当前行下方另起一行进入插入模式 O 在当前行上方另起一行进入插入模式 句子 d) 删除当前句子从光标位置开始到句末的内容 d( 删除当前句子从光标位置开始到句首的内容 段落 d} 删除当前段落从光标位置开始到段末的内容 d{ 删除当前段落从光标位置开始到段首的内容 文本编辑的高效命令 复制与粘贴 yw 复制当前单词从光标开始的部分 yy 复制光标所在行的所有字符 p 将最后一个删除或复制文本放在当前字符 P 将最后一个删除或复制文本放在当前字符之前 撤消与重做 u 撤消更改 Ctrl-R 重做更改 重复操作 .重复上次操作 交换相邻字符或行 xp 交换光标位置的字符和它右边的字符 ddp 交换光标位置的行和它的下一行 大小写转换 ~ 将光标下的字母大小写反向转换 guw 将光标所在的单词变为小写 guw 将光标所在的单词变为小写 gUw 将光标所在的单词变为大写 guu 光标所在的行所有字符变为小写 gUU 光标所在的行所有字符变为大写 g~~ 光标所在的行所有字符大小写反向转换 排序 :1,$!sort 将文件内的所有内容排序 other先定单位再定量 操作对象的范围计算公式为：操作范围 = 操作次数 * 操作单位。比如：5h 命令左移 5 个字符，8w 命令右移 8 个单词。","link":"/2019/06/01/vim%20%E6%8A%80%E5%B7%A7/"},{"title":"关于iframe跨域传输","text":"至于我为什么想写这篇文章是因为最近在项目中使用到了iframe，是的。生无可恋的又写上了一点js，可能是因为前端的人对单点登录啥的或者是页面跳转以及要和后端的逻辑处理起来不是很熟练吧。各大网站，包括淘宝，京东，这些大网站有很多自己的产品，至于前期是怎么样的不是很清楚，网易云至少是用的iframe。参考了一些博客，至于使用不使用iframe，我觉得能解决问题就好，而且如果考虑的多的话就考虑以后扩展以及拆分啥的，毕竟前端又不像后端这样。 因为要解决跨域问题。有很多方案，比如说iframe，jsonp(不过只支持get，对于一些铭感信息就不行了) 原本需求是登录在一个站点，而注册是另外一个站点。因为要实时反馈到iframe子页面，子页面在进行相应。 而在Windows对象下有个postMessage方法，是解决跨越问题的假设有两个不同源的页面，iframe.html和index.html 其中前者是后者的子页面。 123456789&lt;!-- index.html --&gt;&lt;body&gt; &lt;h1&gt;this is index&lt;/h1&gt; &lt;iframe src=&quot;./iframePage.html&quot; id='iframe'&gt;&lt;/iframe&gt;&lt;/body&gt; 1234567&lt;!-- iframePage --&gt;&lt;body&gt; &lt;h1&gt;this is iframePage&lt;/h1&gt;&lt;/body&gt; 现在这两个是无法通信的，因为是不同的站点，所以这个时候就要用到postMessage 123456789101112// idnex.html//获取iframe元素,当然也可以使用其他的js框架iFrame = document.getElementById('iframe')//iframe加载完毕后再发送消息，否则子页面接收不到messageiFrame.onload = function(){ //iframe加载完立即发送一条消息 iFrame.contentWindow.postMessage('MessageFromIndex1','*');} 我们知道postMessage是挂载在window对象上的，所以等iframe加载完毕后，用iFrame.contentWindow获取到iframe的window对象，然后调用postMessage方法，相当于给子页面发送了一条消息。 postMessage方法第二个参数可以设置要发送到哪个url，如果当前子页面的url和设置的不一致，则会发送失败，因为没啥限制就设置为*，代表所有url都允许发送。 消息发送到iframePage.html，我们来接收message 12345678910// iframePage.html//回调函数function receiveMessageFromIndex ( event ) { console.log( 'receiveMessageFromIndex', event )}//监听message事件window.addEventListener(&quot;message&quot;, receiveMessageFromIndex, false); 然后设置好回调函数，就可以了，data中或许还有其他的数值，所以在接受的时候判断一下。","link":"/2018/07/31/%E5%85%B3%E4%BA%8Eiframe%E8%B7%A8%E5%9F%9F%E4%BC%A0%E8%BE%93/"},{"title":"关于生成订单号规则的一些思考","text":"关于我为什么写这篇文章是因为今天在做订单模块的时候,看到之前的PRD上描述的订单生成规则是由 年月日＋用户id2位+企业id位＋四位自增长数。然后竟被我反驳的突然改成了精确时间＋4位自增长数，于是我更失望了。 我们考虑一下，据我所常见的订单基本都14-20位。(年月日时分秒和随机数)基本上就有14位了。虽然一般项目做不到淘宝双11这种支付峰值达到每秒10万笔订单.但是我觉得至少事先可以考虑到，想必当初淘宝或许也没意识到以后发展得这么好。 背景为了达到业务订单的生成。我觉得要至少要符合以下这三种, 全局唯一 一定不能重复 在复杂的分布式系统中，很多场景需要的都是全局唯一ID的场景，一般为了防止冲突可以考虑的有36位的UUID,twitter的snowflake等。 但是可以思考这些问题？ 是不是应该有一些其他意义的思考，比如说订单系统有买家的id(取固定几位) 是否有商品的标识,方便熟悉业务的排查问题或者查询也通过不去系统查找可以有个初步的认识，但是业务量大的话感觉就可以排除这个人为的去辨识了。 个人的看法是主要是唯一，其他关于业务方面的不是太太重要。 查阅了相关资料，主要有以下这几种 UUID, 组成：当前日期+时间+时钟序列+机器识别号（Mac地址或其他）没有mac网卡的话会有别的东西识别。在分布式系统中，所有元素（WEB服务器）都不需要通过中央控制端来判断数据唯一性。几十年之内可以达到全球唯一性。 snowflake的结构如下(每部分用-分开): Mysql通过AUTO_INCREMENT实现、Oracle通过Sequence序列实现。在数据库集群环境下，不同数据库节点可设置不同起步值、相同步长来实现集群下生产全局唯一、递增ID Snowflake算法 雪花算法 41位时间戳+10位机器ID+12位序列号（自增） 转化长度为18位的长整型。 Twitter为满足美秒上万条消息的创建，且ID需要趋势递增，方便客户端排序。 Snowflake虽然有同步锁，但是比uuid效率高。 Redis自增ID 实现了incr(key)用于将key的值递增1，并返回结果。如果key不存在，创建默认并赋值为0。 具有原子性，保证在并发的时候。 但是我在这主要想说的是雪花算法生成id,至于为什么，就测试了一下其他的，感觉这种生成方式个人比较喜欢。 Snowflake算法规则如下 使用41bit作为毫秒数，10bit作为机器的ID（5个bit是数据中心，5个bit的机器ID），12bit作为毫秒内的流水号（意味着每个节点在每毫秒可以产生 4096 个 ID），最后还有一个符号位，永远是0。 0 - 0000000000 0000000000 0000000000 0000000000 0 - 00000 - 00000 - 000000000000 该算法实现基本是二进制操作。 一共加起来刚好64位，为一个Long型。(转换成字符串长度为18) snowflake生成的ID整体上按照时间自增排序，并且整个分布式系统内不会产生ID碰撞（由datacenter和workerId作区分），并且效率较高。据说：snowflake每秒能够产生26万个ID。 以下是代码部分借鉴与网络100万个ID 耗时２秒 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130/** * Created by youze on 18-7-5 */public class IdWorker { /** * 起始的时间戳 */ private final static long START_STMP = 1530795377086L; /** * 每一部分占用的位数 */ /** * 序列号占用的位数 */ private final static long SEQUENCE_BIT = 12; /** * 机器标识占用的位数 */ private final static long MACHINE_BIT = 5; /** * 数据中心占用的位数 */ private final static long DATACENTER_BIT = 5; /** * 每一部分的最大值 */ private final static long MAX_DATACENTER_NUM = -1L ^ (-1L &lt;&lt; DATACENTER_BIT); private final static long MAX_MACHINE_NUM = -1L ^ (-1L &lt;&lt; MACHINE_BIT); private final static long MAX_SEQUENCE = -1L ^ (-1L &lt;&lt; SEQUENCE_BIT); /** * 每一部分向左的位移 */ private final static long MACHINE_LEFT = SEQUENCE_BIT; private final static long DATACENTER_LEFT = SEQUENCE_BIT + MACHINE_BIT; private final static long TIMESTMP_LEFT = DATACENTER_LEFT + DATACENTER_BIT; /** * 数据中心 */ private long datacenterId; /** * 机器标识 */ private long machineId; /** * 序列号 */ private long sequence = 0L; /** * 上一次时间戳 */ private long lastStmp = -1L; public IdWorker(long datacenterId, long machineId) { if (datacenterId &gt; MAX_DATACENTER_NUM || datacenterId &lt; 0) { throw new IllegalArgumentException(&quot;datacenterId can't be greater than MAX_DATACENTER_NUM or less than 0&quot;); } if (machineId &gt; MAX_MACHINE_NUM || machineId &lt; 0) { throw new IllegalArgumentException(&quot;machineId can't be greater than MAX_MACHINE_NUM or less than 0&quot;); } this.datacenterId = datacenterId; this.machineId = machineId; } /** * 产生下一个ID * @return */ public synchronized long nextId() { long currStmp = getNewstmp(); if (currStmp &lt; lastStmp) { throw new RuntimeException(&quot;Clock moved backwards. Refusing to generate id&quot;); } if (currStmp == lastStmp) { //相同毫秒内，序列号自增 sequence = (sequence + 1) &amp; MAX_SEQUENCE; //同一毫秒的序列数已经达到最大 if (sequence == 0L) { currStmp = getNextMill(); } } else { //不同毫秒内，序列号置为0 sequence = 0L; } lastStmp = currStmp; return ( //时间戳部分 currStmp - START_STMP) &lt;&lt; TIMESTMP_LEFT //数据中心部分 | datacenterId &lt;&lt; DATACENTER_LEFT //机器标识部分 | machineId &lt;&lt; MACHINE_LEFT //序列号部分 | sequence; } private long getNextMill() { long mill = getNewstmp(); while (mill &lt;= lastStmp) { mill = getNewstmp(); } return mill; } private long getNewstmp() { return System.currentTimeMillis(); } public static void main(String[] args) { IdWorker snowFlake = new IdWorker(2, 3); long start = System.currentTimeMillis(); for (int i = 0; i &lt; 1000000; i++) { System.out.println(snowFlake.nextId()); } System.out.println(System.currentTimeMillis() - start); }} 最后大家可以看这也有更详细的解释","link":"/2018/08/10/%E5%85%B3%E4%BA%8E%E7%94%9F%E6%88%90%E8%AE%A2%E5%8D%95%E5%8F%B7%E8%A7%84%E5%88%99%E7%9A%84%E4%B8%80%E4%BA%9B%E6%80%9D%E8%80%83/"},{"title":"分析MySQL中隐式转换导致查询结果错误及索引不可用","text":"以下是例子 1SELECT * FROM TABLE WHERE xxx = 11 如果列xxx确实只有11的，你是否就认为筛选出来的就一定只有xxx=11的呢？ 在过滤字段为数值类型的时候，数值类型有一种隐式转换，如果是以数字开头的，包含有字符，后面的字符会被截断，只取前面的数字值。 以下也均为测试数据 当执行 1explain select * from business_flow where business_flow_id = 268805964457574426 看输出会出现这段话 Cannot use ref access on index ‘xxx’ due to type or collation conversion on field ‘business_flow_id’ 当过滤的字段是字符类型的时候，没有使用到索引，走的全表扫描； 所以还是可以查询出结果来的，因为无法使用索引，所以查询出来的结果也是错的。 既然发现查询出来的结果是有误差的，所以猜测用字符串’xxx’和xxy比较应该是相等的。 1select '268805964457574426' =268805964457574421 果不其然，也能查询出 去查询了下其他的 过滤字段为浮点类型，也会比较近似的，将导致结果看起来不一致，也就是可能导致查询结果错误 当MySQL遇到字段类型不匹配的时候，会进行各种隐式转化 所以在查询过滤的时候，一定要注意过滤字段的类型。可能会导致查询慢，甚至会导致错误结果。 官方说是隐式转换 参考","link":"/2019/10/11/%E5%88%86%E6%9E%90MySQL%E4%B8%AD%E9%9A%90%E5%BC%8F%E8%BD%AC%E6%8D%A2%E5%AF%BC%E8%87%B4%E6%9F%A5%E8%AF%A2%E7%BB%93%E6%9E%9C%E9%94%99%E8%AF%AF%E5%8F%8A%E7%B4%A2%E5%BC%95%E4%B8%8D%E5%8F%AF%E7%94%A8/"},{"title":"反射","text":"反射来自元编程，指通过类型检查变量本身数据结构的方式，只有部分编程语言支持反射。反射是指一类应用，它们能够自描述和自控制。也就是说，这类应用通过采用某种机制来实现对自己行为的描述（self-representation）和监测（examination），并能根据自身行为的状态和结果，调整或修改应用所描述行为的状态和相关的语义。 Go 语言提供了一种机制在运行时更新变量和检查它们的值、调用它们的方法，但是在编译时并不知道这些变量的具体类型，这称为反射机制。 在什么情况下需要反射 不能明确接口调用哪个函数，需要根据传入的参数在运行时决定。 不能明确传入函数的参数类型，需要在运行时处理任意对象。 类型反射构建在类型系统之上，Go是静态类型语言，每一个变量都有静态类型，在编译时就确定下来了。 比如 1234type MyInt intvar i intvar j MyInt i和j的底层类型都是int，但i的静态类型是int，j的静态类型是MyInt，这两个是不同类型，是不能直接赋值的，需要类型强制转换。 动态类型 12345var A interface{} // 静态类型interface{}A = 10 // 静态类型为interface{} 动态为intA = &quot;String&quot; // 静态类型为interface{} 动态为stringvar M *intA = M // A的值可以改变 掌握reflect包的以下函数： reflect.ValueOf({}interface) reflect.Value：获取某个变量的值，但值是通过reflect.Value对象描述的。 reflect.TypeOf({}interface) reflect.Type：获取某个变量的静态类型，但值是通过reflect.Type对象描述的，是可以直接使用Println打印的。 reflect.Value.Kind() Kind：获取变量值的底层类型（类别），注意不是类型，是Int、Float，还是Struct，还是Slice，具体见此。 reflect.Value.Type() reflect.Type：获取变量值的类型，效果等同于reflect.TypeOf。 再解释下Kind和Type的区别 123type MyInt intvar x MyInt = 7v := reflect.ValueOf(x) v.Kind()得到的是Int，而Type得到是MyInt。 反射原理反射的意思是在运行时，能够动态知道给定数据对象的类型和结构，并有机会修改它！现在一个数据对象，如何判断它是什么结构？数据interface中保存有结构数据，只要想办法拿到该数据对应的内存地址，然后把该数据转成interface，通过查看interface中的类型结构，就可以知道该数据的结构了 反射三原则 从interface{}可以反射出反射对象 将 Go 语言的 interface{} 变量转换成反射对象。执行 reflect.ValueOf(1) 时，由于 reflect.TypeOf、reflect.ValueOf 两个方法的入参都是 interface{} 类型，所以在方法执行的过程中发生了类型转换。使用 reflect.TypeOf 和 reflect.ValueOf 能够获取 Go 语言中的变量对应的反射对象。一旦获取了反射对象，我们就能得到跟当前类型相关数据和操作，并可以使用这些运行时获取的结构执行方法。 从反射对象中可以获取到interface{} 既然能够将接口类型的变量转换成反射对象，那么一定需要其他方法将反射对象还原成接口类型的变量，reflect 中的 reflect.Value.Interface 就能完成这项工作：不是所有的变量都需要类型转换这一过程。如果变量本身就是 interface{} 类型的，那么它不需要类型转换，因为类型转换这一过程一般都是隐式的，所以我不太需要关心它，只有在我们需要将反射对象转换回基本类型时才需要显式的转换操作。 要修改反射对象, 其值必须可设置 Go 语言的函数调用都是传值的，所以得到的反射对象跟最开始的变量没有任何关系，那么直接修改反射对象无法改变原始变量，程序为了防止错误就会崩溃。先获取指针对应的 reflect.Value，再通过 reflect.Value.Elem 方法得到可以被设置的变量 interface{}本质上Go提供的一种数据类型, 与其他数据类型不同的是, interface{}会为我们提供变量的类型信息以及变量所在的内存地址。 通过反射修改原对象 原理： 因为给Go的函数、方法传递的都是形参的副本，同样的，反射一个对象时，形参被保存为一个接口对象并作为参数传递（复制），该接口变量是non-settable的，返回的Value也是non-settable的，对它调用Set方法会出现错误； Value的CanSet方法用于测试一个Value的Settablity性质，它有点像unaddressability，但是更加严格，描述的是一个反射对象能够修改创造它的那个实际存储的值的能力。settability由反射对象是否保存原始项而决定。 如果想通过反射来修改对象，必须先把该对象的指针传给reflect.ValueOf(&amp;x)，这样得到的Value对象内部就保存了原对象指针的副本，只有找到该指针指向的值才能修改原始对象，通过Elem()方法就可以获得一个保存了原对象的Value对象，此时的Value对象就是settable的； 对于一个settable的Value反射对象，如 d := reflect.ValueOf(&amp;x).Elem()： d.CanAddr()方法：判断它是否可被取地址 d.CanSet()方法：判断它是否可被取地址并可被修改 通过一个settable的Value反射对象来访问、修改其对应的变量的方式： 方式1：通过把反射对象转换回原对象类型的指针，然后直接修改该指针 px := d.Addr().Interface().(*int) 第一步是调用Addr()方法，它返回一个Value，里面保存了指向变量的指针。 然后是在Value上调用Interface()方法，也就是返回一个interface{}，里面通用包含指向变量的指针。 最后，如果知道变量的类型，可以使用类型的断言机制将得到的interface{}类型的接口强制环为普通的类型指针。这样就可以通过这个普通指针来更新变量了 方式2：可直接通过Set()方法来修改 d.Set(reflect.ValueOf(4)) SetInt、SetUint、SetString和SetFloat等方法：d.SetInt(3)，注意：虽然如SetInt()等方法只要参数变量的底层数据类型是有符号整数就可以工作，但不能是一个引用interface{}类型的reflect.Value 小结：Value反射对象为了修改它们所表示的东西必须要有这些东西的地址 1234567891011121314151617181920package mainimport ( &quot;fmt&quot; &quot;reflect&quot;)func main() { var x float64 = 3.4 p := reflect.ValueOf(&amp;x) // 注意这里：把x地址传进去了！ fmt.Println(p.Type()) //*float64 fmt.Println(p.CanSet()) //false 这里的p只是指针，仍然是non-settable的 v := p.Elem() //此时的v保存了x fmt.Println(v.CanSet()) //true v.SetFloat(7.1) fmt.Println(v.Interface()) //7.1 fmt.Println(x) //7.1} 虽然反射可以越过Go语言的导出规则的限制读取结构体中未导出的成员，但不能修改这些未导出的成员。因为一个struct中只有被导出的字段才是settable的。 1234567891011121314151617181920212223242526272829package mainimport ( &quot;fmt&quot; &quot;reflect&quot;)func main() { type T struct { A int B string } t := T{23, &quot;skidoo&quot;} s := reflect.ValueOf(&amp;t).Elem() typeOfT := s.Type() // 把s.Type()返回的Type对象复制给typeofT，typeofT也是一个反射。 for i := 0; i &lt; s.NumField(); i++ { f := s.Field(i) // 迭代s的各个域，注意每个域仍然是反射。 fmt.Printf(&quot;%d: %s %s = %v\\n&quot;, i, typeOfT.Field(i).Name, f.Type(), f.Interface()) // 提取了每个域的名字 } // 0: A int = 23 // 1: B string = skidoo s.Field(0).SetInt(77) // s.Field(0).Set(reflect.ValueOf(77)) s.Field(1).SetString(&quot;Sunset Strip&quot;) fmt.Println(&quot;t is now&quot;, t) // t is now {77 Sunset Strip}} 如何实现字符串和byte切片的零拷贝123456func string2bytes(s string) []byte { return *(*[]byte)(unsafe.Pointer(&amp;s))}func bytes2string(b []byte) string{ return *(*string)(unsafe.Pointer(&amp;b))} 原理上是利用指针的强转 反射慢的原因 发生堆逃逸 （比如ValueOf函数很简单，先将i主动逃逸到堆上，然后将 i 通过unpackEface函数转换成Value。） 逃逸到堆意味着将值拷贝一份到堆上，这也是反射慢的主要原因。 涉及到内存分配以及后续的GC； reflect实现里面有大量的枚举，也就是for循环，比如类型之类的。 建议 可以只使用reflect.TypeOf的话，就不要使用reflect.ValueOf 可以使用断言代替的话，就不要使用反射 如果有可能应当避免使用反射","link":"/2019/03/11/%E5%8F%8D%E5%B0%84/"},{"title":"我所读过的书","text":"这里记录一下关于我读过的书籍 2018年 Java JDK 7学习笔记 时间: 2018年1月18日 18:42:27 很久之前看的了。突然想标记一下。初学的话就花2h看一下吧。不太建议看的书 高效程序员的45个习惯：敏捷开发修炼之道 时间：2018年3月9日09:45:14 可能是现在还不是很注重，所以很快就浏览完了。我感觉工作半年内还是不读的好，这本书的价值不大 Mongo基础命令参考 时间：2018年3月18日23:11:02 一本野书，基本上就是熟悉一些Ｍｏｎｇｏｄｂ的命令，可以花１个小时浏览下就完了 JAVA编程思想 时间：2018年4月17日00:26:07 说实话，这本书真的是太长了，有精华的东西，同时也有淘汰的东西。距离上次记录书籍已经有一个月了，等再看的时候详细看一遍吧。 算法图解 时间：2018年5月14日00:41:01 Aditya Bhargava 作品，很不错的一本书，再加上是我非常喜欢的图灵教育出版的书。推荐读，而且讲的很形象。基础算法 人工智能 时间：2018年06月26日20:21:4５ 李开复的作品，对认知又多了一点，反正我是很相信人工智能带给社会的进步，以后必将是高科技与艺术的并存。工业上可以取代，但艺术不能取代。 Redis 入门指南 时间：2018年7月31日22:51:45 李子骅 编著 很好的一本基础书，命令很多。记了一些常用的。 Go语言编程 时间：2018年11月18日20:05:42 作者：许式伟 吕桂华 有点失望，给我的感觉就好像是Java一些所谓的“从入门到精通”一类的书，300页的书，看看目录其实有的人就会选择不看，但我还是看完了。深度也是点到为止，当然这其实是要自己去挖掘的。网络编程的那些其实我感觉就是翻译一些手册，也不是国外的文章。比较偏向于教科书方面的书 数学之美 时间：2018年12月06日09:22:58 作者：吴军 马尔可夫链是如此的熟悉，常见的新闻分类竟然是利用了余弦函数。其实大学阶段的知识真的是基础，比如说自然语言处理其实就可以抽象为比较简单的通信模型和统计学模型。利用一些概率公式然后再加上马尔科夫假设就可以做到机器翻译和语音识别。以及我现阶段最想做的搜索。其实布尔代数在支撑着搜索引擎索引的数学基础，当然要做好每个方向是要掌握本质以及精髓，做起事情来也会如诗般顺滑。 2017年 java解惑 这本书还好吧，反正让你会怀疑你的基础学的不扎实，新手老手都建议看一看，不过快速的看一下就好，花1天时间吧 图解HTTP 很好的一本书，值得一读 Java核心技术+卷1（原书第9版）》 这本书粗略过了，因为看的是pdf，打算再吧第十版看下 代码整洁之道 业界传闻的巴拉巴拉的必读书，同样建议看一看，好的代码格式，好的代码规范，以及注释等，也能体现一个人的水平。以前面试阿里的时候，就被问到代码规范之类的问题。建议多刻意练习。多看点源码，源码的规范就很好。","link":"/2018/10/10/%E6%88%91%E6%89%80%E8%AF%BB%E8%BF%87%E7%9A%84%E4%B9%A6/"},{"title":"构建自己高效的workflow","text":"喜欢去探索各种效率工具，自然离不开alfred。alfred可以完成很多事情，其中包括打开各种app，搜索文件，搜索引擎等太多了。 这篇文章主要是记录 关于 Alfred的workflow的开发 alfred 插件开发概述Workflow 是alfred2.0推出的最激动人心的特性, 通过与脚本语言的交互，workflow可以支持任意操作，把您日常的重复性事务封装在脚本中，大大的提高工作效率。 Workflow 支持php、bash、perl、ruby以及python作为脚本语言，并内置脚本语言解释器，并通过stdio的形式在各个脚本模块中传递参数。 在代码中插入 {query}块可以接收上一个脚本输出的内容。形成完整的控制链条。 最后由alfred输出至 Output 模块， 在Output模块中， 我们可以启动浏览器、将内容复制到剪切板、 启动通知中心、甚至执行bash脚本。在日常的使用中，我们通常通过关键字来调用某一模块，例如“find xxx” 即是调用find内建模块 query内容为xxx。 在workflow的开发中， 开发者可以自定义自己编写模块的关键字，只要不与其他模块冲突即可。在workflow的结构中，数据流通过alfred的控制线进行传递，每一个脚本模块的STDIO输出会被alfred替换到 下一个脚本的{query}块中。 创建一个新的workflow 首先点击workflow 创建一个workflow 然后图里的主要是Bundle Id ，主要就是唯一、description等这些看自己，都是一些无关键要的东西。可以简单对你的脚本进行描述或者是一个良好的命名或者也行了。 创建一个带有输入参数得workflow然后就是选中之前创建的workflow。在右侧选中input，如下图所示。比如我们这里是创建一个关于时间戳转换的脚本，可以选择input里的Script Filter 然后出来以下这个界面，keyword就是唤醒的关键键。 如果这里是Python脚本的话，language这里是要选择/bin/zsh的，然后如果你的脚本是需要输入参数的话。后面要跟上wtih input as {query}，如果你的脚步完全不依赖于外部库的话，也是可以直接在这里写的 然后 Script 这里的话是需要写你运行的脚本的路径 然后打开上图中 问号旁边的文件夹，会看到这样一个文件。接下来要做的事情就是要把python3有关的workflow代码放在这里去。 这里有一个不小的坑，然后网上大部分帖子都是粘贴复制，所以很多都还是n多年前复制过来的，因为原始仓库里只有python2的代码，python3不支持部分库了，所以我是根据 这里的一个仓库，clone下来后，其实只需要workflow里的代码就行。所以最终你会看到这样一个目录 另外附上t.py里的代码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import requestsimport reimport sysfrom datetime import datetimefrom workflow import Workflow3S = requests.Session()REGEXP_TIMESTAMP = r'^\\d+$'def convert_timestamp_to_datetime(timestamp): try: timestamp = int(timestamp) dt_object = datetime.utcfromtimestamp(timestamp) return dt_object.strftime('%Y-%m-%d %H:%M:%S') except ValueError: return &quot;Invalid timestamp&quot;def generate_feedback_results(judge_code, result): wf = Workflow3() if judge_code == 1: kwargs = { 'title': result, 'subtitle': '', &quot;valid&quot;: True, 'arg': result } else: kwargs = { 'title': result, 'subtitle': '', 'valid': False } wf.add_item(**kwargs) wf.send_feedback()def main(): timestamp = sys.argv[1] if len(re.findall(REGEXP_TIMESTAMP, timestamp)) &gt; 0: result = convert_timestamp_to_datetime(timestamp) generate_feedback_results(1, result) else: generate_feedback_results(0, &quot;Invalid timestamp&quot;)if __name__ == &quot;__main__&quot;: main() 然后效果大概就是这样了 其实类似于这些的话也能在python里通过参数来实现，也就是终端，但是如果是多方的一些东西的话，可能还是借助于该工具比较好 创建一个带有无参数得workflow然后可能也有会有这样的需求，比如根据python处理后的结果，然后需要自动把这个结果比如复制到粘贴板里面去。然后就这样这样做 选择一个outputs，然后再选择copy to clipboard 。 这样输入now的时候，就会自动显示时间戳还能复制到粘贴板里面去了 从debug里的日志也能看出来 debug 模式当然在创建workflow的过程中，还是比较顺畅的，如果要开启debug模式也很简单。 这里就能完美开启了 其他比如在项目中会经常用ide打开某一些开发的应用，也可以一套流程下来。一键启动打开。如果大家有啥好玩的workflow也可以交流交流","link":"/2023/11/05/%E6%88%91%E6%98%AF%E5%A6%82%E4%BD%95%E6%8F%90%E5%8D%87%E6%88%91%E7%9A%84%E6%95%88%E7%8E%87%E7%9A%84/"},{"title":"数据删掉一半，表的大小不变","text":"数据库占用空间太大，把一个最大的表删掉了一半的数据，怎么表文件的大小还是没变？ 先来看看这块儿知识 一个 InnoDB 表包含两部分，即：表结构定义和数据。在 MySQL 8.0 版本以前，表结构是存在以.frm 为后缀的文件里。而 MySQL 8.0 版本，则已经允许把表结构定义放在系统数据表中了。因为表结构定义占用的空间很小 其中有一个参数innodb_file_per_table，它的值有on和off，OFF 表示的是，表的数据放在系统共享表空间，也就是跟数据字典放在一起；ON 表示的是，每个 InnoDB 表数据存储在一个以 .ibd 为后缀的文件中。从 MySQL 5.6.6 版本开始，它的默认值就是 ON 了。 比如要删除ID=5的对应的数据(称为R1)，InnoDB 引擎只会把R1这个记录标记为删除。如果之后要再插入一个ID在4和6之间的记录时，可能会复用这个位置。但是，磁盘文件的大小并不会缩小。InnoDB 的数据是按页存储的，如果删除一个数据页上所有的记录，整个数据页就可以被复用了。 数据页的复用跟记录的复用是不同的 记录的复用，只限于符合范围条件的数据。比如上面的这个例子，R1 这条记录被删除后，如果插入一个 ID 是 400 的行，可以直接复用这个空间。但如果插入的是一个 ID 是 800 的行，就不能复用这个位置了。 而当整个页从 B+ 树里面摘掉以后，可以复用到任何位置。如果将数据页 page A 上的所有记录删除以后，page A 会被标记为可复用。这时候如果要插入一条 ID=50 的记录需要使用新页的时候，page A 是可以被复用的。 如果相邻的两个数据页利用率都很小，系统就会把这两个页上的数据合到其中一个页上，另外一个数据页就被标记为可复用。 进一步地，如果我们用 delete 命令把整个表的数据删除呢？结果就是，所有的数据页都会被标记为可复用。但是磁盘上，文件不会变小。 delete 命令其实只是把记录的位置，或者数据页标记为了“可复用”，但磁盘文件的大小是不会变的。也就是说，通过 delete 命令是不能回收表空间的。这些可以复用，而没有被使用的空间，看起来就像是“空洞”。 不止是删除数据会造成空洞，插入数据也会。如果数据是按照索引递增顺序插入的，那么索引是紧凑的。但如果数据是随机插入的，就可能造成索引的数据页分裂。 比如在乱序的时候插入ID=500的时候，然后就会去申请新的页面page B来保存数据了， 经过大量增删改的表，都是可能是存在空洞的。所以，如果能够把这些空洞去掉，就能达到收缩表空间的目的。 而重建表，就可以达到这样的目的。 可以使用 alter table A engine=InnoDB 命令来重建表。在 MySQL 5.5 版本之前，这个命令的执行流程跟我们前面描述的差不多，区别只是这个临时表 B 不需要你自己创建，MySQL 会自动完成转存数据、交换表名、删除旧表的操作。(如果在这个过程中，有新的数据要写入到表 A 的话，就会造成数据丢失。因此，在整个 DDL 过程中，表 A 中不能有更新。也就是说，这个 DDL 不是 Online 的。) MySQL 5.6 版本开始引入的 Online DDL，对这个操作流程做了优化。 重建表的流程： 建立一个临时文件，扫描表 A 主键的所有数据页； 用数据页中表 A 的记录生成 B+ 树，存储到临时文件中； 生成临时文件的过程中，将所有对 A 的操作记录在一个日志文件（row log）中，对应的是图中 state2 的状态； 临时文件生成后，将日志文件中的操作应用到临时文件，得到一个逻辑数据上与表 A 相同的数据文件，对应的就是图中 state3 的状态； 用临时文件替换表 A 的数据文件。","link":"/2019/12/17/%E6%95%B0%E6%8D%AE%E5%88%A0%E6%8E%89%E4%B8%80%E5%8D%8A%EF%BC%8C%E8%A1%A8%E7%9A%84%E5%A4%A7%E5%B0%8F%E4%B8%8D%E5%8F%98/"},{"title":"由一行代码引发的变量分配思考","text":"整个包都只有一行有效代码，或许是一件值得思考的事情 闲逛GitHub的时候发现 Brad Fitzpatrick的iter包。仔细看了2遍。代码里确实只有一行有效代码 123func N(n int) []struct{} { return make([]struct{}, n)} 刚开始也是一扫而过，然后看了看注释 1It does not cause any allocations. 既然有这么多star还有几乎没提issue，我首先假定了他的注释是对的。立马想到空结构体 struct{} 是不占据空间的，典型的在写代码的时候，会经常这么写来判断某些值是否在之前出现过 1m := make(map[string]struct{}, 0) 以及 空结构体的切片只占用切片头的空间。 但是关于切片的印象是占据24个字节，在64位机器上 123var a []intfmt.Println(unsafe.Sizeof(a))// 这里会打印出来24 所以是否作者写的是错的，为什么说 函数 N 不会引发分配呢？ 为了解决这个疑惑，需要先弄清楚两个问题： 一个 Go 变量可能会被分配在哪里？ 如何确定一个 Go 变量最终会被分配在哪里？ 变量的分配 图片来自 这里 图 6-1 初始化的全局变量或静态变量，会被分配在 Data 段。 未初始化的全局变量或静态变量，会被分配在 BSS 段。 在函数中定义的局部变量，会被分配在堆（Heap 段）或栈（Stack 段）。 实际上，如果考虑到 编译器优化，局部变量还可能会被 分配在寄存器，或者直接被 优化去掉。 Go 内存分配 堆（heap） 由 GC 负责回收。 对应于进程地址空间的堆。 栈（stack） 不涉及 GC 操作。 每个 goroutine 都有自己的栈，初始时被分配在进程地址空间的栈上，扩容时被分配在进程地址空间的堆上。 Go 变量主要分为两种： 全局变量 会被 Go 编译器标记为一些特殊的 符号类型，分配在堆上还是栈上目前尚不清楚，不过不是本文讨论的重点。 局部变量 所以综上，对于在函数中定义的 Go 局部变量：要么被分配在堆上，要么被分配在栈上。 确定 Go 变量最终的分配位置按照官方 FAQ How do I know whether a variable is allocated on the heap or the stack? 的解释： Go 编译器会尽可能将变量分配在栈上 以下两种情况，Go 编译器会将变量分配在堆上 如果一个变量被取地址（has its address taken），并且被逃逸分析（escape analysis）识别为 “逃逸到堆”（escapes to heap） 如果一个变量很大（very large） 逃逸分析1234567package mainimport &quot;github.com/bradfitz/iter&quot;func main() { for range iter.N(4) {}} 12345678910go run -gcflags='-m -m' main.go# command-line-arguments./main.go:5:6: can inline main with cost 7 as: func() { for loop }./main.go:6:18: inlining call to iter.N./main.go:6:18: make([]struct {}, iter.n) escapes to heap:./main.go:6:18: flow: {heap} = &amp;{storage for make([]struct {}, iter.n)}:./main.go:6:18: from make([]struct {}, iter.n) (non-constant size) at ./main.go:6:18./main.go:6:18: make([]struct {}, iter.n) escapes to heap 按照前面的分析，从 “make([]struct {}, iter.n) escapes to heap” 的信息，推断：make([]struct {}, iter.n) 会被分配在堆上。到这里，最初的疑惑似乎已经有了答案：make([]struct {}, iter.n) 一定会引发堆分配，那是 Brad Fitzpatrick 的注释写错了吗？ 内存分配器追踪除了逃逸分析，Go 还提供了一种叫内存分配器追踪（Memory Allocator Trace）的方法，用于细粒度地分析由程序引发的所有堆分配（和释放）操作： 1GODEBUG=allocfreetrace=1 go run main.go 2&gt;&amp;1 | grep -C 10 因为进行内存分配器追踪时，很多由 runtime 引发的分配信息也会被打印出来，所以用 grep 进行过滤，只显示由用户代码（user code）引发的分配信息。然而这里的输出结果为空，表明 make([]struct {}, iter.n) 没有引发任何堆分配。内存分配器追踪的结论与逃逸分析的结论截然相反！那到底哪个结论是对的呢？ 汇编分析黔驴技穷之际，Go’s Memory Allocator - Overview 这篇文章给了提示：So, we know that i is going to be allocated on the heap. But how does the runtime set that up? With the compiler’s help! We can get an idea from reading the generated assembly. 1234567891011go tool compile -N -l -S main.go0x0014 00020 (escape/p10/main.go:8) MOVQ AX, main.n+88(SP)0x0019 00025 (escape/p10/main.go:8) MOVQ $0, main.~r0+24(SP)0x0022 00034 (escape/p10/main.go:8) MOVUPS X15, main.~r0+32(SP)0x0028 00040 (escape/p10/main.go:9) MOVQ main.n+88(SP), CX0x002d 00045 (escape/p10/main.go:9) MOVQ main.n+88(SP), BX0x0032 00050 (escape/p10/main.go:9) LEAQ type:struct {}(SB), AX0x0039 00057 (escape/p10/main.go:9) PCDATA $1, $00x0039 00057 (escape/p10/main.go:9) CALL runtime.makeslice(SB) 可以看到，其中有一处对 runtime.makeslice(SB) 的调用，显然是由 make([]struct{}, n) 引发的。 查看 runtime.makeslice 的源码： 12345func makeslice(et *_type, len, cap int) slice { ... p := mallocgc(et.size*uintptr(cap), et, true) return slice{p, len, cap}} 其中，mallocgc 的源码如下： 1234567891011func mallocgc(size uintptr, typ *_type, needzero bool) unsafe.Pointer { ... if size == 0 { return unsafe.Pointer(&amp;zerobase) } ... if debug.allocfreetrace != 0 { tracealloc(x, size, typ) } ...} 结合上述几段源码，可以看出： makeslice 函数中：slice 结构体是 Go 切片 —— array 是指向数组片段的指针，len 是数组片段的长度，cap 是数组片段的最大长度。 makeslice 函数中：array 的值来自 p，而 p 则是一个指针，它指向由 mallocgc 分配得到的底层数组。 mallocgc 函数中：因为空结构体的 size 为 0，所以 mallocgc 并没有实际进行堆分配；由于没有执行到 tracealloc 的地方，所以进行内存分配器追踪时，不会采集到相关的分配信息。 makeslice 函数中：切片 slice 本身是以结构体的形式返回的，所以只会被分配在栈上。 总结经过一系列的探索和分析，至此，可以得出以下结论： make([]struct{}, n) 只会被分配在栈上，而不会被分配在堆上。 Brad Fitzpatrick 的注释是对的，并且他的意思是 “不会引发堆分配”。 逃逸分析识别出 escapes to heap，并不一定就是堆分配，也可能是栈分配。 进行内存分配器追踪时，如果采集不到堆分配信息，那一定只有栈分配。 最后，来解答文章标题提出的疑问 —— 如何确定一个 Go 变量会被分配在哪里？对此： 先对代码作逃逸分析。 如果该变量被识别为 escapes to heap，那么它十有八九是被分配在堆上。 如果该变量被识别为 does not escape，或者没有与之相关的分析结果，那么它一定是被分配在栈上。 如果对 escapes to heap 心存疑惑，就对代码作内存分配器追踪。 如果有采集到与该变量相关的分配信息，那么它一定是被分配在堆上。 否则，该变量一定是被分配在栈上。 此外，如果想知道 Go 编译器是如何将变量分配在堆上或者栈上的，可以去分析 Go 汇编（以及 runtime 源码）。 相关阅读 The empty struct Go Slices: usage and internals Escape analysis Go’s Memory Allocator - Overview Go internals, Chapter 1: Go assembly Five things that make Go fast","link":"/2022/04/05/%E7%94%B1%E4%B8%80%E8%A1%8C%E4%BB%A3%E7%A0%81%E5%BC%95%E5%8F%91%E7%9A%84%E5%8F%98%E9%87%8F%E5%88%86%E9%85%8D%E6%80%9D%E8%80%83/"},{"title":"给字符串加索引","text":"MySQL是支持前缀索引的，前缀索引的优势就是占用的空间小，这同时带来的损失是，可能会增加额外的记录扫描次数。 比如一些用户表，登录账户是邮箱 如果要使用的是邮箱登录，所以代码中一定会有这种类似的语句 1select f1, f2 from tableName where email='xxx'; 如果email这个字段上没有索引的话，那这些语句就只能做全表扫描 MySQL 是支持前缀索引的，可以定义字符串的一部分作为索引。默认地，如果你创建索引的语句不指定前缀长度，那么索引就会包含整个字符串。 比如，这两个在 email 字段上创建索引的语句： 12alter table t add index index1(email);alter table t add index index2(email(6)); 第一个语句创建的 index1 索引里面，包含了每个记录的整个字符串； 而第二个语句创建的 index2 索引里面，对于每个记录都是只取前 6 个字节。 其中email(6)这个索引结构中每个邮箱字段只取前6个字节，占用的空间会比较小，这是使用前缀索引的优势,但是带来的损失可能会增加额外的记录扫描次数 看看下面这个语句 1select id,name,email from SUser where email='zhangssxyz@xxx.com'; 如果使用的是这种索引index1（即 email 整个字符串的索引结构），执行顺序是这样的： 从 index1 索引树找到满足索引值是’zhangssxyz@xxx.com’的这条记录，取得 ID2 的值； 到主键上查到主键值是 ID2 的行，判断 email 的值是正确的，将这行记录加入结果集； 取 index1 索引树上刚刚查到的位置的下一条记录，发现已经不满足 email=‘zhangssxyz@xxx.com’的条件了，循环结束。 这个过程中，只需要回主键索引取一次数据，所以系统认为只扫描了一行。 如果使用的是 index2（即 email(6) 索引结构），执行顺序是这样的： 从 index2 索引树找到满足索引值是’zhangs’的记录，找到的第一个是 ID1； 到主键上查到主键值是 ID1 的行，判断出 email 的值不是’zhangssxyz@xxx.com’，这行记录丢弃； 取 index2 上刚刚查到的位置的下一条记录，发现仍然是’zhangs’，取出 ID2，再到 ID 索引上取整行然后判断，这次值对了，将这行记录加入结果集； 重复上一步，直到在 idxe2 上取到的值不是’zhangs’时，循环结束。 在这个过程中，要回主键索引取 4 次数据，也就是扫描了 4 行。 所以使用前缀索引有可能会使查询语句读数据的次数变多 使用前缀索引，定义好长度，就可以做到既节省空间，又不用额外增加太多的查询成本。 如果我们能够确定业务需求里面只有按照身份证进行等值查询的需求，这种方法，既可以占用更小的空间，也能达到相同的查询效率。 有以下2中方式 就是使用倒序存储，比如身份证倒序，查询的时候再用函数转一下 以及使用hash字段，在表上创建一个整数字段，来保存身份证的校验码，同时在这个字段上加索引 这两种方式对比区别 从占用的额外空间来看，倒序存储方式在主键索引上，不会消耗额外的存储空间，而 hash 字段方法需要增加一个字段。当然，倒序存储方式使用 4 个字节的前缀长度应该是不够的，如果再长一点，这个消耗跟额外这个 hash 字段也差不多抵消了。 在 CPU 消耗方面，倒序方式每次写和读的时候，都需要额外调用一次 reverse 函数，而 hash 字段的方式需要额外调用一次 crc32() 函数。如果只从这两个函数的计算复杂度来看的话，reverse 函数额外消耗的 CPU 资源会更小些。 从查询效率上看，使用 hash 字段方式的查询性能相对更稳定一些。因为 crc32 算出来的值虽然有冲突的概率，但是概率非常小，可以认为每次查询的平均扫描行数接近 1。而倒序存储方式毕竟还是用的前缀索引的方式，也就是说还是会增加扫描行数。","link":"/2020/02/11/%E7%BB%99%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%8A%A0%E7%B4%A2%E5%BC%95/"},{"title":"深入理解高可扩展性及其在 Azure 中的实现","text":"高可扩展性的技术定义与商业价值在云计算体系架构中，高可扩展性（High Scalability）本质上是一种弹性工程能力，表现为系统通过智能化的资源编排机制，实现计算、存储、网络等基础资源与业务负载的动态匹配。其核心诉求始终如一：通过纵向扩容（Scale Up）或横向拓容（Scale Out）的灵活组合，构建具备非线性增长能力的数字基础设施，既能在流量脉冲场景下实现毫秒级资源弹性供给，又能在业务低峰期自动回收冗余资源，最终达成服务稳定性与成本效率的黄金平衡。 这种能力对现代商业生态具有颠覆性意义：当在线教育平台遭遇百万用户同时接入的直播需求时，高可扩展性系统能像变形金刚般重构资源形态，通过自动扩容GPU算力集群确保4K视频流的实时编解码；而当流量回落时，又能智能释放闲置资源，将云成本曲线精准贴合业务真实需求。 架构演进的二元法则：垂直与水平扩展的博弈云原生架构设计中，垂直扩展（Scale Up）与水平扩展（Scale Out）构成弹性能力的双重引擎。这两种策略犹如航天器的多级推进系统，在不同飞行阶段发挥着不可替代的作用。 垂直扩展（Scale Up）：单体能力的极限突破垂直扩展本质上是纵向扩容的技术演绎，通过升级单台服务器的硬件规格实现性能跃迁。这类似于为超级计算机安装最新一代的量子芯片——当业务遇到单节点性能瓶颈时，通过动态调整虚拟机规格（如AWS EC2的实例类型升级），将CPU从16核扩展到64核，内存从64GB扩容至1TB，存储从HDD替换为NVMe SSD阵列，瞬间突破物理限制。这种”单体强化”模式尤其适用于Oracle数据库等对纵向扩展友好的传统系统，但需警惕硬件天花板的客观存在。 垂直扩展的优势与局限性分析：优势特性： 实施便捷性通过硬件升级方式实现性能提升，无需重构系统架构。例如增加CPU核数、扩展内存容量或升级存储设备等操作，通常可在不改变应用逻辑的情况下快速完成。 运维经济性单一节点的管理模型显著降低运维复杂度，避免了分布式系统常见的网络延迟、数据一致性校验等管理负担，人力成本和运维工具投入相对可控。 资源聚合优势集中式资源配置消除了分布式架构的通信开销，可实现内存级数据交互效率，特别适合对延迟敏感的实时事务处理场景。 发展限制： 物理性能边界受半导体工艺和硬件设计限制，单个服务器的处理能力存在理论极限。当业务规模突破TB级数据处理或百万级并发时，纵向升级将难以满足需求。 边际成本激增高端企业级硬件呈现非线性定价特征，如从128核CPU升级至256核的成本增幅远超性能提升比例，这种”性能溢价”现象在存储扩展时尤为显著。 系统脆弱性单点架构的故障域覆盖整个系统，硬件故障、电力中断或网络抖动等异常情况都可能导致完全服务中断，难以满足现代业务对99.99%以上可用性的要求。 适用场景: 垂直扩展适用于系统负载增长可预测，初期规模较小，对高可用性要求不极端的场景。例如，小型企业内部应用、开发测试环境等。 2.2 水平扩展（Scaling Out）：分布式架构的协同进化 水平扩展（横向扩展）是通过动态增加服务器节点构建分布式集群，将业务负载分摊到多个计算单元的技术策略。这种架构如同蜂群协作，每个节点既是独立的工作单元，又能通过协同机制形成规模化的系统能力。 核心优势 弹性计算资源池：基于流量波动实时增减节点，结合云计算的弹性计费模式，实现”按秒扩容、闲置即停”的动态资源调度，避免硬件资源闲置浪费。 经济性线性增长：采用标准化商用服务器构建集群，通过横向叠加节点获得近似线性的性能提升。相较于垂直扩展的超算级硬件投入，总体拥有成本（TCO）降低40%-60%。 故障域隔离设计：通过多可用区部署和智能流量调度，单节点故障仅影响局部服务。结合健康检查与自动故障转移，系统可用性可达99.999%（全年停机时间＜5分钟）。 技术挑战 分布式系统熵增：节点规模扩大导致通信复杂度呈指数级增长，需引入服务网格（Service Mesh）、分布式追踪等观测性工具，并设计熔断/降级/限流等稳定性模式。 数据一致性困境：在CAP定理约束下，需权衡强一致性（如Paxos协议）与高可用性（如Gossip协议）。金融级交易系统往往采用异步补偿机制，而社交类应用倾向最终一致性。 资源调度博弈：Kubernetes等编排系统需平衡节点亲和性、资源碎片率、冷启动延迟等多维参数，在大规模集群中可能产生调度震荡现象。 典型应用图谱 该架构特别适用于流量存在潮汐效应的互联网服务： 电商秒杀系统：通过预置弹性容量池应对瞬时百倍流量激增，结合边缘计算节点实现地域化分流 实时协作平台：如在线文档编辑场景，采用CRDT无冲突数据类型保证多地写作一致性 IoT数据处理：应对百万级设备并发连接，采用分片（Sharding）机制将设备组映射到不同计算节点 AI推理服务：通过模型并行化技术将大语言模型拆分到GPU集群，实现请求吞吐量弹性扩展 这种架构演化出云原生技术栈，通过容器化、服务网格、声明式API等技术抽象，正在重塑现代软件系统的构建范式。 负载均衡：系统扩展的基石引擎 在分布式架构中，负载均衡技术如同智能调度中枢，通过高效分配请求流量实现系统弹性扩展。无论是垂直扩展的硬件升级，还是水平扩展的服务器集群，负载均衡器都扮演着关键角色，尤其在横向扩展架构中更是核心组件。这类智能调度系统通过多维度算法，将海量用户请求精准路由到最优服务器节点，既防止单点过载风险，又最大化集群处理能力，从而系统性提升资源利用率、服务可用性和业务连续性。 主流负载均衡算法解析： 轮询调度（Round Robin）基础而高效的均衡算法，采用循环机制将请求序列化分配到服务器池，保证各节点获得均等处理机会。适用于硬件配置统一、服务类型标准化的业务场景。 最小连接优先（Least Connections）实时监控服务器活跃连接数的智能算法，动态将新请求导向当前负载最轻的节点。特别适合处理HTTP长连接、实时通信等连接持续时间差异较大的服务场景。 动态响应权重（Adaptive Weighting）基于服务质量反馈的智能算法，通过持续监测服务器响应时间、错误率等指标，自动调整流量分配权重。响应速度越快的节点获得更高流量占比，实现系统性能的自我优化。 地理路由（Geo-Routing）结合用户位置数据的全局调度策略，通过IP地理定位将请求自动引导至最近可用区，有效降低网络延迟。对于跨国部署的电商平台、流媒体服务等具有显著体验提升。 在微软Azure云平台中，智能负载均衡服务提供企业级流量管理方案。其支持四层/七层负载均衡、自动健康检查、会话保持等进阶功能，结合混合云部署能力，可灵活配置基于业务指标的动态调度策略。通过深度集成云监控与自动化伸缩组件，形成完整的弹性架构闭环，助力企业构建智能感知、动态调优的现代化应用架构。 构建弹性系统的高可扩展性实践实现真正的高可扩展性系统需要将弹性思维贯穿于系统全生命周期，通过架构革新、智能调度与持续优化，构建具备自适应性的大规模分布式体系。 弹性架构设计范式 功能模块化解耦：将系统分解为独立的功能模块，例如电商平台拆分为用户管理、商品目录、订单处理等组件。各模块通过标准化接口通信，支持独立迭代与弹性伸缩，避免单体架构的扩展瓶颈。分布式服务治理：通过微服务架构或容器化技术将应用拆分为自治服务单元，各服务可独立部署和扩展。例如利用Docker容器封装服务组件，通过Kubernetes实现动态编排和跨集群调度。数据平面扩展：采用分库分表策略突破存储瓶颈，按用户ID哈希值等业务特征进行数据分片，实现MySQL集群的水平扩展。结合NewSQL数据库实现分布式事务处理，保障数据一致性。智能缓存体系：构建多级缓存体系，使用Redis集群缓存热点商品详情，本地缓存存储会话数据，CDN节点缓存静态资源。通过缓存预热和淘汰策略优化，将核心业务缓存命中率提升至90%以上。 智能化弹性调控 全链路可观测体系：建立涵盖基础设施到应用层的立体化监控体系，通过Prometheus采集容器指标，SkyWalking追踪微服务调用链，ELK聚合业务日志。实时监控QPS、P99时延、错误率等黄金指标，结合时序预测算法预判扩展需求。动态资源编排：基于实时指标设定弹性扩缩容规则，例如当CPU利用率持续5分钟&gt;75%触发ECS实例扩容，并发连接数激增时自动扩展SLB节点。通过Kubernetes HPA实现Pod级别的细粒度弹性调度。成本感知调度：采用混合计费模式优化云资源成本，高峰期使用按需实例应对突发流量，日常负载使用预留实例降低成本。通过资源画像分析，对低利用率实例实施自动关机策略，典型场景可节约30%计算成本。自适应流量治理：集成服务熔断（Hystrix）、流量整形（Sentinel）等机制，在系统过载时自动启动限流降级策略。通过蓝绿发布和金丝雀发布实现无损扩缩容，保障业务连续性。 Azure云原生架构的可扩展性实践在Azure云平台中，可扩展性已从技术愿景转化为原生能力。通过深度整合的云服务和智能工具链，Azure将弹性基因注入企业应用的每一处架构设计，助力构建具备自适应性扩展能力的现代化系统。 智能弹性计算集群（Azure Scale Sets）作为IaaS层的核心扩展组件，Azure虚拟机规模集通过智能编排引擎实现了计算资源的动态平衡。该服务支持创建同构虚拟机集群，并基于多维指标（CPU、内存、网络吞吐量等）进行预测性扩缩容。例如在电商大促场景中，可设置当HTTP请求队列长度超过500时自动触发横向扩展，配合预热脚本实现业务无感知的容量扩充，确保系统在流量洪峰下维持稳定SLA。 容器化扩展编排（Azure Kubernetes Service） AKS作为全托管Kubernetes服务，通过声明式API重构了容器化应用的扩展范式。其内置的Cluster Autoscaler与Horizontal Pod Autoscaler形成双层扩展机制：前者动态调整节点池规模，后者精细化控制Pod副本数量。某金融客户通过配置自定义指标（如每秒交易事务数），实现支付系统在业务高峰期自动扩展至300+Pod，闲时自动收缩至基准规模，资源利用率提升40%。 服务网格化治理（Azure Service Mesh） 基于Istio增强的Azure服务网格为微服务架构注入了智能流量管控能力。通过非侵入式Sidecar代理，实现动态流量分配（如蓝绿部署时自动分流5%流量至新版本）、自适应熔断（当服务错误率超过阈值自动隔离故障节点）等高级特性。某制造企业借助该服务，在全球化部署中实现跨区域流量的智能路由，将亚太区订单处理系统的响应延迟降低至200ms以内。 无服务器事件驱动架构（Azure Functions） Azure Functions突破传统计算范式，通过毫秒级伸缩粒度和事件驱动模型重构扩展边界。其扩展策略可智能匹配事件源特性：针对IoT设备流数据采用并发驱动扩展，面对批量数据处理任务则启用Durable Functions实现工作流编排。某智慧城市项目通过函数计算处理百万级传感器事件，在突发流量下自动扩展至2000个并行实例，事件处理吞吐量达5万TPS，且仅按实际执行计费。 拥抱高可扩展性，驾驭云端未来云计算架构的高可扩展性作为数字时代企业竞争力的核心支撑，正在重新定义业务发展的技术边界。通过采用垂直扩展与水平扩展的混合部署策略，结合智能负载均衡机制的动态调配，配合Azure云平台的全栈式弹性服务矩阵（如Azure Autoscale、Azure Load Balancer等），企业能够打造出智能感知业务压力、自动调节资源配置的云原生架构体系。这种具备预测性扩展能力的云架构不仅实现了成本效益与性能指标的动态平衡，更通过Azure Kubernetes Service的容器化编排、Azure Functions的无服务器架构等PaaS层创新，将传统被动响应式的扩展模式转变为主动预判型的智能弹性机制，为业务连续性构筑起具备自我修复能力的数字化基座。","link":"/2025/02/19/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E9%AB%98%E5%8F%AF%E6%89%A9%E5%B1%95%E6%80%A7%E5%8F%8A%E5%85%B6%E5%9C%A8%20Azure%20%E4%B8%AD%E7%9A%84%E5%AE%9E%E7%8E%B0/"},{"title":"python钉钉机器人自定义回复","text":"大概有这样的需求 达到xxx条件。比如是到了某个时刻。机器人自动在群里通知，并@相关的人 比如在群里回复一个关键词、然后期望得到想要的信息，以达到解放双手的目的。这一块儿就需要自己对接钉钉API来实现了。比如这种问答式的 明白这样的需求后就去 官网 找文档去了。 其实官网文档说的很清楚了。没啥需要补充的。 分析 需要部署到服务器 除了定时还需要自定义回复 不需要性能太高，简单便捷就好。所以在go和Python之间选择了Python 使用Flash启动 主要逻辑代码 12345678910111213from flask import Flask # http://flask.pocoo.org/docs/0.12/api/#flask.Flaskapp = Flask(__name__) @app.route('/HelloWorld')def hello_world(): return &quot;Hello World!&quot; if __name__ == &quot;__main__&quot;: # http://flask.pocoo.org/docs/0.12/quickstart/#a-minimal-application app.run(host='0.0.0.0', port='5000') 用python开启flask web服务时， 你只需要本机访问，那ip只要不设置为0.0.0.0就可以，正常访问就好 如果你需要外网访问，ip需要设置为0.0.0.0，此时，在本机上访问需要使用默认的127.0.0.1（也就是你不设置ip时默认的ip）,在外网上访问则需要使用你本机的ip，不要使用0.0.0.0 也可以这样的方式来启动 123456789if __name__ == &quot;__main__&quot;: server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server_socket.bind((&quot;&quot;, 8083)) server_socket.listen(120) while True: client_socket, client_address = server_socket.accept() handle_client_process = Process(target=handle_client, args=(client_socket,)) handle_client_process.start() client_socket.close() 然后去实现handle_client 就好了。篇幅有限。完整的代码关注公众号 罗尔街 即可获取 架构图 流程 用户通过django admin来添加消息配置，即关键字与回复内容。 用户通过钉钉企业内部群中@机器人 + 关键字。 企业机器人收到后，由socket监听服务接收，并根据消息类型进行处理后返回。 企业机器人收到返回的消息后，通过内网穿透工具给的外网映射地址进行回复。 企业内部群显示回复的消息，用户看见回复的消息。 实践 针对定时、或者达到xxx条件触发的，在智能群助手里面添加机器人就好了 选择 自定义机器人 然后配置其中一种安全方式即可 得到webhook地址。 也可以先构建一个curl来测试一下 123curl 'https://oapi.dingtalk.com/robot/send?access_token=381c2f405e0f906fd556b27cea9f66864120860b5d8b117bb046e10b6599b050&amp;timestamp=1613211530113&amp;secret=SEC2e67120c5e4affa1177ac25fe8dc77ba1c5b49284a9dc7e1888770bc3b76b1fc' \\ -H 'Content-Type: application/json' \\ -d '{&quot;msgtype&quot;: &quot;text&quot;,&quot;text&quot;: {&quot;content&quot;: &quot;test&quot;}}' 主要逻辑代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556import base64import hashlibimport hmacimport jsonimport timeimport urllibimport urllib.parseimport requestsdef sign(): timestamp = str(round(time.time() * 1000)) secret = &quot;SECe5d17dac6060b76c01ea60aec260fe76c6e0644394b932bfffa963342bb630a1&quot; secret_enc = secret.encode('utf-8') string_to_sign = '{}\\n{}'.format(timestamp, secret) string_to_sign_enc = string_to_sign.encode('utf-8') hmac_code = hmac.new(secret_enc, string_to_sign_enc, digestmod=hashlib.sha256).digest() sign_res = urllib.parse.quote_plus(base64.b64encode(hmac_code)) return timestamp, sign_resdef send_ding_message(text_info): webhook = &quot;https://oapi.dingtalk.com/robot/send?access_token=f069339ad1fcb9410d0e96fd947d9a2bf3416451d01dc97e3ef4256c1fdb2b7a&quot; header = { &quot;Content-Type&quot;: &quot;application/json&quot;, &quot;Charset&quot;: &quot;UTF-8&quot; } text = text_info message = { &quot;msgtype&quot;: &quot;text&quot;, &quot;text&quot;: { &quot;content&quot;: text }, &quot;at&quot;: { # &quot;isAtAll&quot;: False # &quot;atMobiles&quot;: [ # &quot;18512345678&quot; # ] &quot;atUserIds&quot;: [ &quot;lbkgv8q&quot; ] } } message_json = json.dumps(message) timestamp, sign_res = sign() webhook += &quot;&amp;timestamp=&quot; + timestamp + &quot;&amp;sign=&quot; + sign_res info = requests.post(url=webhook, data=message_json, headers=header)def usage(): send_ding_message('test 1')if __name__ == &quot;__main__&quot;: usage() 效果 针对自定义回复消息的官方链接 按照链接先添加就好了 既然是需要钉钉回调我们服务的 ，所以要校验token、sign的合法性 1234timestamp = request.headers.get('Timestamp')sign = request.headers.get('Sign')if sign(timestamp) == sign: 生成规则，官方给了示例 123456789def sign(): timestamp = str(round(time.time() * 1000)) secret = &quot;SECe5d17dac6060b76c01ea60aec260fe76c6e0644394b932bfffa963342bb630a1&quot; secret_enc = secret.encode('utf-8') string_to_sign = '{}\\n{}'.format(timestamp, secret) string_to_sign_enc = string_to_sign.encode('utf-8') hmac_code = hmac.new(secret_enc, string_to_sign_enc, digestmod=hashlib.sha256).digest() sign_res = urllib.parse.quote_plus(base64.b64encode(hmac_code)) return timestamp, sign_res 当然也可以用ip段来做安全的校验 如果是企业内部网络、并非是公网，则需要用内网穿透。官方也给了示例，这里就不再重复了。 一些可能会遇到的坑 在使用Python的Flask时、报错 “POST / HTTP/1.1” 405 - 可能是没有允许请求本身的方法 解决方法: 添加method @app.route(‘/‘, methods=[‘GET’, ‘POST’]) 钉钉机器人的webhook是固定的、如果是多个群想用同一个机器人、则需要用app_secret来解决","link":"/2019/07/11/%E9%92%89%E9%92%89%E6%9C%BA%E5%99%A8%E4%BA%BA%E8%87%AA%E5%AE%9A%E4%B9%89%E5%9B%9E%E5%A4%8D/"},{"title":"高阶函数编程技巧","text":"函数是 Go 语言的一等公民，如何利用好其高级用法特性，是一件值得思考和实践的事情 背景在日常业务开发中，对于一些表的不同字段做筛选查询，是基础的功能。而且大部分可能是在根据不同条件去查询。就像这样 123456type XXXRepo interface { GetXXXByIdOrName(ctx context.Context, id int, name string) (o []admin.XXX, err error) GetXXXInfoList(ctx context.Context, req *GetXXXRequest) (total int64, o []admin.XXX, err error) GetXXXInfo(ctx context.Context, columnId, gradeId int) (o []admin.XXX, err error) GetXXXByIdList(ctx context.Context, idList []int) (o []admin.XXX, err error)} 这也还只是少许的一些条件，如果一张表有十多个字段配合查询呢 ？ dao层也会有非常多的冗余代码，可能也就改变了一下入参而已。 假设有一张订单表，简化结构如下 12345678CREATE TABLE `order` ( `id` bigint unsigned NOT NULL AUTO_INCREMENT COMMENT '自增主键', `order_id` bigint NOT NULL COMMENT '订单id', `shop_id` varchar NOT NULL COMMENT '店铺id', `product_id` int NOT NULL DEFAULT '0' COMMENT '商品id', `status` int NOT NULL DEFAULT '0' COMMENT '状态', PRIMARY KEY (`id`),) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='订单表'; 举例说明用以下这些字段的不同组合来查询 1order_id, shop_id, produce_id,status 会在dao层来编写类似于这样的代码 根据orderId来查询 12345678func GetOrderInfoByOrderId(ctx context.Context, orderId int64) ([]*resource.Order) { db := GetDB(ctx) db = db.Table(resource.Order{}.TableName()) var infos []*resource.Order db = db.Where(&quot;order_id = ?&quot;, orderId) db.Find(&amp;infos) return infos} 根据shopId来查询 12345678func GetOrderInfoByShopId(ctx context.Context, shopId int64) ([]*resource.Order) { db := GetDB(ctx) db = db.Table(resource.Order{}.TableName()) var infos []*resource.Order db = db.Where(&quot;shop_id = ?&quot;, shopId) db.Find(&amp;infos) return infos} 可以看到，两个方法的代码极度相似，除了入参和命名不一样，如果再需要按照 produce_id 或者 status 查询，那需要再写几个类似的方法，导致相似的方法非常多。当然很容易想到，如果参数是传多个，传多个不就好了，可能就是这样的写法 12345678func GetOrderInfo(ctx context.Context,orderId, shopId int64) ([]*resource.Order) { db := GetDB(ctx) db = db.Table(resource.Order{}.TableName()) var infos []*resource.Order db = db.Where(&quot;shop_id = ? and order_id = ?&quot;, shopId,orderId) db.Find(&amp;infos) return infos} 如果什么时候业务有变化，需要改条件。也许就会变为这样 12345678910111213func GetOrderInfo(ctx context.Context,orderId, shopId int64) ([]*resource.Order) { db := GetDB(ctx) db = db.Table(resource.Order{}.TableName()) var infos []*resource.Order if orderId != 0 { db = db.Where(&quot;order_id = ?&quot;,orderId) } if shopId != 0 { db = db.Where(&quot;shop_id = ?&quot;,shopId) } db.Find(&amp;infos) return infos} 调用方的代码大概是这样的 12345// 根据shopId 查询infos := GetOrderInfo(ctx, 0, 1)// 根据orderId 查询infos := GetOrderInfo(ctx, 1, 0) 相当于其他不关心的查询字段用对应类型默认的零值来替换了。 当然也可以用结构体来作为一个参数 123456func GetOrderInfo(ctx context.Context,order Order) ([]*resource.Order) { db := GetDB(ctx) db = db.Table(resource.Order{}.TableName()) db.Where(&amp;order).find(&amp;infos) return infos} 但是估计有的人遇到过这样的坑，那就是如果当字段是int,int64等，有0时，不清楚到底是传入了0，还是没有传值，是区分不了的。因为go语言默认的类型零值。如果是类型的0值也想作为参数来查询，则默认是忽略的，可以参考 gorm 官方有这样一句话 1NOTE When querying with struct, GORM will only query with non-zero fields, that means if your field’s value is 0, '', false or other zero values, it won’t be used to build query conditions, for example: 针对这种情况可以选择转化为map或者像以下这种方式来判断 123456789101112131415func GetOrderInfoInfo(ctx context.Context, o Order) ([]*resource.Order) { db := GetDB(ctx) db = db.Table(resource.Order{}.TableName()) var infos []*resource.Order if o.orderId &gt; 0 { db = db.Where(&quot;order_id = ?&quot;, o.orderId) } if o.shopId != &quot;&quot; { db = db.Where(&quot;shop_id = ?&quot;, o.shop_id) } // 后面就先省略了 if xxxx db.Find(&amp;infos) return infos} 这里还只是简短几个字段，如果是十几个字段来组合查询，则要写非常多if判断。 基于以上这种所有情况，有必要来优化一下 可以利用函数式编程来优化 定义如下 1type Option func(*gorm.DB) 定义 Option 是一个函数，这个函数的入参类型是*gorm.DB，返回值为空。 然后针对 表中需要筛选查询的字段定义一个函数，赋值 123456789101112func OrderID(orderID int64) Option { return func(db *gorm.DB) { db.Where(&quot;`order_id` = ?&quot;, userID) }}func ShopID(shopID int64) Option { return func(db *gorm.DB) { db.Where(&quot;`shop_id` = ?&quot;, shopID) }} 所以需要为可能得字段来创建不同的函数，返回一个Option函数，该函数是把入参赋值给【db *gorm.DB】对象 所以基于以上，要改写dao层就很方便了。 12345678910func GetOrderInfo(ctx context.Context, options ...func(option *gorm.DB)) ([]*resource.OrderInfo) { db := GetDB(ctx) db = db.Table(resource.OrderInfo{}.TableName()) for _, option := range options { option(db) } var infos []*resource.OrderInfo db.Find(&amp;infos) return infos} 这样底层的逻辑就不用写很多if判断了，用 for循环来代替 调用者知道自己需要根据什么参数来查询，则就用上面写好的参数函数来作为入参 12345// orderID 查询infos := GetOrderInfo(ctx, OrderID(orderID))// orderID，shopID 组合查询infos := GetOrderInfo(ctx, OrderID(orderID), ShopID(shopID)) 当然还根据其他 in 等条件查询，再写一个函数即可 经过优化之后，简化了逻辑。相当于配置类的Option就生成了，代码优雅了不少。这里只提到了查询，更新也是类似的道理，删除和写入就没太大必要这样了。 参考Self-referential functions and the design of options Using functional options instead of method chaining in Go","link":"/2023/05/05/%E9%AB%98%E9%98%B6%E5%87%BD%E6%95%B0%E7%BC%96%E7%A8%8B%E6%8A%80%E5%B7%A7/"},{"title":"如何利用Azure AI优化大模型推理：TensorRT-LLM与Blackwell平台深度整合","text":"随着生成式人工智能与大语言模型（DeepSeek、GPT、Llama等）加速渗透产业场景，模型推理的高效性、低延迟和成本控制已成为企业落地的关键突破口。微软Azure AI与英伟达Blackwell平台的深度融合为行业带来突破性解决方案，通过整合TensorRT-LLM的量化优化、动态批处理等核心技术，结合Blackwell架构的万亿级参数处理能力，使DeepSeek等百亿参数大模型的推理效率提升达18倍。这种从底层芯片到中间件、云服务的全栈式优化，不仅为Llama-3、Claude等主流模型提供开箱即用的部署方案，更通过Azure AI云平台的弹性算力调度，将大模型应用的边际成本降低47%，真正打通了从算法创新到商业变现的技术闭环。 针对TensorRT-LLM技术原理的详细实现步骤拆解 TensorRT-LLM：大模型推理优化的核心技术量化技术的工程实现流程 实施步骤： 模型预处理阶段 使用SmoothQuant算法对权重矩阵进行白化处理，通过数学变换将激活层方差转移到权重参数： 12W_smooth = W * diag(s)x_smooth = x / s - 执行逐层校准（Per-Channel Calibration），通过FP32推理生成动态范围直方图，确定各通道的缩放因子 混合精度量化部署 对线性层采用INT8量化，激活函数保留FP16精度 通过Quantization-Aware Training微调补偿精度损失 部署时使用TensorRT的IInt8EntropyCalibrator2接口进行最终校准 技术指标： KV缓存使用FP8格式时，每个token仅需0.75MB（原FP16需1.5MB） W4A16配置下，70B模型显存占用从280GB降至78GB 动态批处理的系统架构设计 连续批处理架构： 123456789101112131415161718192021222324pythonCopy Codeclass StreamingBatchProcessor: def __init__(self): self.active_requests = [] # 执行中的请求 self.pending_queue = deque() # 待调度队列 def add_request(self, request): # 动态插入逻辑 if len(self.active_requests) &lt; MAX_GPU_CAPACITY: self._allocate_memory(request) self.active_requests.append(request) else: self.pending_queue.append(request) def _allocate_memory(self, request): # GPU显存预分配策略 request.kv_cache = create_kv_buffer( max_seq_len=4096, num_layers=32, num_heads=16, head_dim=128 ) 分页注意力实现细节： 内存池划分：将显存预分割为4MB的连续块（block） 块映射表维护： 12345678cudaCopy Codestruct PageTable { int block_id; int start_pos; int end_pos; bool is_allocated;}; 按需分配策略： 12345678910pythonCopy Codedef allocate_attention_memory(seq_len): required_blocks = ceil(seq_len * d_model / 4MB) free_blocks = find_contiguous_blocks(required_blocks) if not free_blocks: free_blocks = compact_memory() # 内存碎片整理 mark_blocks_allocated(free_blocks) return build_virtual_address_mapping(free_blocks) 注意力机制优化步骤 GQA实现流程： 123456789101112131415pythonCopy Code# 分组策略（以16头为例）num_groups = 4key_states = repeat_kv(key_states, num_groups) # [bs, 4, seq, 128]value_states = repeat_kv(value_states, num_groups)# 查询重组query_states = query_states.view( batch_size, num_heads // num_groups, # 4 num_groups, # 4 head_dim) KV缓存优化： 采用交错存储模式： 1234cudaCopy Code__device__ float2* kv_cache = ...; // 使用float2类型提高访存效率 缓存压缩算法： 12345678pythonCopy Codedef compress_kv_cache(cache): for layer in cache: # 使用Zigzag编码+霍夫曼压缩 compressed = huffman_encode(zigzag_transform(layer)) layer[:] = pad_to_block_size(compressed) 算子融合技术实现 LayerNorm融合步骤： 123456789101112131415161718192021222324cudaCopy Code__global__ void fused_layernorm_relu( float* input, float* output, float* gamma, float* beta, int N) { extern __shared__ float s_data[]; // 1. 并行计算均值 float mean = block_reduce_sum(input) / N; // 2. 计算方差 float var = block_reduce_sum((input - mean)^2) / N; // 3. 归一化计算 float x_hat = (input - mean) / sqrt(var + 1e-5); // 4. 仿射变换 + ReLU output = max(0, gamma * x_hat + beta);} 图优化策略： 使用ONNX Runtime进行子图模式匹配： 123456789101112pythonCopy Codepatterns = [ (&quot;LayerNorm&quot;, &quot;Add&quot;, &quot;Relu&quot;), # 识别可融合模式 (&quot;MatMul&quot;, &quot;BiasAdd&quot;)]for pattern in patterns: matches = find_subgraph_matches(model, pattern) for match in matches: replace_with_fused_op(model, match, &quot;FusedLN_Add_Relu&quot;) 性能优化数据 显存优化效果： 70B模型显存占用对比： 12345678textCopy Code| 精度模式 | 显存占用 | 相对比例 ||------------|---------|---------|| FP16 | 140GB | 100% || W8A16 | 98GB | 70% || W4A16 | 56GB | 40% | 延迟优化对比（A100测试）： 123456789textCopy Code| 请求长度 | 批处理方式 | 吞吐量(query/s) ||---------|-------------|----------------|| 256 | 静态批处理 | 12.5 || 256 | 连续批处理 | 53.8 || 2048 | 分页注意力 | 9.7 || 2048 | 普通注意力 | 5.2 | Azure AI与Blackwell平台的深度整合微软Azure AI服务与英伟达Blackwell平台的战略级深度整合，构建了从芯片架构到云服务的全栈式AI工程体系。该整合方案通过硬件协同设计、软件中间件优化和云原生服务重构三个维度，实现了AI工作负载的端到端性能突破。 基于Blackwell架构的下一代AI算力集群在硬件基础设施层面，Azure推出全新NDGB200 V6超算级虚拟机系列，采用模块化服务器设计。每个计算节点搭载： 72颗NVIDIA GB200 NVL GPU芯片，通过NVLink-C2C互连技术实现1.8TB/s的超高带宽 双量子级InfiniBand网络加速卡，支持自适应路由和SHARPv3协议，将分布式训练通信开销降低至传统方案的1/5 定制化液冷散热系统，使GPU持续运行在45°C最佳温度区间该架构在千亿参数模型训练中展现突破性表现：当运行70B参数大模型时，跨128节点的线性扩展效率达92%，每美元训练成本较前代H100集群降低40%。 面向2025年AI演进趋势，双方联合规划下一代产品路线： Blackwell Ultra GPU将集成192GB HBM4显存，支持8K上下文窗口的MoE模型 RTX PRO 6000服务器版采用Chiplet设计，单卡提供1.3PetaFLOPS的INT8算力，专攻视频生成与科学计算场景 配套推出BlueField-4 DPU，实现网络/存储/安全功能的硬件卸载 深度重构的AI开发范式在中间件层，NVIDIA NIM微服务与Azure AI Foundry的融合创造了新的开发范式。技术架构包含： 模型优化引擎：集成TensorRT-LLM 5.0编译器，自动实施算子融合、动态张量内存和量化感知训练 服务编排层：基于Kuberflow框架实现多模型流水线编排，支持复杂推理链的DAG可视化配置 效能监控系统：内置100+种健康指标探针，实时追踪模型漂移、显存碎片和计算密度 典型应用案例显示，当部署Meta Llama-3-405B模型时： 通过选择性激活（Selective Activation）技术，将KV缓存压缩率提升至70% 使用动态批处理（Dynamic Batching）策略，吞吐量从1200 tokens/s提升至2100 tokens/s 结合FP8量化，使70B参数模型的推理延迟稳定在85ms以内 智能弹性的云原生服务体系Azure重构了AI云服务的核心组件： 无服务器GPU容器服务（Azure Container Apps）采用革命性的”热池”预调度算法： 基于LSTM的负载预测模型，实现GPU实例的亚秒级唤醒（冷启动&lt;800ms） 细粒度计费系统支持按10秒为单位计量，并引入中断任务续算功能 内置故障转移机制，在硬件异常时可保留95%的显存状态 多模态模型库进行战略性扩展： 新增Mistral Small 3.1架构模型，支持128路并行思维链推理 推出医疗专用版Llama-Nemotron，集成PubMed 4000万篇论文知识图谱 引入CodeFusion-X代码引擎，在Python开发场景实现98%的自动补全准确率 该技术体系已在多个行业落地验证：在放射科诊断场景，Blackwell驱动的3D医学影像模型将病灶检测速度提升6倍；在自动驾驶领域，多模态推理管道使复杂路况决策延迟降至23ms。微软预计，到2025年该架构将支撑超过2000个企业级AI应用的工业化部署。 从模型优化到业务落地（深度技术解析）医疗影像实时诊断系统优化实践 项目背景某头部医疗科技公司基于Azure NDGB200虚拟机（配备8×NVIDIA A100 80GB GPU集群）部署Llama-2-13B模型，用于CT影像的病理特征提取与诊断建议生成。原系统面临两大瓶颈： 单次CT影像（512×512×300体素）推理耗时达2秒 并发处理能力上限为10请求/秒 GPU显存利用率不足40% 核心技术方案 TensorRT-LLM INT8量化优化 采用混合精度量化策略，对Embedding层保留FP16，全连接层执行通道级INT8量化 部署动态范围校准（Dynamic Range Calibration），使用5000张CT影像建立量化校准集 模型体积从48.7GB压缩至12.2GB，实现4倍压缩率 连续批处理优化 实现请求队列的动态优先级调度：急诊病例优先于常规检查 开发自适应批处理调度器，根据显存余量动态调整批尺寸（8-32范围） 引入异步流水线机制，将数据预处理→模型推理→结果解析解耦 显存优化 采用内存池技术预分配15GB显存缓冲区 启用零拷贝数据传输，PCIe带宽利用率提升至92% 实施效果 指标 优化前 优化后 提升倍数 单次推理耗时 2000ms 480ms 4.17x 最大并发量 10 50 5x 日均服务量 2.4万 10万+ 4.16x GPU利用率 38% 89% 2.34x 该方案使三甲医院急诊科的平均诊断响应时间从15分钟缩短至3分钟，并支持DICOM影像的实时流式处理。 案例2：工业数字孪生仿真系统升级 项目背景BlackForest Labs为汽车制造客户构建数字孪生系统，其FLUX模型（基于Transformer的物理仿真网络）原采用FP32精度在Azure NCv3系列虚拟机运行，面临： 单次设备状态仿真耗时8.3秒 多产线并行仿真时显存溢出率达35% 迭代验证周期长达72小时 关键优化路径 FP8量化改造 开发自定义量化感知训练（QAT）流程，保留关键物理参数精度 对反向传播梯度执行8-bit截断，训练收敛速度提升40% 模型显存占用从24GB降至9.6GB 分页注意力优化 实现显存分页管理引擎，动态分配注意力头内存空间 采用LRU缓存淘汰机制，缓存命中率达92% 注意力计算延迟从320ms降至85ms 分布式推理优化 部署NCCL多GPU通信框架，梯度同步耗时降低65% 采用模型并行策略，将FLUX模型分割到4块GPU 实施成效 显存效率：峰值显存占用从37GB降至14.8GB（↓60%） 计算性能：单次仿真耗时从8.3s→2.7s（↑3.07x） 业务价值：某新能源汽车客户产线调试周期从14天缩短至4天，良品率提升2.3个百分点 深度部署指南：以Llama-13B为例 阶段1：环境配置1234567891011121314151617181920212223242526# Azure虚拟机选型VM_TYPE=Standard_ND96amsr_A100_v4GPU_DRIVER_VERSION=535.104.05CUDA_VERSION=12.2TRTLLM_VERSION=0.7.1# 基础环境部署az vm create --name trtllm-inference \\ --resource-group myResourceGroup \\ --image Ubuntu2204 \\ --size $VM_TYPE \\ --accelerated-networking true \\ --admin-username azureuser \\ --generate-ssh-keys# CUDA环境安装sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/3bf863cc.pubsudo apt-get -y install cuda-toolkit-12-2 libcudnn8=8.9.4.*-1+cuda12.2# TensorRT-LLM编译安装git clone -b v0.7.1 https://github.com/NVIDIA/TensorRT-LLM.gitcd TensorRT-LLM &amp;&amp; mkdir build &amp;&amp; cd buildcmake .. -DTRTLLM_VERSION=${TRTLLM_VERSION} \\ -DCMAKE_CUDA_ARCHITECTURES=&quot;80;90&quot; \\ -DSKIP_MPI=ONmake -j$(nproc) 阶段2：模型转换优化123456789101112131415161718192021222324252627282930313233343536373839404142from tensorrt_llm import Builder, NetworkConfigfrom transformers import AutoModelForCausalLM# 加载原始模型model = AutoModelForCausalLM.from_pretrained( &quot;meta-llama/Llama-2-13b-hf&quot;, device_map=&quot;auto&quot;, torch_dtype=torch.float16)# 构建优化配置builder_config = NetworkConfig( precision=&quot;int8&quot;, use_fused_mlp=True, enable_context_fmha=True, max_batch_size=64, max_input_len=2048, max_output_len=512, quantization={ &quot;quant_algo&quot;: &quot;W8A8&quot;, &quot;kv_cache_quant_algo&quot;: &quot;FP8&quot; })# 执行模型转换builder = Builder()optimized_model = builder.build( model=model, config=builder_config, output_dir=&quot;./engines/llama-13b-int8&quot;)# 生成TensorRT引擎engine = optimized_model.build_engine( max_batch_size=64, max_beam_width=1, scheduler_config={ &quot;enable_in_flight_batching&quot;: True, &quot;max_requests&quot;: 512, &quot;preemption_mode&quot;: &quot;recompute&quot; }) 阶段3：生产级服务部署123456789101112131415161718192021222324252627282930313233343536373839404142# AKS部署配置文件（trtllm-deployment.yaml）apiVersion: apps/v1kind: Deploymentmetadata: name: trtllm-inferencespec: replicas: 4 selector: matchLabels: app: trtllm template: metadata: labels: app: trtllm spec: containers: - name: trtllm-container image: trtllm-api:1.2.0 resources: limits: nvidia.com/gpu: 2 memory: 120Gi requests: nvidia.com/gpu: 2 memory: 100Gi ports: - containerPort: 8000 env: - name: ENGINE_PATH value: &quot;/engines/llama-13b-int8&quot; - name: MAX_CONCURRENT_REQUESTS value: &quot;50&quot;# 启用HPA自动扩缩容kubectl autoscale deployment trtllm-inference \\ --cpu-percent=75 \\ --min=4 \\ --max=16 \\ --metrics=memory=70%# 配置GPU共享策略（MIG模式）nvidia-smi mig -cgi 1g.10gb,1g.10gb -C 监控体系构建 Prometheus监控指标： trtllm_inference_latency_seconds gpu_mem_utilization_percent batch_size_distribution 弹性扩缩容策略： Python代码 1234567891011# 基于请求队列的自动扩缩容逻辑def scaling_policy(current_replicas, metrics): pending_requests = metrics['pending_requests'] avg_latency = metrics['avg_latency'] if pending_requests &gt; 1000 or avg_latency &gt; 1.5: return min(current_replicas * 2, 16) elif pending_requests &lt; 200 and avg_latency &lt; 0.8: return max(current_replicas // 2, 4) else: return current_replicas 容灾机制： 实现跨可用区GPU实例部署 配置请求重试策略（指数退避算法） 部署影子模型集群用于A/B测试 一些小看法 量化选择策略： 医疗影像推荐INT8+FP16混合精度 物理仿真优先采用FP8格式 对话场景建议4-bit GPTQ 批处理优化技巧： 动态批处理窗口建议设为推理延时的1.2-1.5倍 对长短请求实施分组处理（设置最大序列长度差阈值） 显存优化进阶： 采用vLLM的PagedAttention技术 启用NVIDIA的MPS（Multi-Process Service） 使用CUDA Unified Memory实现CPU-GPU内存交换 成本优化与能效管理体系微软Azure AI通过创新性的”芯片-算法-云服务”全栈协同设计，构建了业界领先的AI推理能效管理解决方案。该体系在硬件架构、软件框架和服务模式三个层面实现突破： 算力能效革命性升级基于NVIDIA Blackwell架构的第四代AI加速芯片，通过FP8新型浮点计算单元实现算力密度跃升。相较于前代FP16架构，Blackwell的混合精度计算引擎可实现每瓦特算力提升200%，单芯片峰值算力达到10 PFLOPS（千万亿次浮点运算）。配合Azure自研的TensorRT-LLM推理优化框架，采用动态稀疏量化技术，在保证模型精度损失小于0.5%的前提下，实现显存占用压缩60%、计算时延降低45%，综合能效比提升2.8倍。经实测验证，典型NLP推理场景下，单次推理能耗从3.2Wh降至1.9Wh，降幅达40.6%。 智能弹性资源调度Azure Kubernetes服务(AKS)搭载的智能调度器，通过实时分析推理请求队列深度、GPU利用率矩阵和能耗监测数据，实现计算资源的纳米级调度。其特有的”脉冲式扩缩容”算法可在100ms内完成GPU实例的冷启动，配合分层预热技术保持核心实例池的即时响应能力。例如某全球头部电商平台，在”黑色星期五”大促期间，其推荐系统通过动态弹性伸缩机制，在5分钟内将推理集群从基准的20个GPU实例扩展至100个，峰值QPS达到120万次/秒，而资源成本仅相当于维持同等峰值能力的固定资源池的32.7%。这得益于： 毫秒级计费单元：采用10秒级粒度计量计费，避免传统云服务按小时计费的资源浪费 智能预测扩缩：基于LSTM时序预测模型，提前15分钟预加载50%的预估资源 混合精度负载均衡：将70%的常规流量分配至FP8量化模型，30%长尾请求路由至FP16高精度模型 全链路能耗监控体系Azure能耗管理控制台集成芯片级功耗传感器（精度±1.5%）、机架级PDU监控和数据中心级热力学建模，构建三维能效评估模型。管理员可实时查看从单个GPU芯片到整个AI计算集群的PUE（电源使用效率）、WUE（水利用效率）等150+项能效指标，并通过数字孪生系统模拟不同调度策略的能耗影响。实践数据显示，该体系帮助某自动驾驶客户在模型推理环节实现年度碳排放减少420吨，相当于种植6000棵成年乔木的碳汇能力。 这种”芯片级能效优化+集群级智能调度+平台级能耗治理”的三层架构，使得Azure AI推理服务在同等算力输出下，将总体拥有成本（TCO）降低58%-72%，创造了AI普惠化部署的新范式。 AI推理的下一代架构 异构计算与边缘协同：Blackwell Ultra GPU将支持CPU-GPU-NPU协同推理，推动边缘端实时AI应用（如自动驾驶决策）。 自适应量化技术：基于强化学习的动态量化策略，根据输入数据自动选择最优精度配置512。 生态扩展：Azure Marketplace计划集成NVIDIA Omniverse和Isaac Sim，支持工业数字孪生与机器人仿真的端到端优化 微软Azure AI与英伟达Blackwell平台的深度整合，标志着大模型推理从“可用”向“高效可用”的跨越。通过TensorRT-LLM的算法优化和Azure的云原生服务，企业能够以更低成本、更高性能实现AI规模化落地。未来，随着Blackwell Ultra等硬件的普及，这一技术栈有望成为行业标准，赋能金融、医疗、制造等领域的智能化转型。","link":"/2025/01/12/%E5%A6%82%E4%BD%95%E5%88%A9%E7%94%A8Azure%20AI%E4%BC%98%E5%8C%96%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%EF%BC%9ATensorRT-LLM%E4%B8%8EBlackwell%E5%B9%B3%E5%8F%B0%E6%B7%B1%E5%BA%A6%E6%95%B4%E5%90%88/"},{"title":"内存对齐","text":"CPU与内存的交互CPU获取内存数据：CPU通过地址总线（Address Bus）发送地址信号到内存，并将控制总线（Control Bus）信号设置为Enable信号，之后内存会将数据通过数据总线（Data Bus）返回给CPU。CPU写入内存数据：CPU通过地址总线（Address Bus）发送地址信号到内存，并将控制总线（Control Bus）信号设置为Set信号，最后将数据通过数据总线（data bus）发送到内存并进行写入。 内存CPU要想从内存读取数据，需要通过地址总线，把地址传输给内存，内存准备好数据，输出到数据总线若是32位地址总线，可以寻址[0,2的32次方-1]，占用内存4g 有些CPU是能够支持访问任意地址的，它是做了很多处理，比如想从地址1读取8字节的数据，CPU会分2次读，第一次从0-7,只取后7字节，第二次从8-15，但只取第一字节。把2次结果拼接起来拿到所需数据。这样比较耗费性能，编译器会把各种类型的值安排到合适的位置，并占用合适的长度。每种类型的对齐边值就是它的对齐边界。int16（2），int32（4），内存对齐要求数据存储地址以及占用的字节数都是它对齐边界的倍数。 内存对齐的收益 提高代码平台兼容性 优化数据对内存的使用 避免一些内存不对齐带来的坑 有助于一些源码的阅读 为什么要对齐列举一些常见的单位 位 bit 计算机内存数据存储的最小单位 字节 byte 计算机数据处理的基本单位 机器字 machine word 计算机用来一次性处理事务的一个固定长度 平台原因 某些硬件平台只能在某些地址处取某些特定类似的数据 性能原因 数据结构应该尽可能地在自然边界上对齐，为了访问未对齐的内存，处理器需要作2次内存访问，而内存对齐就只需要一次访问 64位字的安全访问保证 在x86-32上，64位函数使用Pentium MMX之前不存在的指令。在非Linux ARM上，64位函数使用ARMv6k内核之前不可用的指令 在ARM、x86-32和32MIPS上，调用方有责任安排对原子访问的64位字对齐。变量或分配的结构、数组或切片中的第一个字(word)可以依赖当做是64位对齐的(摘抄的,不是太懂) 操作系统的cpu不是一个字节一个字节访问的，而是2，4，8这样的字长来访问的 处理器从存储器子系统读取数据至寄存器，或者，写寄存器数据到存储器，传送的数据长度通常是字长。 如何确定每种类型的对齐边界？ 和平台有关 go语言支持这些平台 archName PtrSize(指针宽度) RegSize(寄存器宽度) 386 4 8 amd64 8 8 arm 4 4 arm64 5 8 …… 被Go语言称为寄存器宽度的这个值，就可以理解为机器字长，也是平台对应的最大对齐边界，而数据类型的对齐边界是取类型大小与平台最大对齐边界中的较小的那个 类型 大小 RegSize int8 1 byte 8 byte int16 2 byte 8 byte int32 4 byte 8 byte int64 8 byte 8 byte string 16 byte 8 byte slice 24 byte 8 byte … … … 同一个类型在不同平台上的大小可能不同，不按照最大对齐边界或者最小对齐边界来考虑是为了减少浪费、提高性能如何确定一个结构体的对齐边界先确定每个成员的对齐边界，然后取最大值 123456789type T stract { a int8 1 byte b int64 8 byte c int32 4 byte 最大对齐 8 byte d int16 2 byte } 内存对齐的第一个要求、存储这个结构体的起始地址是对齐边界的整数倍 为啥要限制类型大小等于其对其边界的整数倍 ？假如不扩张到对齐边界的整数倍，这个结构体大小就是22字节，如果要使用长度为2的T类型数组，按照元素类型大小，会占用44字节，就会导致于第二个元素并没有内存对齐 所以只有每个结构体的大小是对齐值的整数倍，才能保证数组中的每一个都是内存对齐的内存对齐的第二个要求：结构体整体占用字节数需要是类型对齐边界的倍数，不够的话要往后扩张一下举个特例 1234567891011121314151617type T1 struct { a struct{} x int64}type T2 struct { x int64 a struct{}}a1 := T1{}a2 := T2{}fmt.Printf(&quot;zone size struct{} of T1 size:%d,Ts(as final field) size:%d&quot;, unfafe.Sizeof(a1), // 8 unfafe.Sizeof(a2), // 64位，16；32位：12) T2可能做了一个Padding(填充)，因为在边界，可能会对一些边界的值进行引用等特殊：struct{} 和[0]T{} 的大小为0; 不同的大小为0的变量可能指向同一块地址。 零大小字段对齐零大小字段（zero sized field）是指struct{}大小为0，按理作为字段时不需要对齐，但当在作为结构体最后一个字段（final field）时需要对齐的。为什么？因为，如果有指针指向这个final zero field, 返回的地址将在结构体之外（即指向了别的内存），如果此指针一直存活不释放对应的内存，就会有内存泄露的问题（该内存不因结构体释放而释放）使用 golangci-lint 检测对齐golangci-lint run –disable-all -E maligned 结论 内存对齐是为了cpu更高效的访问内存中的数据 结构体对齐依赖类型的大小保证和对齐保证 地址对齐保证是:如果类型t的对齐保证是n，那么类型t的每个值的地址在运行时必须是n的倍数 零大小字段要避免只作为struct最后一个字段，会有内存浪费 参考 【Golang】这个内存对齐呀！？ Golang 是否有必要内存对齐？ Go 的内存对齐和指针运算详解和实践","link":"/2020/08/11/%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90/"}],"tags":[{"name":"go","slug":"go","link":"/tags/go/"},{"name":"杂谈","slug":"杂谈","link":"/tags/%E6%9D%82%E8%B0%88/"},{"name":"azure","slug":"azure","link":"/tags/azure/"},{"name":"context","slug":"context","link":"/tags/context/"},{"name":"MySQL","slug":"MySQL","link":"/tags/MySQL/"},{"name":"字符串","slug":"字符串","link":"/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2/"},{"name":"golang","slug":"golang","link":"/tags/golang/"},{"name":"elasticsearch","slug":"elasticsearch","link":"/tags/elasticsearch/"},{"name":"kafka","slug":"kafka","link":"/tags/kafka/"},{"name":"Docker","slug":"Docker","link":"/tags/Docker/"},{"name":"vim","slug":"vim","link":"/tags/vim/"},{"name":"跨域","slug":"跨域","link":"/tags/%E8%B7%A8%E5%9F%9F/"},{"name":"iframe","slug":"iframe","link":"/tags/iframe/"},{"name":"订单号","slug":"订单号","link":"/tags/%E8%AE%A2%E5%8D%95%E5%8F%B7/"},{"name":"规则","slug":"规则","link":"/tags/%E8%A7%84%E5%88%99/"},{"name":"读书","slug":"读书","link":"/tags/%E8%AF%BB%E4%B9%A6/"},{"name":"个人","slug":"个人","link":"/tags/%E4%B8%AA%E4%BA%BA/"},{"name":"索引","slug":"索引","link":"/tags/%E7%B4%A2%E5%BC%95/"},{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"内存对齐","slug":"内存对齐","link":"/tags/%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90/"}],"categories":[{"name":"panic","slug":"panic","link":"/categories/panic/"},{"name":"杂谈","slug":"杂谈","link":"/categories/%E6%9D%82%E8%B0%88/"},{"name":"azure","slug":"azure","link":"/categories/azure/"},{"name":"golang","slug":"golang","link":"/categories/golang/"},{"name":"MySQL","slug":"MySQL","link":"/categories/MySQL/"},{"name":"gin","slug":"gin","link":"/categories/gin/"},{"name":"map","slug":"map","link":"/categories/map/"},{"name":"string","slug":"string","link":"/categories/string/"},{"name":"内存逃逸","slug":"内存逃逸","link":"/categories/%E5%86%85%E5%AD%98%E9%80%83%E9%80%B8/"},{"name":"规范","slug":"规范","link":"/categories/%E8%A7%84%E8%8C%83/"},{"name":"kafka","slug":"kafka","link":"/categories/kafka/"},{"name":"docker","slug":"docker","link":"/categories/docker/"},{"name":"vim","slug":"vim","link":"/categories/vim/"},{"name":"反射","slug":"反射","link":"/categories/%E5%8F%8D%E5%B0%84/"},{"name":"效率","slug":"效率","link":"/categories/%E6%95%88%E7%8E%87/"},{"name":"mysql","slug":"mysql","link":"/categories/mysql/"},{"name":"Python","slug":"Python","link":"/categories/Python/"},{"name":"函数","slug":"函数","link":"/categories/%E5%87%BD%E6%95%B0/"}],"pages":[{"title":"","text":"梁友泽个人信息 Github https://github.com/youzeliang 技术博客 https://www.liangyouze.com email: i@liangyouze.com 就职于某在线教育公司，负责方向:网课直播相关业务","link":"/about/index.html"}]}