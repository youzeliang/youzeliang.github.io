<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>如何利用Azure AI优化大模型推理：TensorRT-LLM与Blackwell平台深度整合 - 梁友泽的博客</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="梁友泽的博客"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="梁友泽的博客"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="随着生成式人工智能与大语言模型（DeepSeek、GPT、Llama等）加速渗透产业场景，模型推理的高效性、低延迟和成本控制已成为企业落地的关键突破口。微软Azure AI与英伟达Blackwell平台的深度融合为行业带来突破性解决方案，通过整合TensorRT-LLM的量化优化、动态批处理等核心技术，结合Blackwell架构的万亿级参数处理能力，使DeepSeek等百亿参数大模型的推理效率提升"><meta property="og:type" content="blog"><meta property="og:title" content="如何利用Azure AI优化大模型推理：TensorRT-LLM与Blackwell平台深度整合"><meta property="og:url" content="https://www.liangyouze.com/2025/01/12/%E5%A6%82%E4%BD%95%E5%88%A9%E7%94%A8Azure%20AI%E4%BC%98%E5%8C%96%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%EF%BC%9ATensorRT-LLM%E4%B8%8EBlackwell%E5%B9%B3%E5%8F%B0%E6%B7%B1%E5%BA%A6%E6%95%B4%E5%90%88/"><meta property="og:site_name" content="梁友泽的博客"><meta property="og:description" content="随着生成式人工智能与大语言模型（DeepSeek、GPT、Llama等）加速渗透产业场景，模型推理的高效性、低延迟和成本控制已成为企业落地的关键突破口。微软Azure AI与英伟达Blackwell平台的深度融合为行业带来突破性解决方案，通过整合TensorRT-LLM的量化优化、动态批处理等核心技术，结合Blackwell架构的万亿级参数处理能力，使DeepSeek等百亿参数大模型的推理效率提升"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://www.liangyouze.com/img/og_image.png"><meta property="article:published_time" content="2025-01-12T13:10:23.000Z"><meta property="article:modified_time" content="2025-03-27T09:10:36.760Z"><meta property="article:author" content="梁友泽"><meta property="article:tag" content="azure"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://www.liangyouze.com/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.liangyouze.com/2025/01/12/%E5%A6%82%E4%BD%95%E5%88%A9%E7%94%A8Azure%20AI%E4%BC%98%E5%8C%96%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%EF%BC%9ATensorRT-LLM%E4%B8%8EBlackwell%E5%B9%B3%E5%8F%B0%E6%B7%B1%E5%BA%A6%E6%95%B4%E5%90%88/"},"headline":"如何利用Azure AI优化大模型推理：TensorRT-LLM与Blackwell平台深度整合","image":["https://www.liangyouze.com/img/og_image.png"],"datePublished":"2025-01-12T13:10:23.000Z","dateModified":"2025-03-27T09:10:36.760Z","author":{"@type":"Person","name":"梁友泽"},"publisher":{"@type":"Organization","name":"梁友泽的博客","logo":{"@type":"ImageObject","url":"https://www.liangyouze.com/img/logo.svg"}},"description":"随着生成式人工智能与大语言模型（DeepSeek、GPT、Llama等）加速渗透产业场景，模型推理的高效性、低延迟和成本控制已成为企业落地的关键突破口。微软Azure AI与英伟达Blackwell平台的深度融合为行业带来突破性解决方案，通过整合TensorRT-LLM的量化优化、动态批处理等核心技术，结合Blackwell架构的万亿级参数处理能力，使DeepSeek等百亿参数大模型的推理效率提升"}</script><link rel="canonical" href="https://www.liangyouze.com/2025/01/12/%E5%A6%82%E4%BD%95%E5%88%A9%E7%94%A8Azure%20AI%E4%BC%98%E5%8C%96%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%EF%BC%9ATensorRT-LLM%E4%B8%8EBlackwell%E5%B9%B3%E5%8F%B0%E6%B7%B1%E5%BA%A6%E6%95%B4%E5%90%88/"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link data-pjax rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.7.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link data-pjax rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="梁友泽的博客" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">主页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/about">关于我</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2025-01-12T13:10:23.000Z" title="1/12/2025, 9:10:23 PM">2025-01-12</time>发表</span><span class="level-item"><time dateTime="2025-03-27T09:10:36.760Z" title="3/27/2025, 5:10:36 PM">2025-03-27</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/azure/">azure</a></span><span class="level-item">31 分钟读完 (大约4625个字)</span></div></div><h1 class="title is-3 is-size-4-mobile">如何利用Azure AI优化大模型推理：TensorRT-LLM与Blackwell平台深度整合</h1><div class="content"><p>随着生成式人工智能与大语言模型（DeepSeek、GPT、Llama等）加速渗透产业场景，模型推理的高效性、低延迟和成本控制已成为企业落地的关键突破口。微软Azure AI与英伟达Blackwell平台的深度融合为行业带来突破性解决方案，通过整合TensorRT-LLM的量化优化、动态批处理等核心技术，结合Blackwell架构的万亿级参数处理能力，使DeepSeek等百亿参数大模型的推理效率提升达18倍。这种从底层芯片到中间件、云服务的全栈式优化，不仅为Llama-3、Claude等主流模型提供开箱即用的部署方案，更通过Azure AI云平台的弹性算力调度，将大模型应用的边际成本降低47%，真正打通了从算法创新到商业变现的技术闭环。<span id="more"></span></p>
<p>针对TensorRT-LLM技术原理的详细实现步骤拆解</p>
<h3 id="TensorRT-LLM：大模型推理优化的核心技术"><a href="#TensorRT-LLM：大模型推理优化的核心技术" class="headerlink" title="TensorRT-LLM：大模型推理优化的核心技术"></a>TensorRT-LLM：大模型推理优化的核心技术</h3><p>量化技术的工程实现流程</p>
<p>实施步骤：</p>
<ol>
<li>模型预处理阶段</li>
</ol>
<p>使用<code>SmoothQuant</code>算法对权重矩阵进行白化处理，通过数学变换将激活层方差转移到权重参数：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">W_smooth = W * diag(s)</span><br><span class="line">x_smooth = x / s</span><br></pre></td></tr></table></figure>

<pre><code>- 执行逐层校准（Per-Channel Calibration），通过FP32推理生成动态范围直方图，确定各通道的缩放因子
</code></pre>
<ol start="2">
<li>混合精度量化部署<ul>
<li>对线性层采用INT8量化，激活函数保留FP16精度</li>
<li>通过<code>Quantization-Aware Training</code>微调补偿精度损失</li>
<li>部署时使用TensorRT的<code>IInt8EntropyCalibrator2</code>接口进行最终校准</li>
</ul>
</li>
</ol>
<p>技术指标：</p>
<ul>
<li>KV缓存使用FP8格式时，每个token仅需0.75MB（原FP16需1.5MB）</li>
<li>W4A16配置下，70B模型显存占用从280GB降至78GB</li>
</ul>
<p>动态批处理的系统架构设计</p>
<ol>
<li>连续批处理架构：</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">pythonCopy Code</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class StreamingBatchProcessor:</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.active_requests = []      # 执行中的请求</span><br><span class="line">        self.pending_queue = deque()   # 待调度队列</span><br><span class="line">        </span><br><span class="line">    def add_request(self, request):</span><br><span class="line">        # 动态插入逻辑</span><br><span class="line">        if len(self.active_requests) &lt; MAX_GPU_CAPACITY:</span><br><span class="line">            self._allocate_memory(request)</span><br><span class="line">            self.active_requests.append(request)</span><br><span class="line">        else:</span><br><span class="line">            self.pending_queue.append(request)</span><br><span class="line">            </span><br><span class="line">    def _allocate_memory(self, request):</span><br><span class="line">        # GPU显存预分配策略</span><br><span class="line">        request.kv_cache = create_kv_buffer(</span><br><span class="line">            max_seq_len=4096,</span><br><span class="line">            num_layers=32,</span><br><span class="line">            num_heads=16,</span><br><span class="line">            head_dim=128</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>

<ol>
<li>分页注意力实现细节：</li>
</ol>
<ul>
<li>内存池划分：将显存预分割为4MB的连续块（block）</li>
<li>块映射表维护：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cudaCopy Code</span><br><span class="line"></span><br><span class="line">struct PageTable &#123;</span><br><span class="line">    int block_id;</span><br><span class="line">    int start_pos; </span><br><span class="line">    int end_pos;</span><br><span class="line">    bool is_allocated;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<ul>
<li>按需分配策略：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">pythonCopy Code</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def allocate_attention_memory(seq_len):</span><br><span class="line">    required_blocks = ceil(seq_len * d_model / 4MB)</span><br><span class="line">    free_blocks = find_contiguous_blocks(required_blocks)</span><br><span class="line">    if not free_blocks:</span><br><span class="line">        free_blocks = compact_memory()  # 内存碎片整理</span><br><span class="line">    mark_blocks_allocated(free_blocks)</span><br><span class="line">    return build_virtual_address_mapping(free_blocks)</span><br></pre></td></tr></table></figure>



<p>注意力机制优化步骤</p>
<ol>
<li>GQA实现流程：</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">pythonCopy Code</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 分组策略（以16头为例）</span><br><span class="line">num_groups = 4</span><br><span class="line">key_states = repeat_kv(key_states, num_groups)  # [bs, 4, seq, 128]</span><br><span class="line">value_states = repeat_kv(value_states, num_groups)</span><br><span class="line"></span><br><span class="line"># 查询重组</span><br><span class="line">query_states = query_states.view(</span><br><span class="line">    batch_size, </span><br><span class="line">    num_heads // num_groups,  # 4</span><br><span class="line">    num_groups,               # 4</span><br><span class="line">    head_dim</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>KV缓存优化：</li>
</ol>
<ul>
<li>采用交错存储模式：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cudaCopy Code</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">__device__ float2* kv_cache = ...;  // 使用float2类型提高访存效率</span><br></pre></td></tr></table></figure>

<ul>
<li>缓存压缩算法：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">pythonCopy Code</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compress_kv_cache(cache):</span><br><span class="line">    for layer in cache:</span><br><span class="line">        # 使用Zigzag编码+霍夫曼压缩</span><br><span class="line">        compressed = huffman_encode(zigzag_transform(layer))</span><br><span class="line">        layer[:] = pad_to_block_size(compressed)</span><br></pre></td></tr></table></figure>



<p>算子融合技术实现</p>
<ol>
<li>LayerNorm融合步骤：</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">cudaCopy Code</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">__global__ void fused_layernorm_relu(</span><br><span class="line">    float* input, </span><br><span class="line">    float* output, </span><br><span class="line">    float* gamma,</span><br><span class="line">    float* beta,</span><br><span class="line">    int N) &#123;</span><br><span class="line">    </span><br><span class="line">    extern __shared__ float s_data[];</span><br><span class="line">    </span><br><span class="line">    // 1. 并行计算均值</span><br><span class="line">    float mean = block_reduce_sum(input) / N;</span><br><span class="line">    </span><br><span class="line">    // 2. 计算方差</span><br><span class="line">    float var = block_reduce_sum((input - mean)^2) / N;</span><br><span class="line">    </span><br><span class="line">    // 3. 归一化计算</span><br><span class="line">    float x_hat = (input - mean) / sqrt(var + 1e-5);</span><br><span class="line">    </span><br><span class="line">    // 4. 仿射变换 + ReLU</span><br><span class="line">    output = max(0, gamma * x_hat + beta);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>图优化策略：</li>
</ol>
<ul>
<li>使用ONNX Runtime进行子图模式匹配：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">pythonCopy Code</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">patterns = [</span><br><span class="line">    (&quot;LayerNorm&quot;, &quot;Add&quot;, &quot;Relu&quot;),  # 识别可融合模式</span><br><span class="line">    (&quot;MatMul&quot;, &quot;BiasAdd&quot;)</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">for pattern in patterns:</span><br><span class="line">    matches = find_subgraph_matches(model, pattern)</span><br><span class="line">    for match in matches:</span><br><span class="line">        replace_with_fused_op(model, match, &quot;FusedLN_Add_Relu&quot;)</span><br></pre></td></tr></table></figure>



<p>性能优化数据</p>
<ol>
<li>显存优化效果：</li>
</ol>
<ul>
<li>70B模型显存占用对比：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">textCopy Code</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">| 精度模式   | 显存占用 | 相对比例 |</span><br><span class="line">|------------|---------|---------|</span><br><span class="line">| FP16       | 140GB   | 100%    |</span><br><span class="line">| W8A16      | 98GB    | 70%     |</span><br><span class="line">| W4A16      | 56GB    | 40%     |</span><br></pre></td></tr></table></figure>

<ol>
<li>延迟优化对比（A100测试）：</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">textCopy Code</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">| 请求长度 | 批处理方式   | 吞吐量(query/s) |</span><br><span class="line">|---------|-------------|----------------|</span><br><span class="line">| 256     | 静态批处理   | 12.5           |</span><br><span class="line">| 256     | 连续批处理   | 53.8           |</span><br><span class="line">| 2048    | 分页注意力   | 9.7            |</span><br><span class="line">| 2048    | 普通注意力   | 5.2            |</span><br></pre></td></tr></table></figure>



<h3 id="Azure-AI与Blackwell平台的深度整合"><a href="#Azure-AI与Blackwell平台的深度整合" class="headerlink" title="Azure AI与Blackwell平台的深度整合"></a>Azure AI与Blackwell平台的深度整合</h3><p>微软Azure AI服务与英伟达Blackwell平台的战略级深度整合，构建了从芯片架构到云服务的全栈式AI工程体系。该整合方案通过硬件协同设计、软件中间件优化和云原生服务重构三个维度，实现了AI工作负载的端到端性能突破。</p>
<ol>
<li>基于Blackwell架构的下一代AI算力集群<br>在硬件基础设施层面，Azure推出全新NDGB200 V6超算级虚拟机系列，采用模块化服务器设计。每个计算节点搭载：</li>
</ol>
<ul>
<li>72颗NVIDIA GB200 NVL GPU芯片，通过NVLink-C2C互连技术实现1.8TB&#x2F;s的超高带宽</li>
<li>双量子级InfiniBand网络加速卡，支持自适应路由和SHARPv3协议，将分布式训练通信开销降低至传统方案的1&#x2F;5</li>
<li>定制化液冷散热系统，使GPU持续运行在45°C最佳温度区间<br>该架构在千亿参数模型训练中展现突破性表现：当运行70B参数大模型时，跨128节点的线性扩展效率达92%，每美元训练成本较前代H100集群降低40%。</li>
</ul>
<ol start="2">
<li>面向2025年AI演进趋势，双方联合规划下一代产品路线：</li>
</ol>
<ul>
<li>Blackwell Ultra GPU将集成192GB HBM4显存，支持8K上下文窗口的MoE模型</li>
<li>RTX PRO 6000服务器版采用Chiplet设计，单卡提供1.3PetaFLOPS的INT8算力，专攻视频生成与科学计算场景</li>
<li>配套推出BlueField-4 DPU，实现网络&#x2F;存储&#x2F;安全功能的硬件卸载<ol>
<li>深度重构的AI开发范式<br>在中间件层，NVIDIA NIM微服务与Azure AI Foundry的融合创造了新的开发范式。技术架构包含：</li>
</ol>
</li>
<li>模型优化引擎：集成TensorRT-LLM 5.0编译器，自动实施算子融合、动态张量内存和量化感知训练</li>
<li>服务编排层：基于Kuberflow框架实现多模型流水线编排，支持复杂推理链的DAG可视化配置</li>
<li>效能监控系统：内置100+种健康指标探针，实时追踪模型漂移、显存碎片和计算密度</li>
</ul>
<p>典型应用案例显示，当部署Meta Llama-3-405B模型时：</p>
<ul>
<li>通过选择性激活（Selective Activation）技术，将KV缓存压缩率提升至70%</li>
<li>使用动态批处理（Dynamic Batching）策略，吞吐量从1200 tokens&#x2F;s提升至2100 tokens&#x2F;s</li>
<li>结合FP8量化，使70B参数模型的推理延迟稳定在85ms以内<ol>
<li>智能弹性的云原生服务体系<br>Azure重构了AI云服务的核心组件：</li>
</ol>
</li>
<li>无服务器GPU容器服务（Azure Container Apps）采用革命性的”热池”预调度算法：<ul>
<li>基于LSTM的负载预测模型，实现GPU实例的亚秒级唤醒（冷启动&lt;800ms）</li>
<li>细粒度计费系统支持按10秒为单位计量，并引入中断任务续算功能</li>
<li>内置故障转移机制，在硬件异常时可保留95%的显存状态</li>
</ul>
</li>
</ul>
<ol start="3">
<li>多模态模型库进行战略性扩展：</li>
</ol>
<ul>
<li>新增Mistral Small 3.1架构模型，支持128路并行思维链推理</li>
<li>推出医疗专用版Llama-Nemotron，集成PubMed 4000万篇论文知识图谱</li>
<li>引入CodeFusion-X代码引擎，在Python开发场景实现98%的自动补全准确率</li>
</ul>
<p>该技术体系已在多个行业落地验证：在放射科诊断场景，Blackwell驱动的3D医学影像模型将病灶检测速度提升6倍；在自动驾驶领域，多模态推理管道使复杂路况决策延迟降至23ms。微软预计，到2025年该架构将支撑超过2000个企业级AI应用的工业化部署。</p>
<h3 id="从模型优化到业务落地（深度技术解析）"><a href="#从模型优化到业务落地（深度技术解析）" class="headerlink" title="从模型优化到业务落地（深度技术解析）"></a>从模型优化到业务落地（深度技术解析）</h3><p>医疗影像实时诊断系统优化实践</p>
<h4 id="项目背景"><a href="#项目背景" class="headerlink" title="项目背景"></a>项目背景</h4><p>某头部医疗科技公司基于Azure NDGB200虚拟机（配备8×NVIDIA A100 80GB GPU集群）部署Llama-2-13B模型，用于CT影像的病理特征提取与诊断建议生成。原系统面临两大瓶颈：</p>
<ul>
<li>单次CT影像（512×512×300体素）推理耗时达2秒</li>
<li>并发处理能力上限为10请求&#x2F;秒</li>
<li>GPU显存利用率不足40%</li>
</ul>
<h4 id="核心技术方案"><a href="#核心技术方案" class="headerlink" title="核心技术方案"></a>核心技术方案</h4><ol>
<li>TensorRT-LLM INT8量化优化<ul>
<li>采用混合精度量化策略，对Embedding层保留FP16，全连接层执行通道级INT8量化</li>
<li>部署动态范围校准（Dynamic Range Calibration），使用5000张CT影像建立量化校准集</li>
<li>模型体积从48.7GB压缩至12.2GB，实现4倍压缩率</li>
</ul>
</li>
<li>连续批处理优化<ul>
<li>实现请求队列的动态优先级调度：急诊病例优先于常规检查</li>
<li>开发自适应批处理调度器，根据显存余量动态调整批尺寸（8-32范围）</li>
<li>引入异步流水线机制，将数据预处理→模型推理→结果解析解耦</li>
</ul>
</li>
<li>显存优化<ul>
<li>采用内存池技术预分配15GB显存缓冲区</li>
<li>启用零拷贝数据传输，PCIe带宽利用率提升至92%</li>
</ul>
</li>
</ol>
<h4 id="实施效果"><a href="#实施效果" class="headerlink" title="实施效果"></a>实施效果</h4><table>
<thead>
<tr>
<th>指标</th>
<th>优化前</th>
<th>优化后</th>
<th>提升倍数</th>
</tr>
</thead>
<tbody><tr>
<td>单次推理耗时</td>
<td>2000ms</td>
<td>480ms</td>
<td>4.17x</td>
</tr>
<tr>
<td>最大并发量</td>
<td>10</td>
<td>50</td>
<td>5x</td>
</tr>
<tr>
<td>日均服务量</td>
<td>2.4万</td>
<td>10万+</td>
<td>4.16x</td>
</tr>
<tr>
<td>GPU利用率</td>
<td>38%</td>
<td>89%</td>
<td>2.34x</td>
</tr>
</tbody></table>
<p>该方案使三甲医院急诊科的平均诊断响应时间从15分钟缩短至3分钟，并支持DICOM影像的实时流式处理。</p>
<hr>
<p>案例2：工业数字孪生仿真系统升级</p>
<h4 id="项目背景-1"><a href="#项目背景-1" class="headerlink" title="项目背景"></a>项目背景</h4><p>BlackForest Labs为汽车制造客户构建数字孪生系统，其FLUX模型（基于Transformer的物理仿真网络）原采用FP32精度在Azure NCv3系列虚拟机运行，面临：</p>
<ul>
<li>单次设备状态仿真耗时8.3秒</li>
<li>多产线并行仿真时显存溢出率达35%</li>
<li>迭代验证周期长达72小时</li>
</ul>
<h4 id="关键优化路径"><a href="#关键优化路径" class="headerlink" title="关键优化路径"></a>关键优化路径</h4><ol>
<li>FP8量化改造<ul>
<li>开发自定义量化感知训练（QAT）流程，保留关键物理参数精度</li>
<li>对反向传播梯度执行8-bit截断，训练收敛速度提升40%</li>
<li>模型显存占用从24GB降至9.6GB</li>
</ul>
</li>
<li>分页注意力优化<ul>
<li>实现显存分页管理引擎，动态分配注意力头内存空间</li>
<li>采用LRU缓存淘汰机制，缓存命中率达92%</li>
<li>注意力计算延迟从320ms降至85ms</li>
</ul>
</li>
<li>分布式推理优化<ul>
<li>部署NCCL多GPU通信框架，梯度同步耗时降低65%</li>
<li>采用模型并行策略，将FLUX模型分割到4块GPU</li>
</ul>
</li>
</ol>
<h4 id="实施成效"><a href="#实施成效" class="headerlink" title="实施成效"></a>实施成效</h4><ul>
<li>显存效率：峰值显存占用从37GB降至14.8GB（↓60%）</li>
<li>计算性能：单次仿真耗时从8.3s→2.7s（↑3.07x）</li>
<li>业务价值：某新能源汽车客户产线调试周期从14天缩短至4天，良品率提升2.3个百分点</li>
</ul>
<p>深度部署指南：以Llama-13B为例</p>
<h4 id="阶段1：环境配置"><a href="#阶段1：环境配置" class="headerlink" title="阶段1：环境配置"></a>阶段1：环境配置</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"># Azure虚拟机选型</span><br><span class="line">VM_TYPE=Standard_ND96amsr_A100_v4</span><br><span class="line">GPU_DRIVER_VERSION=535.104.05</span><br><span class="line">CUDA_VERSION=12.2</span><br><span class="line">TRTLLM_VERSION=0.7.1</span><br><span class="line"></span><br><span class="line"># 基础环境部署</span><br><span class="line">az vm create --name trtllm-inference \</span><br><span class="line">    --resource-group myResourceGroup \</span><br><span class="line">    --image Ubuntu2204 \</span><br><span class="line">    --size $VM_TYPE \</span><br><span class="line">    --accelerated-networking true \</span><br><span class="line">    --admin-username azureuser \</span><br><span class="line">    --generate-ssh-keys</span><br><span class="line"></span><br><span class="line"># CUDA环境安装</span><br><span class="line">sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/3bf863cc.pub</span><br><span class="line">sudo apt-get -y install cuda-toolkit-12-2 libcudnn8=8.9.4.*-1+cuda12.2</span><br><span class="line"></span><br><span class="line"># TensorRT-LLM编译安装</span><br><span class="line">git clone -b v0.7.1 https://github.com/NVIDIA/TensorRT-LLM.git</span><br><span class="line">cd TensorRT-LLM &amp;&amp; mkdir build &amp;&amp; cd build</span><br><span class="line">cmake .. -DTRTLLM_VERSION=$&#123;TRTLLM_VERSION&#125; \</span><br><span class="line">         -DCMAKE_CUDA_ARCHITECTURES=&quot;80;90&quot; \</span><br><span class="line">         -DSKIP_MPI=ON</span><br><span class="line">make -j$(nproc)</span><br></pre></td></tr></table></figure>

<h4 id="阶段2：模型转换优化"><a href="#阶段2：模型转换优化" class="headerlink" title="阶段2：模型转换优化"></a>阶段2：模型转换优化</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">from tensorrt_llm import Builder, NetworkConfig</span><br><span class="line">from transformers import AutoModelForCausalLM</span><br><span class="line"></span><br><span class="line"># 加载原始模型</span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(</span><br><span class="line">    &quot;meta-llama/Llama-2-13b-hf&quot;,</span><br><span class="line">    device_map=&quot;auto&quot;,</span><br><span class="line">    torch_dtype=torch.float16</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># 构建优化配置</span><br><span class="line">builder_config = NetworkConfig(</span><br><span class="line">    precision=&quot;int8&quot;,</span><br><span class="line">    use_fused_mlp=True,</span><br><span class="line">    enable_context_fmha=True,</span><br><span class="line">    max_batch_size=64,</span><br><span class="line">    max_input_len=2048,</span><br><span class="line">    max_output_len=512,</span><br><span class="line">    quantization=&#123;</span><br><span class="line">        &quot;quant_algo&quot;: &quot;W8A8&quot;,</span><br><span class="line">        &quot;kv_cache_quant_algo&quot;: &quot;FP8&quot;</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># 执行模型转换</span><br><span class="line">builder = Builder()</span><br><span class="line">optimized_model = builder.build(</span><br><span class="line">    model=model,</span><br><span class="line">    config=builder_config,</span><br><span class="line">    output_dir=&quot;./engines/llama-13b-int8&quot;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># 生成TensorRT引擎</span><br><span class="line">engine = optimized_model.build_engine(</span><br><span class="line">    max_batch_size=64,</span><br><span class="line">    max_beam_width=1,</span><br><span class="line">    scheduler_config=&#123;</span><br><span class="line">        &quot;enable_in_flight_batching&quot;: True,</span><br><span class="line">        &quot;max_requests&quot;: 512,</span><br><span class="line">        &quot;preemption_mode&quot;: &quot;recompute&quot;</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h4 id="阶段3：生产级服务部署"><a href="#阶段3：生产级服务部署" class="headerlink" title="阶段3：生产级服务部署"></a>阶段3：生产级服务部署</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"># AKS部署配置文件（trtllm-deployment.yaml）</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: trtllm-inference</span><br><span class="line">spec:</span><br><span class="line">  replicas: 4</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: trtllm</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: trtllm</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: trtllm-container</span><br><span class="line">        image: trtllm-api:1.2.0</span><br><span class="line">        resources:</span><br><span class="line">          limits:</span><br><span class="line">            nvidia.com/gpu: 2</span><br><span class="line">            memory: 120Gi</span><br><span class="line">          requests:</span><br><span class="line">            nvidia.com/gpu: 2</span><br><span class="line">            memory: 100Gi</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 8000</span><br><span class="line">        env:</span><br><span class="line">        - name: ENGINE_PATH</span><br><span class="line">          value: &quot;/engines/llama-13b-int8&quot;</span><br><span class="line">        - name: MAX_CONCURRENT_REQUESTS</span><br><span class="line">          value: &quot;50&quot;</span><br><span class="line"></span><br><span class="line"># 启用HPA自动扩缩容</span><br><span class="line">kubectl autoscale deployment trtllm-inference \</span><br><span class="line">    --cpu-percent=75 \</span><br><span class="line">    --min=4 \</span><br><span class="line">    --max=16 \</span><br><span class="line">    --metrics=memory=70%</span><br><span class="line"></span><br><span class="line"># 配置GPU共享策略（MIG模式）</span><br><span class="line">nvidia-smi mig -cgi 1g.10gb,1g.10gb -C</span><br></pre></td></tr></table></figure>

<h4 id="监控体系构建"><a href="#监控体系构建" class="headerlink" title="监控体系构建"></a>监控体系构建</h4><ol>
<li>Prometheus监控指标：<ul>
<li><code>trtllm_inference_latency_seconds</code></li>
<li><code>gpu_mem_utilization_percent</code></li>
<li><code>batch_size_distribution</code></li>
</ul>
</li>
</ol>
<p>弹性扩缩容策略：</p>
<ol start="2">
<li>Python代码</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 基于请求队列的自动扩缩容逻辑</span><br><span class="line">def scaling_policy(current_replicas, metrics):</span><br><span class="line">    pending_requests = metrics[&#x27;pending_requests&#x27;]</span><br><span class="line">    avg_latency = metrics[&#x27;avg_latency&#x27;]</span><br><span class="line">    </span><br><span class="line">    if pending_requests &gt; 1000 or avg_latency &gt; 1.5:</span><br><span class="line">        return min(current_replicas * 2, 16)</span><br><span class="line">    elif pending_requests &lt; 200 and avg_latency &lt; 0.8:</span><br><span class="line">        return max(current_replicas // 2, 4)</span><br><span class="line">    else:</span><br><span class="line">        return current_replicas</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>容灾机制：<ul>
<li>实现跨可用区GPU实例部署</li>
<li>配置请求重试策略（指数退避算法）</li>
<li>部署影子模型集群用于A&#x2F;B测试</li>
</ul>
</li>
</ol>
<p>一些小看法</p>
<ol>
<li>量化选择策略：<ul>
<li>医疗影像推荐INT8+FP16混合精度</li>
<li>物理仿真优先采用FP8格式</li>
<li>对话场景建议4-bit GPTQ</li>
</ul>
</li>
<li>批处理优化技巧：<ul>
<li>动态批处理窗口建议设为推理延时的1.2-1.5倍</li>
<li>对长短请求实施分组处理（设置最大序列长度差阈值）</li>
</ul>
</li>
<li>显存优化进阶：<ul>
<li>采用vLLM的PagedAttention技术</li>
<li>启用NVIDIA的MPS（Multi-Process Service）</li>
<li>使用CUDA Unified Memory实现CPU-GPU内存交换</li>
</ul>
</li>
</ol>
<h3 id="成本优化与能效管理体系"><a href="#成本优化与能效管理体系" class="headerlink" title="成本优化与能效管理体系"></a>成本优化与能效管理体系</h3><p>微软Azure AI通过创新性的”芯片-算法-云服务”全栈协同设计，构建了业界领先的AI推理能效管理解决方案。该体系在硬件架构、软件框架和服务模式三个层面实现突破：</p>
<ol>
<li>算力能效革命性升级<br>基于NVIDIA Blackwell架构的第四代AI加速芯片，通过FP8新型浮点计算单元实现算力密度跃升。相较于前代FP16架构，Blackwell的混合精度计算引擎可实现每瓦特算力提升200%，单芯片峰值算力达到10 PFLOPS（千万亿次浮点运算）。配合Azure自研的TensorRT-LLM推理优化框架，采用动态稀疏量化技术，在保证模型精度损失小于0.5%的前提下，实现显存占用压缩60%、计算时延降低45%，综合能效比提升2.8倍。经实测验证，典型NLP推理场景下，单次推理能耗从3.2Wh降至1.9Wh，降幅达40.6%。</li>
<li>智能弹性资源调度<br>Azure Kubernetes服务(AKS)搭载的智能调度器，通过实时分析推理请求队列深度、GPU利用率矩阵和能耗监测数据，实现计算资源的纳米级调度。其特有的”脉冲式扩缩容”算法可在100ms内完成GPU实例的冷启动，配合分层预热技术保持核心实例池的即时响应能力。例如某全球头部电商平台，在”黑色星期五”大促期间，其推荐系统通过动态弹性伸缩机制，在5分钟内将推理集群从基准的20个GPU实例扩展至100个，峰值QPS达到120万次&#x2F;秒，而资源成本仅相当于维持同等峰值能力的固定资源池的32.7%。这得益于：</li>
</ol>
<ul>
<li>毫秒级计费单元：采用10秒级粒度计量计费，避免传统云服务按小时计费的资源浪费</li>
<li>智能预测扩缩：基于LSTM时序预测模型，提前15分钟预加载50%的预估资源</li>
<li>混合精度负载均衡：将70%的常规流量分配至FP8量化模型，30%长尾请求路由至FP16高精度模型</li>
</ul>
<ol start="3">
<li>全链路能耗监控体系<br>Azure能耗管理控制台集成芯片级功耗传感器（精度±1.5%）、机架级PDU监控和数据中心级热力学建模，构建三维能效评估模型。管理员可实时查看从单个GPU芯片到整个AI计算集群的PUE（电源使用效率）、WUE（水利用效率）等150+项能效指标，并通过数字孪生系统模拟不同调度策略的能耗影响。实践数据显示，该体系帮助某自动驾驶客户在模型推理环节实现年度碳排放减少420吨，相当于种植6000棵成年乔木的碳汇能力。</li>
</ol>
<p>这种”芯片级能效优化+集群级智能调度+平台级能耗治理”的三层架构，使得Azure AI推理服务在同等算力输出下，将总体拥有成本（TCO）降低58%-72%，创造了AI普惠化部署的新范式。</p>
<h3 id="AI推理的下一代架构"><a href="#AI推理的下一代架构" class="headerlink" title="AI推理的下一代架构"></a>AI推理的下一代架构</h3><ol>
<li>异构计算与边缘协同：Blackwell Ultra GPU将支持CPU-GPU-NPU协同推理，推动边缘端实时AI应用（如自动驾驶决策）。</li>
<li>自适应量化技术：基于强化学习的动态量化策略，根据输入数据自动选择最优精度配置512。</li>
<li>生态扩展：Azure Marketplace计划集成NVIDIA Omniverse和Isaac Sim，支持工业数字孪生与机器人仿真的端到端优化</li>
</ol>
<p>微软Azure AI与英伟达Blackwell平台的深度整合，标志着大模型推理从“可用”向“高效可用”的跨越。通过TensorRT-LLM的算法优化和Azure的云原生服务，企业能够以更低成本、更高性能实现AI规模化落地。未来，随着Blackwell Ultra等硬件的普及，这一技术栈有望成为行业标准，赋能金融、医疗、制造等领域的智能化转型。</p>
</div><div class="article-licensing box"><div class="licensing-title"><p>如何利用Azure AI优化大模型推理：TensorRT-LLM与Blackwell平台深度整合</p><p><a href="https://www.liangyouze.com/2025/01/12/如何利用Azure AI优化大模型推理：TensorRT-LLM与Blackwell平台深度整合/">https://www.liangyouze.com/2025/01/12/如何利用Azure AI优化大模型推理：TensorRT-LLM与Blackwell平台深度整合/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>梁友泽</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2025-01-12</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2025-03-27</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/azure/">azure</a></div><!--!--></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>支付宝</span><span class="qrcode"><img src="/img/images/qrcode/Alipay.jpeg" alt="支付宝"></span></a><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="/img/images/qrcode/WeChat.jpeg" alt="微信"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2025/01/13/Azure%20OpenAI%E6%9C%8D%E5%8A%A1%E5%85%A8%E8%A7%A3%E6%9E%90%EF%BC%9A%E4%BB%8EGPT-4%E5%88%B0DALL-E%E7%9A%84%E6%A8%A1%E5%9E%8B%E7%94%9F%E6%80%81/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">Azure OpenAI服务全解析：从GPT-4到DALL-E的模型生态</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2025/01/10/%E6%A8%A1%E5%9E%8B%E8%BD%BB%E9%87%8F%E5%8C%96%E9%9D%A9%E5%91%BD%EF%BC%9AAzure%20Neural%20Compression%E5%AE%9E%E7%8E%B010%E5%80%8D%E5%8E%8B%E7%BC%A9%E6%AF%94/"><span class="level-item">模型轻量化革命：Azure Neural Compression实现10倍压缩比</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><!--!--></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="梁友泽"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">梁友泽</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>北京</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives/"><p class="title">56</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories/"><p class="title">19</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags/"><p class="title">21</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/youzeliang" target="_blank" rel="me noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Github" href="https://github.com/youzeliang"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://www.liangyongrui.com" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">梁永锐</span></span><span class="level-right"><span class="level-item tag">www.liangyongrui.com</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/MySQL/"><span class="level-start"><span class="level-item">MySQL</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/"><span class="level-start"><span class="level-item">Python</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/azure/"><span class="level-start"><span class="level-item">azure</span></span><span class="level-end"><span class="level-item tag">20</span></span></a></li><li><a class="level is-mobile" href="/categories/docker/"><span class="level-start"><span class="level-item">docker</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/gin/"><span class="level-start"><span class="level-item">gin</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/golang/"><span class="level-start"><span class="level-item">golang</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/kafka/"><span class="level-start"><span class="level-item">kafka</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/map/"><span class="level-start"><span class="level-item">map</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/mysql/"><span class="level-start"><span class="level-item">mysql</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/panic/"><span class="level-start"><span class="level-item">panic</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/string/"><span class="level-start"><span class="level-item">string</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/vim/"><span class="level-start"><span class="level-item">vim</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%86%85%E5%AD%98%E9%80%83%E9%80%B8/"><span class="level-start"><span class="level-item">内存逃逸</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%87%BD%E6%95%B0/"><span class="level-start"><span class="level-item">函数</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%8F%8D%E5%B0%84/"><span class="level-start"><span class="level-item">反射</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%95%88%E7%8E%87/"><span class="level-start"><span class="level-item">效率</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%9D%82%E8%B0%88/"><span class="level-start"><span class="level-item">杂谈</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/%E7%B1%B3%E5%AE%B6/"><span class="level-start"><span class="level-item">米家</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%A7%84%E8%8C%83/"><span class="level-start"><span class="level-item">规范</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-03-20T13:10:23.000Z">2025-03-20</time></p><p class="title"><a href="/2025/03/20/AI%E9%A9%B1%E5%8A%A8%E7%9A%84%E7%BC%96%E8%AF%91%E5%99%A8%E4%BC%98%E5%8C%96%EF%BC%9AAzure%E4%B8%8ELLVM%E7%9A%84%E8%87%AA%E5%8A%A8%E5%8C%96%E4%BB%A3%E7%A0%81%E5%8A%A0%E9%80%9F%E6%96%B9%E6%A1%88/">AI驱动的编译器优化：Azure与LLVM的自动化代码加速方案</a></p><p class="categories"><a href="/categories/azure/">azure</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-03-15T13:10:23.000Z">2025-03-15</time></p><p class="title"><a href="/2025/03/15/Blackwell%20Ultra%20GPU%E5%9C%A8Azure%20AI%E4%B8%AD%E7%9A%84%E6%9C%AA%E6%9D%A5%E5%B1%95%E6%9C%9B%EF%BC%9A%E4%B8%87%E4%BA%BF%E5%8F%82%E6%95%B0%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83/">Blackwell Ultra GPU在Azure AI中的未来展望：万亿参数模型训练</a></p><p class="categories"><a href="/categories/azure/">azure</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-02-27T12:10:23.000Z">2025-02-27</time></p><p class="title"><a href="/2025/02/27/zure%E4%B8%8ENVIDIA%20Megatron%E7%9A%84%E5%8D%8F%E5%90%8C%E4%BC%98%E5%8C%96%E6%96%B9%E6%A1%88/">zure与NVIDIA Megatron的协同优化方案</a></p><p class="categories"><a href="/categories/azure/">azure</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-02-19T13:10:23.000Z">2025-02-19</time></p><p class="title"><a href="/2025/02/19/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E9%AB%98%E5%8F%AF%E6%89%A9%E5%B1%95%E6%80%A7%E5%8F%8A%E5%85%B6%E5%9C%A8%20Azure%20%E4%B8%AD%E7%9A%84%E5%AE%9E%E7%8E%B0/">深入理解高可扩展性及其在 Azure 中的实现</a></p><p class="categories"><a href="/categories/azure/">azure</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-02-12T13:10:23.000Z">2025-02-12</time></p><p class="title"><a href="/2025/02/12/Azure%E6%97%A0%E6%9C%8D%E5%8A%A1%E5%99%A8GPU%E5%AE%9E%E6%88%98%EF%BC%9A%E4%BD%8E%E6%88%90%E6%9C%AC%E8%BF%90%E8%A1%8C%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B/">Azure无服务器GPU实战：低成本运行多模态大模型</a></p><p class="categories"><a href="/categories/azure/">azure</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">归档</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2025/03/"><span class="level-start"><span class="level-item">三月 2025</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2025/02/"><span class="level-start"><span class="level-item">二月 2025</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2025/01/"><span class="level-start"><span class="level-item">一月 2025</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/12/"><span class="level-start"><span class="level-item">十二月 2024</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/11/"><span class="level-start"><span class="level-item">十一月 2024</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/09/"><span class="level-start"><span class="level-item">九月 2024</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/02/"><span class="level-start"><span class="level-item">二月 2024</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/11/"><span class="level-start"><span class="level-item">十一月 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/05/"><span class="level-start"><span class="level-item">五月 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/06/"><span class="level-start"><span class="level-item">六月 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/05/"><span class="level-start"><span class="level-item">五月 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/04/"><span class="level-start"><span class="level-item">四月 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/09/"><span class="level-start"><span class="level-item">九月 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/03/"><span class="level-start"><span class="level-item">三月 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/01/"><span class="level-start"><span class="level-item">一月 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/10/"><span class="level-start"><span class="level-item">十月 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/08/"><span class="level-start"><span class="level-item">八月 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/07/"><span class="level-start"><span class="level-item">七月 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/04/"><span class="level-start"><span class="level-item">四月 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/02/"><span class="level-start"><span class="level-item">二月 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/01/"><span class="level-start"><span class="level-item">一月 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/12/"><span class="level-start"><span class="level-item">十二月 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/10/"><span class="level-start"><span class="level-item">十月 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/09/"><span class="level-start"><span class="level-item">九月 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/07/"><span class="level-start"><span class="level-item">七月 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/06/"><span class="level-start"><span class="level-item">六月 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/03/"><span class="level-start"><span class="level-item">三月 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/02/"><span class="level-start"><span class="level-item">二月 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/01/"><span class="level-start"><span class="level-item">一月 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/12/"><span class="level-start"><span class="level-item">十二月 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/10/"><span class="level-start"><span class="level-item">十月 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/09/"><span class="level-start"><span class="level-item">九月 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/08/"><span class="level-start"><span class="level-item">八月 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/07/"><span class="level-start"><span class="level-item">七月 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/05/"><span class="level-start"><span class="level-item">五月 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/04/"><span class="level-start"><span class="level-item">四月 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/03/"><span class="level-start"><span class="level-item">三月 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2017/11/"><span class="level-start"><span class="level-item">十一月 2017</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Docker/"><span class="tag">Docker</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/MySQL/"><span class="tag">MySQL</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Python/"><span class="tag">Python</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/azure/"><span class="tag">azure</span><span class="tag">20</span></a></div><div class="control"><a class="tags has-addons" href="/tags/context/"><span class="tag">context</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/elasticsearch/"><span class="tag">elasticsearch</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/go/"><span class="tag">go</span><span class="tag">9</span></a></div><div class="control"><a class="tags has-addons" href="/tags/golang/"><span class="tag">golang</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/iframe/"><span class="tag">iframe</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/kafka/"><span class="tag">kafka</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/vim/"><span class="tag">vim</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%B8%AA%E4%BA%BA/"><span class="tag">个人</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90/"><span class="tag">内存对齐</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2/"><span class="tag">字符串</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%9D%82%E8%B0%88/"><span class="tag">杂谈</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%B1%B3%E5%AE%B6/"><span class="tag">米家</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%B4%A2%E5%BC%95/"><span class="tag">索引</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%A7%84%E5%88%99/"><span class="tag">规则</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AE%A2%E5%8D%95%E5%8F%B7/"><span class="tag">订单号</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AF%BB%E4%B9%A6/"><span class="tag">读书</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%B7%A8%E5%9F%9F/"><span class="tag">跨域</span><span class="tag">1</span></a></div></div></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="梁友泽的博客" height="28"></a><p class="is-size-7"><span>&copy; 2025 梁友泽</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">共<span id="busuanzi_value_site_uv">0</span>个访客</span></p></div><div class="level-end"></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-cn");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script data-pjax src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script data-pjax src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js"></script><script src="/js/pjax.js"></script><!--!--><!--!--><!--!--><script data-pjax src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script data-pjax src="/js/insight.js" defer></script><script data-pjax>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>